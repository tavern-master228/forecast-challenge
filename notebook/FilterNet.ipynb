{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "PATH = r'C:\\Users\\MikhailPetrovBrainer\\Documents\\Private\\SoSe22\\Projektseminar\\data\\q4_2017.xlsx'\n",
    "columns=['Sp_nummer', 'Due_date', 'Fc_horizon', 'Fc_date', 'Fc_and_order', 'Billing']\n",
    "df = pd.read_excel(PATH, index_col=None, header=1)\n",
    "df.drop(labels=['Unnamed: 0'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sp_number</th>\n",
       "      <th>Due_date</th>\n",
       "      <th>Fc_horizon</th>\n",
       "      <th>Fc_date</th>\n",
       "      <th>Fc_and_order</th>\n",
       "      <th>Billing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Product_19</td>\n",
       "      <td>201813</td>\n",
       "      <td>13</td>\n",
       "      <td>201752</td>\n",
       "      <td>176316</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Product_20</td>\n",
       "      <td>201813</td>\n",
       "      <td>13</td>\n",
       "      <td>201752</td>\n",
       "      <td>516510</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Product_22</td>\n",
       "      <td>201813</td>\n",
       "      <td>13</td>\n",
       "      <td>201752</td>\n",
       "      <td>237587</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Product_30</td>\n",
       "      <td>201813</td>\n",
       "      <td>13</td>\n",
       "      <td>201752</td>\n",
       "      <td>393741</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Product_39</td>\n",
       "      <td>201813</td>\n",
       "      <td>13</td>\n",
       "      <td>201752</td>\n",
       "      <td>92112</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sp_number  Due_date  Fc_horizon  Fc_date  Fc_and_order  Billing\n",
       "0  Product_19    201813          13   201752        176316      NaN\n",
       "1  Product_20    201813          13   201752        516510      NaN\n",
       "2  Product_22    201813          13   201752        237587      NaN\n",
       "3  Product_30    201813          13   201752        393741      NaN\n",
       "4  Product_39    201813          13   201752         92112      NaN"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = df['Sp_number'].unique()\n",
    "prod2idx = {}\n",
    "idx2prod = {}\n",
    "for idx, prod in enumerate(products):\n",
    "    if prod not in prod2idx:\n",
    "        prod2idx[prod] = idx\n",
    "        idx2prod[idx] = prod\n",
    "        \n",
    "#Add column with integer product names\n",
    "\n",
    "products_int = []\n",
    "for idx, row in df['Sp_number'].iteritems():\n",
    "    products_int.append(prod2idx[row])\n",
    "    \n",
    "df['products'] = products_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = {\n",
    "    'products': 'product',\n",
    "    'Fc_horizon': 'horizon',\n",
    "    'Fc_and_order': 'forecast',\n",
    "    'Billing': 'billing',\n",
    "    \"Due_date\": \"ddate\",\n",
    "    \"Fc_date\": \"fdate\"\n",
    "}\n",
    "df.rename(columns=mapper, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['isodate'] = df[['ddate']].apply(lambda x: dt.datetime.strptime(str(x['ddate'])+'-1',\"%Y%W-%w\"), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sp_number</th>\n",
       "      <th>ddate</th>\n",
       "      <th>horizon</th>\n",
       "      <th>fdate</th>\n",
       "      <th>forecast</th>\n",
       "      <th>billing</th>\n",
       "      <th>product</th>\n",
       "      <th>isodate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Product_19</td>\n",
       "      <td>201813</td>\n",
       "      <td>13</td>\n",
       "      <td>201752</td>\n",
       "      <td>176316</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-03-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Product_20</td>\n",
       "      <td>201813</td>\n",
       "      <td>13</td>\n",
       "      <td>201752</td>\n",
       "      <td>516510</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-03-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Product_22</td>\n",
       "      <td>201813</td>\n",
       "      <td>13</td>\n",
       "      <td>201752</td>\n",
       "      <td>237587</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-03-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Product_30</td>\n",
       "      <td>201813</td>\n",
       "      <td>13</td>\n",
       "      <td>201752</td>\n",
       "      <td>393741</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>2018-03-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Product_39</td>\n",
       "      <td>201813</td>\n",
       "      <td>13</td>\n",
       "      <td>201752</td>\n",
       "      <td>92112</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-03-26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sp_number   ddate  horizon   fdate  forecast  billing  product    isodate\n",
       "0  Product_19  201813       13  201752    176316      NaN        0 2018-03-26\n",
       "1  Product_20  201813       13  201752    516510      NaN        1 2018-03-26\n",
       "2  Product_22  201813       13  201752    237587      NaN        2 2018-03-26\n",
       "3  Product_30  201813       13  201752    393741      NaN        3 2018-03-26\n",
       "4  Product_39  201813       13  201752     92112      NaN        4 2018-03-26"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27064 entries, 0 to 27063\n",
      "Data columns (total 8 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   Sp_number  27064 non-null  object        \n",
      " 1   ddate      27064 non-null  int64         \n",
      " 2   horizon    27064 non-null  int64         \n",
      " 3   fdate      27064 non-null  int64         \n",
      " 4   forecast   27064 non-null  int64         \n",
      " 5   billing    26254 non-null  float64       \n",
      " 6   product    27064 non-null  int64         \n",
      " 7   isodate    27064 non-null  datetime64[ns]\n",
      "dtypes: datetime64[ns](1), float64(1), int64(5), object(1)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dh = df.groupby(['isodate', 'horizon', 'product'], as_index=False).sum()\n",
    "idxs = df_dh.loc[df_dh['billing'] == 0].index\n",
    "df_dh.drop(idxs, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 26124 entries, 0 to 26123\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count  Dtype         \n",
      "---  ------    --------------  -----         \n",
      " 0   isodate   26124 non-null  datetime64[ns]\n",
      " 1   horizon   26124 non-null  int64         \n",
      " 2   product   26124 non-null  int64         \n",
      " 3   ddate     26124 non-null  int64         \n",
      " 4   fdate     26124 non-null  int64         \n",
      " 5   forecast  26124 non-null  int64         \n",
      " 6   billing   26124 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(1), int64(5)\n",
      "memory usage: 1.4 MB\n"
     ]
    }
   ],
   "source": [
    "df_dh.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_dh.loc[df_dh['product'] == 0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isodate</th>\n",
       "      <th>horizon</th>\n",
       "      <th>product</th>\n",
       "      <th>ddate</th>\n",
       "      <th>fdate</th>\n",
       "      <th>forecast</th>\n",
       "      <th>billing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-01-06</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>201401</td>\n",
       "      <td>201352</td>\n",
       "      <td>220834</td>\n",
       "      <td>209000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2014-01-06</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>201401</td>\n",
       "      <td>201351</td>\n",
       "      <td>244710</td>\n",
       "      <td>209000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2014-01-06</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>201401</td>\n",
       "      <td>201350</td>\n",
       "      <td>250756</td>\n",
       "      <td>209000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2014-01-06</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>201401</td>\n",
       "      <td>201349</td>\n",
       "      <td>425917</td>\n",
       "      <td>209000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2014-01-06</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>201401</td>\n",
       "      <td>201348</td>\n",
       "      <td>421559</td>\n",
       "      <td>209000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26074</th>\n",
       "      <td>2017-12-25</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>201752</td>\n",
       "      <td>201743</td>\n",
       "      <td>178765</td>\n",
       "      <td>301000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26084</th>\n",
       "      <td>2017-12-25</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>201752</td>\n",
       "      <td>201742</td>\n",
       "      <td>178101</td>\n",
       "      <td>301000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26094</th>\n",
       "      <td>2017-12-25</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>201752</td>\n",
       "      <td>201741</td>\n",
       "      <td>167090</td>\n",
       "      <td>301000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26104</th>\n",
       "      <td>2017-12-25</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>201752</td>\n",
       "      <td>201740</td>\n",
       "      <td>171074</td>\n",
       "      <td>301000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26114</th>\n",
       "      <td>2017-12-25</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>201752</td>\n",
       "      <td>201739</td>\n",
       "      <td>196910</td>\n",
       "      <td>301000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2623 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         isodate  horizon  product   ddate   fdate  forecast   billing\n",
       "0     2014-01-06        1        0  201401  201352    220834  209000.0\n",
       "9     2014-01-06        2        0  201401  201351    244710  209000.0\n",
       "18    2014-01-06        3        0  201401  201350    250756  209000.0\n",
       "27    2014-01-06        4        0  201401  201349    425917  209000.0\n",
       "36    2014-01-06        5        0  201401  201348    421559  209000.0\n",
       "...          ...      ...      ...     ...     ...       ...       ...\n",
       "26074 2017-12-25        9        0  201752  201743    178765  301000.0\n",
       "26084 2017-12-25       10        0  201752  201742    178101  301000.0\n",
       "26094 2017-12-25       11        0  201752  201741    167090  301000.0\n",
       "26104 2017-12-25       12        0  201752  201740    171074  301000.0\n",
       "26114 2017-12-25       13        0  201752  201739    196910  301000.0\n",
       "\n",
       "[2623 rows x 7 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "hors = df1.horizon.unique()\n",
    "dates = df1.isodate.unique()\n",
    "data = {}\n",
    "\n",
    "for date in dates:\n",
    "    for h in hors:\n",
    "        val = df1.forecast.loc[(df1.horizon == h) & (df1.isodate == date)].values.tolist()\n",
    "        if not val:\n",
    "            val = [0]\n",
    "        if h not in data:\n",
    "            data[h] = []\n",
    "        data[h].append(val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-01-06</th>\n",
       "      <td>220834</td>\n",
       "      <td>244710</td>\n",
       "      <td>250756</td>\n",
       "      <td>425917</td>\n",
       "      <td>421559</td>\n",
       "      <td>421549</td>\n",
       "      <td>431115</td>\n",
       "      <td>455151</td>\n",
       "      <td>455123</td>\n",
       "      <td>463780</td>\n",
       "      <td>463904</td>\n",
       "      <td>459784</td>\n",
       "      <td>458313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-13</th>\n",
       "      <td>262289</td>\n",
       "      <td>250018</td>\n",
       "      <td>249931</td>\n",
       "      <td>250071</td>\n",
       "      <td>224503</td>\n",
       "      <td>220719</td>\n",
       "      <td>196538</td>\n",
       "      <td>192658</td>\n",
       "      <td>192600</td>\n",
       "      <td>179523</td>\n",
       "      <td>173930</td>\n",
       "      <td>159483</td>\n",
       "      <td>216909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-20</th>\n",
       "      <td>449633</td>\n",
       "      <td>525423</td>\n",
       "      <td>539445</td>\n",
       "      <td>536118</td>\n",
       "      <td>539133</td>\n",
       "      <td>409341</td>\n",
       "      <td>440793</td>\n",
       "      <td>408595</td>\n",
       "      <td>398796</td>\n",
       "      <td>401730</td>\n",
       "      <td>365757</td>\n",
       "      <td>290489</td>\n",
       "      <td>287652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-27</th>\n",
       "      <td>206085</td>\n",
       "      <td>186124</td>\n",
       "      <td>114911</td>\n",
       "      <td>117072</td>\n",
       "      <td>108968</td>\n",
       "      <td>107969</td>\n",
       "      <td>111797</td>\n",
       "      <td>115140</td>\n",
       "      <td>113152</td>\n",
       "      <td>115152</td>\n",
       "      <td>116882</td>\n",
       "      <td>124997</td>\n",
       "      <td>123413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-02-03</th>\n",
       "      <td>126028</td>\n",
       "      <td>171459</td>\n",
       "      <td>186274</td>\n",
       "      <td>167025</td>\n",
       "      <td>168022</td>\n",
       "      <td>170353</td>\n",
       "      <td>169847</td>\n",
       "      <td>195428</td>\n",
       "      <td>178293</td>\n",
       "      <td>178154</td>\n",
       "      <td>183590</td>\n",
       "      <td>202598</td>\n",
       "      <td>195719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-27</th>\n",
       "      <td>183804</td>\n",
       "      <td>222646</td>\n",
       "      <td>297679</td>\n",
       "      <td>241484</td>\n",
       "      <td>224469</td>\n",
       "      <td>224019</td>\n",
       "      <td>235650</td>\n",
       "      <td>212731</td>\n",
       "      <td>230485</td>\n",
       "      <td>216830</td>\n",
       "      <td>213490</td>\n",
       "      <td>216072</td>\n",
       "      <td>224634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-04</th>\n",
       "      <td>328866</td>\n",
       "      <td>344773</td>\n",
       "      <td>319621</td>\n",
       "      <td>422807</td>\n",
       "      <td>277310</td>\n",
       "      <td>265061</td>\n",
       "      <td>256083</td>\n",
       "      <td>220126</td>\n",
       "      <td>239771</td>\n",
       "      <td>253228</td>\n",
       "      <td>240649</td>\n",
       "      <td>242886</td>\n",
       "      <td>268371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-11</th>\n",
       "      <td>0</td>\n",
       "      <td>329160</td>\n",
       "      <td>284936</td>\n",
       "      <td>280945</td>\n",
       "      <td>303023</td>\n",
       "      <td>295784</td>\n",
       "      <td>272773</td>\n",
       "      <td>237929</td>\n",
       "      <td>251200</td>\n",
       "      <td>241125</td>\n",
       "      <td>248384</td>\n",
       "      <td>245842</td>\n",
       "      <td>253764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-18</th>\n",
       "      <td>194600</td>\n",
       "      <td>0</td>\n",
       "      <td>215511</td>\n",
       "      <td>225748</td>\n",
       "      <td>222492</td>\n",
       "      <td>279558</td>\n",
       "      <td>269261</td>\n",
       "      <td>255357</td>\n",
       "      <td>249216</td>\n",
       "      <td>239889</td>\n",
       "      <td>255379</td>\n",
       "      <td>246320</td>\n",
       "      <td>251957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-25</th>\n",
       "      <td>182723</td>\n",
       "      <td>209772</td>\n",
       "      <td>0</td>\n",
       "      <td>201367</td>\n",
       "      <td>193309</td>\n",
       "      <td>192361</td>\n",
       "      <td>189891</td>\n",
       "      <td>191804</td>\n",
       "      <td>178765</td>\n",
       "      <td>178101</td>\n",
       "      <td>167090</td>\n",
       "      <td>171074</td>\n",
       "      <td>196910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                1       2       3       4       5       6       7       8   \\\n",
       "2014-01-06  220834  244710  250756  425917  421559  421549  431115  455151   \n",
       "2014-01-13  262289  250018  249931  250071  224503  220719  196538  192658   \n",
       "2014-01-20  449633  525423  539445  536118  539133  409341  440793  408595   \n",
       "2014-01-27  206085  186124  114911  117072  108968  107969  111797  115140   \n",
       "2014-02-03  126028  171459  186274  167025  168022  170353  169847  195428   \n",
       "...            ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "2017-11-27  183804  222646  297679  241484  224469  224019  235650  212731   \n",
       "2017-12-04  328866  344773  319621  422807  277310  265061  256083  220126   \n",
       "2017-12-11       0  329160  284936  280945  303023  295784  272773  237929   \n",
       "2017-12-18  194600       0  215511  225748  222492  279558  269261  255357   \n",
       "2017-12-25  182723  209772       0  201367  193309  192361  189891  191804   \n",
       "\n",
       "                9       10      11      12      13  \n",
       "2014-01-06  455123  463780  463904  459784  458313  \n",
       "2014-01-13  192600  179523  173930  159483  216909  \n",
       "2014-01-20  398796  401730  365757  290489  287652  \n",
       "2014-01-27  113152  115152  116882  124997  123413  \n",
       "2014-02-03  178293  178154  183590  202598  195719  \n",
       "...            ...     ...     ...     ...     ...  \n",
       "2017-11-27  230485  216830  213490  216072  224634  \n",
       "2017-12-04  239771  253228  240649  242886  268371  \n",
       "2017-12-11  251200  241125  248384  245842  253764  \n",
       "2017-12-18  249216  239889  255379  246320  251957  \n",
       "2017-12-25  178765  178101  167090  171074  196910  \n",
       "\n",
       "[208 rows x 13 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ = pd.DataFrame(data, columns=data.keys(), index=dates)\n",
    "df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2014-01-06    397884.230769\n",
       "2014-01-13    213013.230769\n",
       "2014-01-20    430223.461538\n",
       "2014-01-27    127820.153846\n",
       "2014-02-03    176368.461538\n",
       "                  ...      \n",
       "2017-11-27    226461.000000\n",
       "2017-12-04    283042.461538\n",
       "2017-12-11    249605.000000\n",
       "2017-12-18    223483.692308\n",
       "2017-12-25    173320.538462\n",
       "Length: 208, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_.T.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "hors = df1.horizon.unique()\n",
    "dates = df1.isodate.unique()\n",
    "data = {}\n",
    "\n",
    "means = df_.T.mean()\n",
    "\n",
    "for date in dates:\n",
    "    mean = means[date]\n",
    "    for h in hors:\n",
    "        val = df1.forecast.loc[(df1.horizon == h) & (df1.isodate == date)].values.tolist()\n",
    "        if not val:\n",
    "            val = [mean]\n",
    "        if h not in data:\n",
    "            data[h] = []\n",
    "        data[h].append(val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = pd.DataFrame(data, columns=data.keys(), index=dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-01-06</th>\n",
       "      <td>220834.0</td>\n",
       "      <td>244710.000000</td>\n",
       "      <td>250756.000000</td>\n",
       "      <td>425917.0</td>\n",
       "      <td>421559.0</td>\n",
       "      <td>421549.0</td>\n",
       "      <td>431115.0</td>\n",
       "      <td>455151.0</td>\n",
       "      <td>455123.0</td>\n",
       "      <td>463780.0</td>\n",
       "      <td>463904.0</td>\n",
       "      <td>459784.0</td>\n",
       "      <td>458313.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-13</th>\n",
       "      <td>262289.0</td>\n",
       "      <td>250018.000000</td>\n",
       "      <td>249931.000000</td>\n",
       "      <td>250071.0</td>\n",
       "      <td>224503.0</td>\n",
       "      <td>220719.0</td>\n",
       "      <td>196538.0</td>\n",
       "      <td>192658.0</td>\n",
       "      <td>192600.0</td>\n",
       "      <td>179523.0</td>\n",
       "      <td>173930.0</td>\n",
       "      <td>159483.0</td>\n",
       "      <td>216909.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-20</th>\n",
       "      <td>449633.0</td>\n",
       "      <td>525423.000000</td>\n",
       "      <td>539445.000000</td>\n",
       "      <td>536118.0</td>\n",
       "      <td>539133.0</td>\n",
       "      <td>409341.0</td>\n",
       "      <td>440793.0</td>\n",
       "      <td>408595.0</td>\n",
       "      <td>398796.0</td>\n",
       "      <td>401730.0</td>\n",
       "      <td>365757.0</td>\n",
       "      <td>290489.0</td>\n",
       "      <td>287652.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-27</th>\n",
       "      <td>206085.0</td>\n",
       "      <td>186124.000000</td>\n",
       "      <td>114911.000000</td>\n",
       "      <td>117072.0</td>\n",
       "      <td>108968.0</td>\n",
       "      <td>107969.0</td>\n",
       "      <td>111797.0</td>\n",
       "      <td>115140.0</td>\n",
       "      <td>113152.0</td>\n",
       "      <td>115152.0</td>\n",
       "      <td>116882.0</td>\n",
       "      <td>124997.0</td>\n",
       "      <td>123413.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-02-03</th>\n",
       "      <td>126028.0</td>\n",
       "      <td>171459.000000</td>\n",
       "      <td>186274.000000</td>\n",
       "      <td>167025.0</td>\n",
       "      <td>168022.0</td>\n",
       "      <td>170353.0</td>\n",
       "      <td>169847.0</td>\n",
       "      <td>195428.0</td>\n",
       "      <td>178293.0</td>\n",
       "      <td>178154.0</td>\n",
       "      <td>183590.0</td>\n",
       "      <td>202598.0</td>\n",
       "      <td>195719.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-27</th>\n",
       "      <td>183804.0</td>\n",
       "      <td>222646.000000</td>\n",
       "      <td>297679.000000</td>\n",
       "      <td>241484.0</td>\n",
       "      <td>224469.0</td>\n",
       "      <td>224019.0</td>\n",
       "      <td>235650.0</td>\n",
       "      <td>212731.0</td>\n",
       "      <td>230485.0</td>\n",
       "      <td>216830.0</td>\n",
       "      <td>213490.0</td>\n",
       "      <td>216072.0</td>\n",
       "      <td>224634.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-04</th>\n",
       "      <td>328866.0</td>\n",
       "      <td>344773.000000</td>\n",
       "      <td>319621.000000</td>\n",
       "      <td>422807.0</td>\n",
       "      <td>277310.0</td>\n",
       "      <td>265061.0</td>\n",
       "      <td>256083.0</td>\n",
       "      <td>220126.0</td>\n",
       "      <td>239771.0</td>\n",
       "      <td>253228.0</td>\n",
       "      <td>240649.0</td>\n",
       "      <td>242886.0</td>\n",
       "      <td>268371.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-11</th>\n",
       "      <td>249605.0</td>\n",
       "      <td>329160.000000</td>\n",
       "      <td>284936.000000</td>\n",
       "      <td>280945.0</td>\n",
       "      <td>303023.0</td>\n",
       "      <td>295784.0</td>\n",
       "      <td>272773.0</td>\n",
       "      <td>237929.0</td>\n",
       "      <td>251200.0</td>\n",
       "      <td>241125.0</td>\n",
       "      <td>248384.0</td>\n",
       "      <td>245842.0</td>\n",
       "      <td>253764.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-18</th>\n",
       "      <td>194600.0</td>\n",
       "      <td>223483.692308</td>\n",
       "      <td>215511.000000</td>\n",
       "      <td>225748.0</td>\n",
       "      <td>222492.0</td>\n",
       "      <td>279558.0</td>\n",
       "      <td>269261.0</td>\n",
       "      <td>255357.0</td>\n",
       "      <td>249216.0</td>\n",
       "      <td>239889.0</td>\n",
       "      <td>255379.0</td>\n",
       "      <td>246320.0</td>\n",
       "      <td>251957.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-25</th>\n",
       "      <td>182723.0</td>\n",
       "      <td>209772.000000</td>\n",
       "      <td>173320.538462</td>\n",
       "      <td>201367.0</td>\n",
       "      <td>193309.0</td>\n",
       "      <td>192361.0</td>\n",
       "      <td>189891.0</td>\n",
       "      <td>191804.0</td>\n",
       "      <td>178765.0</td>\n",
       "      <td>178101.0</td>\n",
       "      <td>167090.0</td>\n",
       "      <td>171074.0</td>\n",
       "      <td>196910.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  1              2              3         4         5   \\\n",
       "2014-01-06  220834.0  244710.000000  250756.000000  425917.0  421559.0   \n",
       "2014-01-13  262289.0  250018.000000  249931.000000  250071.0  224503.0   \n",
       "2014-01-20  449633.0  525423.000000  539445.000000  536118.0  539133.0   \n",
       "2014-01-27  206085.0  186124.000000  114911.000000  117072.0  108968.0   \n",
       "2014-02-03  126028.0  171459.000000  186274.000000  167025.0  168022.0   \n",
       "...              ...            ...            ...       ...       ...   \n",
       "2017-11-27  183804.0  222646.000000  297679.000000  241484.0  224469.0   \n",
       "2017-12-04  328866.0  344773.000000  319621.000000  422807.0  277310.0   \n",
       "2017-12-11  249605.0  329160.000000  284936.000000  280945.0  303023.0   \n",
       "2017-12-18  194600.0  223483.692308  215511.000000  225748.0  222492.0   \n",
       "2017-12-25  182723.0  209772.000000  173320.538462  201367.0  193309.0   \n",
       "\n",
       "                  6         7         8         9         10        11  \\\n",
       "2014-01-06  421549.0  431115.0  455151.0  455123.0  463780.0  463904.0   \n",
       "2014-01-13  220719.0  196538.0  192658.0  192600.0  179523.0  173930.0   \n",
       "2014-01-20  409341.0  440793.0  408595.0  398796.0  401730.0  365757.0   \n",
       "2014-01-27  107969.0  111797.0  115140.0  113152.0  115152.0  116882.0   \n",
       "2014-02-03  170353.0  169847.0  195428.0  178293.0  178154.0  183590.0   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2017-11-27  224019.0  235650.0  212731.0  230485.0  216830.0  213490.0   \n",
       "2017-12-04  265061.0  256083.0  220126.0  239771.0  253228.0  240649.0   \n",
       "2017-12-11  295784.0  272773.0  237929.0  251200.0  241125.0  248384.0   \n",
       "2017-12-18  279558.0  269261.0  255357.0  249216.0  239889.0  255379.0   \n",
       "2017-12-25  192361.0  189891.0  191804.0  178765.0  178101.0  167090.0   \n",
       "\n",
       "                  12        13  \n",
       "2014-01-06  459784.0  458313.0  \n",
       "2014-01-13  159483.0  216909.0  \n",
       "2014-01-20  290489.0  287652.0  \n",
       "2014-01-27  124997.0  123413.0  \n",
       "2014-02-03  202598.0  195719.0  \n",
       "...              ...       ...  \n",
       "2017-11-27  216072.0  224634.0  \n",
       "2017-12-04  242886.0  268371.0  \n",
       "2017-12-11  245842.0  253764.0  \n",
       "2017-12-18  246320.0  251957.0  \n",
       "2017-12-25  171074.0  196910.0  \n",
       "\n",
       "[208 rows x 13 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = df_.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = df_dh[['isodate','billing']].loc[df['product'] == 0].drop_duplicates(['isodate']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.set_index(['isodate'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>billing</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>isodate</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-01-06</th>\n",
       "      <td>209000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-13</th>\n",
       "      <td>760000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-20</th>\n",
       "      <td>50000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-27</th>\n",
       "      <td>207000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-02-03</th>\n",
       "      <td>1225000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-27</th>\n",
       "      <td>960000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-04</th>\n",
       "      <td>200000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-11</th>\n",
       "      <td>135000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-18</th>\n",
       "      <td>1472500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-25</th>\n",
       "      <td>357500.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              billing\n",
       "isodate              \n",
       "2014-01-06   209000.0\n",
       "2014-01-13   760000.0\n",
       "2014-01-20    50000.0\n",
       "2014-01-27   207000.0\n",
       "2014-02-03  1225000.0\n",
       "...               ...\n",
       "2017-11-27   960000.0\n",
       "2017-12-04   200000.0\n",
       "2017-12-11   135000.0\n",
       "2017-12-18  1472500.0\n",
       "2017-12-25   357500.0\n",
       "\n",
       "[208 rows x 1 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-01-06</th>\n",
       "      <td>220834.0</td>\n",
       "      <td>244710.000000</td>\n",
       "      <td>250756.000000</td>\n",
       "      <td>425917.0</td>\n",
       "      <td>421559.0</td>\n",
       "      <td>421549.0</td>\n",
       "      <td>431115.0</td>\n",
       "      <td>455151.0</td>\n",
       "      <td>455123.0</td>\n",
       "      <td>463780.0</td>\n",
       "      <td>463904.0</td>\n",
       "      <td>459784.0</td>\n",
       "      <td>458313.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-13</th>\n",
       "      <td>262289.0</td>\n",
       "      <td>250018.000000</td>\n",
       "      <td>249931.000000</td>\n",
       "      <td>250071.0</td>\n",
       "      <td>224503.0</td>\n",
       "      <td>220719.0</td>\n",
       "      <td>196538.0</td>\n",
       "      <td>192658.0</td>\n",
       "      <td>192600.0</td>\n",
       "      <td>179523.0</td>\n",
       "      <td>173930.0</td>\n",
       "      <td>159483.0</td>\n",
       "      <td>216909.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-20</th>\n",
       "      <td>449633.0</td>\n",
       "      <td>525423.000000</td>\n",
       "      <td>539445.000000</td>\n",
       "      <td>536118.0</td>\n",
       "      <td>539133.0</td>\n",
       "      <td>409341.0</td>\n",
       "      <td>440793.0</td>\n",
       "      <td>408595.0</td>\n",
       "      <td>398796.0</td>\n",
       "      <td>401730.0</td>\n",
       "      <td>365757.0</td>\n",
       "      <td>290489.0</td>\n",
       "      <td>287652.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-27</th>\n",
       "      <td>206085.0</td>\n",
       "      <td>186124.000000</td>\n",
       "      <td>114911.000000</td>\n",
       "      <td>117072.0</td>\n",
       "      <td>108968.0</td>\n",
       "      <td>107969.0</td>\n",
       "      <td>111797.0</td>\n",
       "      <td>115140.0</td>\n",
       "      <td>113152.0</td>\n",
       "      <td>115152.0</td>\n",
       "      <td>116882.0</td>\n",
       "      <td>124997.0</td>\n",
       "      <td>123413.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-02-03</th>\n",
       "      <td>126028.0</td>\n",
       "      <td>171459.000000</td>\n",
       "      <td>186274.000000</td>\n",
       "      <td>167025.0</td>\n",
       "      <td>168022.0</td>\n",
       "      <td>170353.0</td>\n",
       "      <td>169847.0</td>\n",
       "      <td>195428.0</td>\n",
       "      <td>178293.0</td>\n",
       "      <td>178154.0</td>\n",
       "      <td>183590.0</td>\n",
       "      <td>202598.0</td>\n",
       "      <td>195719.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-27</th>\n",
       "      <td>183804.0</td>\n",
       "      <td>222646.000000</td>\n",
       "      <td>297679.000000</td>\n",
       "      <td>241484.0</td>\n",
       "      <td>224469.0</td>\n",
       "      <td>224019.0</td>\n",
       "      <td>235650.0</td>\n",
       "      <td>212731.0</td>\n",
       "      <td>230485.0</td>\n",
       "      <td>216830.0</td>\n",
       "      <td>213490.0</td>\n",
       "      <td>216072.0</td>\n",
       "      <td>224634.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-04</th>\n",
       "      <td>328866.0</td>\n",
       "      <td>344773.000000</td>\n",
       "      <td>319621.000000</td>\n",
       "      <td>422807.0</td>\n",
       "      <td>277310.0</td>\n",
       "      <td>265061.0</td>\n",
       "      <td>256083.0</td>\n",
       "      <td>220126.0</td>\n",
       "      <td>239771.0</td>\n",
       "      <td>253228.0</td>\n",
       "      <td>240649.0</td>\n",
       "      <td>242886.0</td>\n",
       "      <td>268371.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-11</th>\n",
       "      <td>249605.0</td>\n",
       "      <td>329160.000000</td>\n",
       "      <td>284936.000000</td>\n",
       "      <td>280945.0</td>\n",
       "      <td>303023.0</td>\n",
       "      <td>295784.0</td>\n",
       "      <td>272773.0</td>\n",
       "      <td>237929.0</td>\n",
       "      <td>251200.0</td>\n",
       "      <td>241125.0</td>\n",
       "      <td>248384.0</td>\n",
       "      <td>245842.0</td>\n",
       "      <td>253764.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-18</th>\n",
       "      <td>194600.0</td>\n",
       "      <td>223483.692308</td>\n",
       "      <td>215511.000000</td>\n",
       "      <td>225748.0</td>\n",
       "      <td>222492.0</td>\n",
       "      <td>279558.0</td>\n",
       "      <td>269261.0</td>\n",
       "      <td>255357.0</td>\n",
       "      <td>249216.0</td>\n",
       "      <td>239889.0</td>\n",
       "      <td>255379.0</td>\n",
       "      <td>246320.0</td>\n",
       "      <td>251957.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-25</th>\n",
       "      <td>182723.0</td>\n",
       "      <td>209772.000000</td>\n",
       "      <td>173320.538462</td>\n",
       "      <td>201367.0</td>\n",
       "      <td>193309.0</td>\n",
       "      <td>192361.0</td>\n",
       "      <td>189891.0</td>\n",
       "      <td>191804.0</td>\n",
       "      <td>178765.0</td>\n",
       "      <td>178101.0</td>\n",
       "      <td>167090.0</td>\n",
       "      <td>171074.0</td>\n",
       "      <td>196910.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  1              2              3         4         5   \\\n",
       "2014-01-06  220834.0  244710.000000  250756.000000  425917.0  421559.0   \n",
       "2014-01-13  262289.0  250018.000000  249931.000000  250071.0  224503.0   \n",
       "2014-01-20  449633.0  525423.000000  539445.000000  536118.0  539133.0   \n",
       "2014-01-27  206085.0  186124.000000  114911.000000  117072.0  108968.0   \n",
       "2014-02-03  126028.0  171459.000000  186274.000000  167025.0  168022.0   \n",
       "...              ...            ...            ...       ...       ...   \n",
       "2017-11-27  183804.0  222646.000000  297679.000000  241484.0  224469.0   \n",
       "2017-12-04  328866.0  344773.000000  319621.000000  422807.0  277310.0   \n",
       "2017-12-11  249605.0  329160.000000  284936.000000  280945.0  303023.0   \n",
       "2017-12-18  194600.0  223483.692308  215511.000000  225748.0  222492.0   \n",
       "2017-12-25  182723.0  209772.000000  173320.538462  201367.0  193309.0   \n",
       "\n",
       "                  6         7         8         9         10        11  \\\n",
       "2014-01-06  421549.0  431115.0  455151.0  455123.0  463780.0  463904.0   \n",
       "2014-01-13  220719.0  196538.0  192658.0  192600.0  179523.0  173930.0   \n",
       "2014-01-20  409341.0  440793.0  408595.0  398796.0  401730.0  365757.0   \n",
       "2014-01-27  107969.0  111797.0  115140.0  113152.0  115152.0  116882.0   \n",
       "2014-02-03  170353.0  169847.0  195428.0  178293.0  178154.0  183590.0   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2017-11-27  224019.0  235650.0  212731.0  230485.0  216830.0  213490.0   \n",
       "2017-12-04  265061.0  256083.0  220126.0  239771.0  253228.0  240649.0   \n",
       "2017-12-11  295784.0  272773.0  237929.0  251200.0  241125.0  248384.0   \n",
       "2017-12-18  279558.0  269261.0  255357.0  249216.0  239889.0  255379.0   \n",
       "2017-12-25  192361.0  189891.0  191804.0  178765.0  178101.0  167090.0   \n",
       "\n",
       "                  12        13  \n",
       "2014-01-06  459784.0  458313.0  \n",
       "2014-01-13  159483.0  216909.0  \n",
       "2014-01-20  290489.0  287652.0  \n",
       "2014-01-27  124997.0  123413.0  \n",
       "2014-02-03  202598.0  195719.0  \n",
       "...              ...       ...  \n",
       "2017-11-27  216072.0  224634.0  \n",
       "2017-12-04  242886.0  268371.0  \n",
       "2017-12-11  245842.0  253764.0  \n",
       "2017-12-18  246320.0  251957.0  \n",
       "2017-12-25  171074.0  196910.0  \n",
       "\n",
       "[208 rows x 13 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkkAAAGNCAYAAAAFEeULAAAgAElEQVR4nOzddVhV9x/A8fcNLi2hlM5CEUUUBbGxZ87WoU7n7NkxY05/ds0p9qzNWVNnK3YHBpKCBaISIiUldbnc+P2BAYqIimKc1/PwPHriez7n3Pqc7/mGSKPRaBAIBAKBQCAQ5CIu6gAEAoFAIBAIPkVCkiQQCAQCgUCQByFJEggEAoFAIMiDkCQJBAKBQCAQ5EFa1AEIBAKBQJCX8PBwzpw5g7e3N7du3SIjIwMtLS3KlStHrVq1aNiwITVr1kQkEhV1qIIvlEjo3SYQCASCT8nFixdxc3PjwoULtGzZEmdnZ6pVq4a+vj4KhYKQkBC8vb05ceIEpqamjBo1ih9++AGJRFLUoQu+MF9VkhQYGMiDBw/o0KFDUYcieElsbCz79+9n0KBBwl2hQPCVSklJYcKECRw5coQpU6bwww8/YGBg8NrtVSoVx44dY968eYjFYv755x8qVqz4ESMWfOm+mjZJ9+/fZ9GiRbRt27aoQxHkwdzcHCMjI5YvX17UoQgEgiIQFRVF7dq1USgUBAYGMmTIkHwTJACJREK7du24ePEiXbt2pV69epw7d+4jRSz4GnwVNUlKpZIWLVqwfft2rKysijocQT769etH//79cXFxKepQBALBRxIVFUXTpk358ccf+e233965nLNnz/L999+za9cumjRpUogRCr5WX0WStGPHDvz9/VmwYEFRhyJ4g7t37zJs2DBOnjxZ1KEIBIKPQK1W07x5c1xcXJg1a9Z7l3f69Gl69epFQEAAFhYWhRBh0UlISODRo0fo6OgU2WPEBw8ekJaWhoWFBWZmZkUSQ1F6i8dtamKun+SQuzvuz/8O4xGSCuoYrp88hLu7O4cv3yc9j30OnQogVv10afwNzhxyx/3QMfyi1YV8Sq9at24dvXr1+uDHeT9qHp1dwphBi7mQ/uati5Q6ivPLxjJo8XnSCrloGxsbnjx5QkhISCGX/OU6cuQIqampRR2GQPBO/vzzTzIzM5k+fXqhlNe8eXP69+/PkCFD+NzrAHbv3o2bmxtpaYX9TVtwaWlprFu3jo0bNxZZDEXpLZIkMVohmxjcuQvjt90i6lEgW8b3ZZmXBsRahGwaTOfOo9kdo4t2zn2C/2Fgpw506DqHs3KARI7+2p6W7TvSfeYlsvQ/bLOoZ8+3q1Wr9kGP8/7EWFYQc8M3Hl1ZUcfyBmILKklvE/DEKMdrXXjq16/P+fPnP0DJX6aRI0cSExNT1GEIBG8tPT2dmTNn8vfffxdqz7QZM2YQGBjI1atXC63MolK6dGkcHByK7Pj29vaUL1++yI5f1N4uQ3nyhCdY0OjHMQweMol5S1Ywqpn+01VPwKIZ3dtYkeutnirnm6q2SOWPiY5Xk3ltCX8EFKeClpRqHXviZFh4J5OXO3fuUK5cuRw9ptKJj4wgNlUFgDo1jrhUNSAnITKCmBTlhwlEEc/9G4HcDk/idUeQ+/oRVa0OVUUpxEREEBmfd5WSMjGMsHglICfmQRgJig8T8usp8PFLwNnFhpSw+0QXcs2Xra0t169fL9xCBQLBJ2fHjh3UqVOHKlWqFGq52trajBgxgpUrVxZquZ+9ZB/WjevPxI2BCHXPBfMWSVImPp7+yLWrU9shgr8Hjse/Xk9cLMSQ6YOnvxztGnWpJXtpH69QbBrWwFgTR0xkEOtne+DUrCQxGkuc6lTgQ49qERgYmPtZriqRE7804ju32yhRcHlaEzqvuItKncDh0Q3ovvp+ocegjt3H0M6TOBZ8i93DWzDsiDyPrRTc8LxBaWdntFPOMLlZC8btD+Xl/Cfx3Aqm/DaIVj/NYcPs6Sz8tQv1Bu0hodCjzofyJp5BJuA1l9/nDKTxd26FWryNjQ0BAQGFWqZAIPj0bN68mcGDB+ezRSZ+60fSt09fRq2+RuKDPUzt/xNj1nqR8oay+/Xrx4EDB0hP/9TbL3xEBiY88b+Jbs0q5N9vUPBMwZMk1T08faPBMI5DA7vym78RtobPVnniGy2msnNtTMUv7RNYHKdvy2JGLHc2zmJX2XHUj/clVbsGdRxfPKxRx11l8y4vMgvpxJ5JSkrC0DBHdZXEiqaNbElLSECt8OOUTzKpcbGoNJkkiBozpO87NI5ThHP6v1OEqfJerY7yxyfagMou3fltw16mN9HJY6M4PH2heoUHbFp0iRp/X2T7ADtefvJm0mQks3pXRxIcjsmA+Syc2ZUS/p4Ev5RNpR8YRbPJp9/+XApAHeuJZ0gW1u2ns2DZYByCrgJKbrh9R2UbG2xsKlHJthp12o1gg3/2/YoqaBXda3XG7XqOV1h+kFENx3L4pZzR0NCQ5OTkDxK7QCD4NKhUKnx8fN7Qk1Wb6g2sCNq9A99UU8RR3lwKq0iPXs686SGEsbHxp1krrXrE0WX/4P0OP3aZ9w6zetkqFo4bzOJLcUSFxvJWKaDcj+vxNalX+cVkGylRocQKeeRrFXxakmQvvG+rMXedwX+rzNm7T5sq0mervLmtNse1TiWkqEmNiUNjZoFhshe+GQ6McyzNUVEch9zVbLxalltdYhFXcaa28bPClfgsGcpK2SZ6dS/cE1Sr1YjFOTM3McaWZig944ncfQXld53RDYgh2eMY/s4TWG/x9m2klHc3MeX3DFZ0bUHZPNZLHcayauAohjZpQ5d1//Fbgzw2yvDk2oNiFDs4md/TxuLbyPw1Gaya6Gt+yDrNo11JMcqgKOLNKlLypVdSnRROUMSTtz6Xgsjw9CSs0SD62WujDn1EnFEpAOQJkaia/MHpWXWQZiVz979x/DBgITU9Z1FNHsu9AHdODF1Es7NTqKENqJOJuB1O8ktt9yUSCSrVazJOgUDwRQgKCsLCwgITE5N8t5NU7kPfRgsYu2UM/c6WZczWudQtYDMNPT09vvvuO4oVK1YIEb/ZuHHjGDly5Os3UIWzb2Q//rWewtQ718mVvokMKFm5Amava5OadJwJww7x7dZVNLm7izNG55jRP5C+x2ZRv4DtWBUBntwu74zT8/oJOSdn9Me/7zFmFbSQr0yBk6RM36v4Zcio7lwLmbYFvXo8X4PvVT8ytB2oW0sHdcxOhna6QN9zK3Hxucq9Sk2oYSGjZDEjGk6Yxfd6l+lyR415j1pUlACouLd1LGO23uHRN2vZXMWaK+G1WTnBBW3UxLjPZPqDlgwUbeWgphKxJw8SrNuQMX9Mp0NZKerEa/w9YwHbfR6jXbkLv80fhYvZi/RCW1ubrKysXOcitrRE5+EhVvg2YfTQWK4cO8Xv/5bl56XVn16QVG79N585f53jgcoKlwEzmPaDPbJLboy4VpeVY+sjI5PzC0cR4DIG4793E/IgiymTavDXgqakeEZTvH41LMWAOgY/vwwch27mhFkf6q04zpgG3Um7eZno4vWpZpkdqyLwKjfKdebAdH0etjiLV2YHGkolSEVx3LycozzS8LwWStVu1ZCRxvkDXti6jucbMSR5rWPK7C34pZWjZaVENAAKDxZPuAA6Xhy9W5dZ24ZjfODVc9PxX8+ko0pKhx7lUKguNXtPYXqf6higJi5XrApuXL1OqbozMULF3T1HkHeZ9+La6hpjYWWFNlZYdG1GhVV3iFJBNUBcsTNdtdcwfFE7zkyp8dpG3wqFAm3tD9EkXCAQfCpiY2MpVarUmzcUl6JLdxcm/uyP6fz/6GhV8BtZHR0dEhISSEj4OA0SkpKS8t/gyS3OXbxHrOIUu5NeOg9JWdqMeX2SlOD+F96OE1hkJkZm5kpHdTKO610wlalJjghHYaJLjN8d0ks54GRtnEdTFjXRXv5o1ejDi7RURvOp66lvKkOdHEG4wgTdGD/upJfCwckaY2GWlwImScqbbFhykEi1BrOwezzBAuPnqzaw5GAkGokO24d2YcfNs/g7rOVPyQ3+cjvIw0xbIqT9mHXCA92q5lyZtYbz6Rp04qKIUkE5iYRvvm1A+ak36bD6f3TOXMmfKz0IHeeCrfwiC/53DIu1rpwZt4Wt37ix9+8dpK3oRM9fq9FoS3U2u/bhcJO/2Ha6OpFunek71RbPtW3QfxqftbU1u3fvzn3SJa0wuXkd69W9KGv5HyZ39mL8+1Lq6GWvTzk6ns6z5EzdsZ/WHGNCj678YnqNJUlXOHSlFEvHgowsQi8fwtPmD5b/1JplnhmMHdeGkqqzDOq5jgYBhxhoAmRcZdWQdZTo1xET/zSadqyLNgpOLerJugYBHBpoAqiJ9vRFU6MzliWsaFP2D6b//Bs/DpvOgGpXWJSzPIU/V6+nct94LStv3OSydDzL+pVFnHKYCb1XI1uwiwOO93Dr4YqiAqCM4OrmJTwevIRRgypjeXo8bfI4t2UaT/6b70PnzbvYZufP/7q7MsnSi1XfSrmSM1Z1NNdupyKP3sqahbF4BHfhz2W1ASWgIfHq3/zv12OIVWmEelzCZMImmj/Ld6Tl6b+8ITPaDmdRuzNMqZT32y0iIoIKFSoU8C0sEAg+RxqNpkBTEKnjz+C2/zFljGPxOOtPZrsGBe5VW7t2bSpXrszEiRPfL9g3eNZD741MWuN2ZBn/W/iI76cNpXqBK2+URN6PpHj5CsiAtNQ0dMXujGnvyQi/34kc3Zh5j+2pVb0Uj08PpsSsC2zubkZKVARy07KYaQNk4OUVhX1PG6QoCXCbgVf3X9Ee0x7PEX78HjmaxvMeY1+rOqUen2ZwiVlc2Nidd3i48kUpWJIkrcrQw5EMzXPVUA5H5rUGhh+NZPiz/9Q0AsByjifJc14qI/EBoUbOjKhiiUlmFcrHnCUkS4X6z+mcqj2di1UD+flhIybtGEgNczGKdo0w+SWEsNNnWRJUlSH9o7l4MBq1fknUN/yJULWh8tMM2MHBgeDg4FzHE5cbxr7bP2NkIgG68vfNThgaP2snlImn+2nM+x6lVzUzJPRiat91tDpwjczGeZ2ljGImBki1JJSwMEQUcofHNdvg8qx2V78jf11rRVJcMpoBAzHREYPqDnce16TN843ElBl9Cv+n//t5vx8/oYOOFFR3cpeneuiJv+lI1i8eRBmxHiN0st/BmdeOcN7yB451qIiZpCKThjbl36fjMWoktflhUm86m2Zyathrzq0DaDcaxrROFTGjPBN7L6ftAS8ym1nljlX8DUN232GkREGKXMYw/dxvIYmeEcWLF0es1kdjrcXeLdvw6DWXZ5dOajucFePdaTl8Ee0OlcnzfRMUFESNGjXyXCcQCL4MRkZGb67hSfNjyQA3tH89iNvW2rTfuYkzMxvQRj//3Z5JT0+nTJkylC5d+v0DzsfblC8p3ZE5v6eheKunW1KsG7uQPH00E+6bI7HuwZTeOdcrsR/2L5t7GJO8qQt1zgZC9/ocHPkdXqN8WNpIhvLeLrZdkKOwWsjMc4Hsv2DD2hFicv46Ku2H8e/mHhgnb6JLnbMEZnXH4iuv1C94m6QPKNXXl4d23bGXAeLKVC62iZDb29m80ZgxR1ujFzCJm980ZEbJ7IQg7f59kiy+g5snyCxbBdWDELKHHqzKT8NaUCpHFaGZmRkikYjk5GSMjIyeLtXhxWNwbYyMyUFDeoYcHX19su9xROjp6aDMUqARkWNwMhXqV8bBVJMir8fsTQ2wzVlNKdbB2OJFY211ipx6szfRwPY1dZlSHXReU16qpydBZu0ppWeQq+GiJkNOprYuuk9vzHQMDHh2RJFMH0OdN5wbIDMyfloDl71cnpGG6pVYxchkYkCK4StfVCKKVe/GmAlNsu/0VF0xrNeQNcen0vh5D18pVYavYLx7S4Yv6YPxy0WQ3SNx4MCBeV8bgUDwRbCzsyMkJISMjAx0dXVf3SDNh+WunZmeMAzvutrE7jWEyJ3MXTGM5r/WeKVTS158fX0/yfk6xXr65NF9J1/6TX7n7NFUMrUN0JMA6TnSG5ERFhbZ11BLVxuxSgnpAQRpmtP6aXWVtMJP7An76fku2UN3pudIkkQYWVigm10I2mLVa4er+Zp8AhVpCm743KZMTcfsF0daiSrWj9gz9ncie86ibxmI8/XlfmwUUZlAmjcrVl3HpWdbSluZo1XMkZ8mTWbyqDZoXT1GvFWVV3o9dO/eHXd39wLGI8PBqQrBJw4TrgJU4Rw5EYxtTXt0tHUQPY4iTg2k+OATlP0WEonFiJVKVGoxxjVcqGGS/2UVG9fApYZJAS5+HuXV+JnlvS2ISMydoclq1sH+3imOPVQBKVw9703yK4PNvv7cZECq32W80wB1HOc97lPFyQGdAsf6qsywa/hHmVGqpFbuFdIqDF8xHt31yzjzUq88uVzO9evXadq06TscUSAQfC50dHSoXLkyPj4+eW+g78SoQ+GkXv6VylJjGrndQKFKwqOACZJCocDPzw8nJ6dCjbsoSfSeJkgFoTLnx5WLaJ3XnaigwD6BJElDliqLu5vG8IeHAtCliq0Ur0eNmTG6OlIU+HjfpUadDOY0d6FunQFca70Ot65mmHacxBiRG81qN6aOc1/OOv6PcU1eHf1hxIgRbNq0qYBD1Isp2/8PftVZTosaDWno2IrVxaayeGBZdBr/QM8ni2lerxF1mi3kgYkRYkBSsgYOGevp1HwW1z7wwI5GVZrTtWsrqr2UiIlL9WXhrzosbeFEw/pNmeErQ1/68vP+15+bGNCkHGdSi29p16Qps9OGM/en0m/5BlERur4jVqbFKV6iBN80ciNrwEp+bfjqV5q0ynBW/Fr3lbupnTt30qdPH7S0tF7ZRyAQfFm6du36waa72LNnD7Vq1cLU1PSDlP8xFC9enEePHrFnz56339nQmopW7/+wyN3dnZCQEMzNzd+7rM+S5lOgStVEh0ZqkrI0Go0qTPN3JztN9y2RGpVGo9Fk+Wn+5+SomeKdpVGmxmsS0lQv7azUpMTGaBIz8z/EokWLNBs2bHirsOQJjzRRrxScqYkPD9M8ztBoNCqlRvksHFWmJjPrrYr/IFQZ8Zqox2mal6/Sy14+t4zDAzTl2qzRRCWEa+6GJmjecDk/iJiYGE3btm01qampRXD0z5e1tbUmJCSkqMMQCN5aTEyMxtjYWBMXF1eo5arVak29evU0e/fuLdRyX2fGjBkaQDNr1qyPcjzBx/MJ1CQBYn0sypZE98pCXFu04XfNWOb3LJldi5Hog1+cHU5VpEj0TTHRezlkCQZm5hi/of513LhxhISEcObMmQKHpW1iheUrBcswLV2G4jqAWILkWThiGbJPoIWXWMcUy+J6b6wByvvcQGJSmoplTQpUnV2YkpOT+fXXX1m/fj36+gVslSkQCD5r5ubm/Pjjj4wbN65Qy920aRNpaWm0b9++UMsVfH0+gZ/1F6SVWjJwcjPsGtV60fi6WAeWn22Pld77lS0SiZg7d64wkvNryOr9yq5yhuQ/rNuHo6Ojw6pVq/JuwCkQCL5Y8+bNw8HBgd27d9OtW7f3Li80NJSJEydy8uRJpNJP6idO8Bn6pN5BYosafPvtSwu1zShvXXjHeNHDTZCT2KQitYoqQwJh8EiB4Culr6/P9u3badeuHQYGBrRu3fqdywoLC6NZs2ZMnz4dBweHQoxS8LX6NB63CQQCgeCr5ezszIEDB/jxxx9Zt25dATvZ5Hbp0iUaN27M6NGjGT58+Jt3EAgKQEiSBAKBQFDk6tWrx9mzZ1m3bh1t2rQp8MS0UVFRjBs3jm7durF06VJGjx79gSMVfE2EJEkgEAgEn4SqVaty5coVWrRoQbt27XBxcWH58uVcuXKF1NRUNBoNmZmZ3Lx5k40bN9KzZ0/s7OyQy+UEBATQqVOnoj4FwRfmk2qTJBAIBIKvm5aWFuPHj2fMmDEcOHCAkydPsmnTJm7duoVcLkcqlVKuXDmcnZ1p2LAhq1evxthYGDFR8GGINPk8/HV1dS3wJIQCgaDg4uPjWbduHdbW798roUKFCpw4cUKYFFjwxfvUfo+uXLnCxIkT8fDweL6sYcOGLFq0iDp16hRhZILC8saapHr16nH58uVP6o0pEHzOtLS06N27N0FBQYWSJAkEX4tP7Xeodu3axMXF5VqWkJBArVq1iigiQWF7Y5Lk6enJf//9h1gsNF8SCAqDq6trUYcgEAgKgUQiYdq0afzwww/Pl02bNg2JpKATrAk+dULmIxAIBALBO3J1dcXW1hYAOzu7QhkQU/Dp+GqSJLVajVwuR6VSFXUoAoFAIPhCPKtNAqEW6Uv0xfZuk8vl7Ny5k1OnTuHl5UVwcDBSqZSsrCxKly6Ns7MzLi4u9OnT57OeJVogEAgKIj09vahD+GK1b9+eDh060LZtW+E6fyBSqRSZ7GPPKvoFJkkZGRksWLCA1atXU6tWLbp06cIvv/yCnZ0dWlpaqNVq7t27h7e3N0ePHmXGjBl069aNuXPnYm5uXtThCwRFRxXBtVMxlP62FlZigEySYxORy0ywMM6eNkYd5cPJcHOa1yn9Fl8echIexZGqfNGRViQ1wKykKTqv2SM9LoJ4TCltJiIuIh5MS2Gm/4ErvtPjyD6UGQU6lDqFRyFRYFWRkobvH5syIRhv/zDkxSrh5FgWQzGQmUhUrBxdC6unk3hnkhgVi8LACgtDFYlRsch1LbB6Oll1ZmIUsQoDrCwMX3l9OnbsiJOT03vHKchbpUqVmDt3blGH8UVSKpXIZDLmzZv30Y/9RSVJXl5e/Pjjj9jb23Pp0iVsbGxe2UYsFmNjY4ONjQ09e/YkNjaWhQsX4uDgwKpVq+jSpUsRRC4QFDUV99cO4vtj3bjybXbPHGXg7zSvNYPrFcZz0X8hdWWA6Dp/um4n+MQxRlYq4GMF+QnG1ezMplj180Xi4n3Y83AznfLMklLYOcCGnzWrkW8TMcDmZzSrY3Dv92HnXUzZOQCbnzWsjnHndYdSJ/rwz/SNKIcuYcg3e/i5emHEJidw7U90G7+LuxkiRGoRpnXHs+3AXBoFTqV+y/VoDz+O17KmGGZ6MKVuO7wG+XJ1QjRT67dkvfZwjnsto6lhJh5T6tLOaxC+V6di99LLY2dnh7W1NTdv3vzkeol9KbKysoo6hC9OVlYWvXv3xt3dvUiO/8UkSSdPnqRXr16sWLGCHj16FHg/c3NzFi1aRNeuXenZsycPHz5k1KhRHzBSgeATlHmZFct8cPrftqe1SAq8Nm8nQNcI/bs7+PvUNOq2NUBs2Zkezr8yfeVFBi1v8tqaoFdJqDR0L+7jqyEFRBIDLGSJPAh8CN9UpbwJJIXdIkJlRRXrAlSpq5MID4pBWkKPmKDHlKhVk9KyFB54eXFfUwYn54oYSwDSiPDz4lasCAu7WtQorf+0gDTCfby4m14C+9r2WLw0v7I68QE3H8I3VctjQhJhtyJQWVmjf24lM9Z60a1tFMkVmjN1716o9voy1UnhBMVI+aaUgjs+kejaOmFvmfuqKQMXM2jcIQwG7uf+/FZoX/6N9h2XM2PDj5x0BsgieO045ve8xLyar16KrOC1jJvfk0t5rXxJUFAQixcvRir9Yr76BV+4ffv2kZiYWGTH/yIabp87d45evXqxd+/et0qQcqpXrx7nz59n6dKlrF69upAjFAg+bQrPPRx6WI0mTZ+OXJx2ho0771Np0CJ+rhLNvn8OEg+AEU2aOhB5cBeXM9/uGMqMJOJiY4mNjSNVaoBe2gFGOzszan8KIOfExAY4D9pOjgqn10vbzwhHJ+o4OVK72QD+unmDdV2rYt9mIMO6OlGl1SL80uM4OMSR6u3HMnfmYJpVcWTY4QRI8+KPlpWwcelKrw5OVKw5gP9Cc3foSDswGmfnUWSHdoKJDZwZtOUsS2duIzLrNiu7/cSmuyeZ06ULs0+lvbbMtP0jcHRuQsOGrej9Q2tq2DZj/nVljiOpCD16BF+NC0OmtKOcngyrFnM4ER7NhUlVkACIzSlV4jYrxi7huuLlCyHGvFQJbq8Yy5JXVwoEgvf02SdJiYmJ9OnTh61bt+Li4vJeZZUtW5ZTp04xbdo0AgMDCylCgeBTpybhxk0eGltTsUT2V0Likc3sj7anW58f+NG1JqnHNvJfmBoQY1rBGtOYmwREFySbeUbF/Y0/0bBePerVc6Hvurso37xTLumhnhw/epSjx85y47EaNHJEzdYRFnWKUcmrmXdEn6Hut7h9bQHVPf9g6eFALpwLRb9KC3r+soytO1YyuKaMqG3TmX2pHNOuRhAVspceii1MmH8K+ZsCkDoybelAysoc+O3ScUaVefb1qc63TE2GjFarr3Pb63eaZvpy7kpCrusSFxcPusUpUexZeTqYFjfk+dMySTn6zRlKGb9FjF1zj9zpnIRy/eYwtIwfi8au4Z7QeVcgKFSffZI0btw4OnToQKtWrQqlPGtraxYsWEDfvn2F4QIEXwkNaWkZoKuPnghQR7F38xEea8LY2qcuPTc9QJN2nk1bb6MCRHp66CAnQ/7aGY3yIKHKhEukZmaSmZmO13SH58/61ersZEulUvH6ElVE7x5P+7ZtadvuB5b6qAAJlevUx7K4KdoRD4lRPmRbvxrYNV9GWCkL1Ok2DJ8/mupRmxjbvS3te/zMrIMhhN8LI9PCkQa2eoiLN6KBnYSYB/d58krOpyY7NBUqVX7nqsy/TElZbCvrIDYqjql2diPUF4eSYGFphij1EQ/jni5V3We/22K2e8U+3U6Ebp2pLO5XAo95szmSnDsWkW4dpi7uRwmPecw+kpzPNRR8ilJSUvD19X3r/S5cuEA+s4oJCslnnSSFh4dz8OBBFixYUKjl9u/fH7FYzLFjxwq1XIHg0yShuEUJtJ4kkaQGVegOtp7JpFrP8Qz/qS8/DZ3AT7W18du6EU8FqBMTeaJlQUnztxwPRiRBSyZDJpMiEQMSXXRkaqLDQkmL9+TanfweF0koO2gnQffucS/kCvNdtAARMm0dxIDMxoay0hK0XniB64em0bV5Fzo5xnLh6hNqz/Eh+v4ZJld7xJG9l7GuUl/I8hgAACAASURBVAm9Rx4cuhpH+oP9HPVTUa5adYxzfBtKdHWQqaMJC00j3vMaz0MTixFpMslMz0Tx/B5K+oYyxWRPWJBXY2kJZdt3o4HuJVZMWMP5W7c5v2IcI3+dyd9XU18kPCJjWs5aSK9iUUQmv/zDKMK45SwW9ipGVKSQJH1OHj58yPDhw7GysnrrfdVqNaNGjUKpfNs6WcHb+KyTpNWrV9OnTx8MDQ0LtVyRSMTIkSNZsWJFoZYrEHyqDJ1rY5cexK2wTG5u/pfL4sb8PG8iY8eOZezYCSwY1w7jkB1sOJlM6I07pFapTe1i73lQnUZ8/305bs+tg1WNqdwpVgItsSjPVAJAYmRFeWtrrMuXxUwve9mzTlqyuuNYOMySwz2+wdBuCP9FmWJdvhylJIGs/qE69k16sPyOLf2HdsKsxxz+6JLJ+hZWFLMZwAXrCayaVI+czcV1Gn3P9+VuM7eOFTWm3qFYCS3EIhHSclWx1bvDwsaNmXf7WZYkLlCZryOpNJy//x6K5cVxNK1qR9OJFynRewUrh1iTMw0Vm3Vk3vwuWOaVm4rN6DhvPl3yXCn4VI0aNYq5c+e+U5LUpEkTHB0dWb9+/QeITPCMSJNPfZ2rqysikYht27Z9knO3Va5cme3bt1Oz5rNeHZn4rR/P0gtPMKo/nJmtI1g8253UOsOZPcSZt0ml5HI5xYsXJyYmBgMDgw8RvuAr5erqSps2bbCwsKBNmzbvXV6FChU4ceIEFSpUePdCVMEsblaf465+HBliBRoRUmnOH1w1KqUaiGFt2+psa3GJ8xMrU7CfZBWK9EzUUh10ZC9/jyhJjXuCqLhpwcYmetOR0hJIUBpgZvQiPcmMf8D9GBUmZa2x1H/Rjig95gER6caUK18c7bwKU6YS90REcVP9XHeTyqQIQpP0KF3u5f0KUGa+wacQFRpDlklpypi+9d5vNHr0aKRSKb///vuX27st5SqrJq/GM1ULXR0xyowsvukyl5kdDbm6ajKrPdMw+3YC8/rYZSewyVdYOWUNnimmtJj0B33tpKRdXcn4P6+RbtKAkXOHUOsDff0HBwczYcIEDhw48M5lZGRk0KBBg3d6XPe52LdvH9ra2nh4eBTJOEmfXuZTQE+ePOHhw4dUq1Ytx1JtqjewImj3DnxTTRFHeXMprCI9er1dggSgo6NDtWrVvug3n0DwnKQSAyZ1IeLfLQQhfSlBAhAjkUrh7ha2hXVk4sCCJkgAEmR6enkkSABSDMwKJ0ECkOib5kqQALSLl6eKXcUcCRKAGD2LCtjml8xIDTB7KUECkBqXpuIrCVIBy8w3eEOsKlT8IAnSV0PPmCfe29kdU5vZf66gv1UYUSJjQA/jJ95s3xVBuSaVX9TwGZiQetOd/ds3cuyWApQ3WTV9Hv/+u4NbJRpS8wPeH3t4eNC6dev3KkNXVxcrKytiY2MLKSrByz7bJCkgIAB7e/tX7ogklfvQt5E23lvG0G/OE8Zs/Y267/g0ztHREX9//0KIViD49Bm3Xc31079QOZ/sR1JpHKcC19NBmMlH8CnKCOB6MNjWro3ubQ9UQ0+yroMekEHA9WCoVI8GVjl+9tKvc0PkjLOpnLiYJCL+ncdpAyfMpaWo06Bi7huBtAj8zh7l2IUbxORoPqdOiyMqMed4GCpSYmNIVuS/7vr16y9meVCEsGPyUFZ5KUAVys7pK7iiUBN3fjHDp+/P95TNzc25fv36u10vwRt9tknSkydPMDExeXWFuBRdurugdcsf0+EL6fj0A5ESFUrsW06pY2JiwoMHDwgJCXmrv7S0tEI4Q4HgY5Mg09HOv4ZIIkNHJrR7EXyaFAHX8EsRk351IR27LsL/WaM1RQDX/FIp7lSbKjnuqxXXvQgv1xQnc3h8/z/m/1eC7+0fE6FTkzqOz2r0VEQc/IVGtToy7d+jHFo/nCb1f2ZvpBpQ4ju/ORUaz8LrWeKUeYHfGv/EtgRFPuvUpKamoqurm71cVoYyyiucupmBOuYEm/50xzceDOWhPBSXy/ec9fT0SE1NLaQrKHjZZ/tgWiwWP+86nJM6/gxu+x9TxjgWj7P+ZLZrgDZyTs7oj3/fY8yqX/AJ8tRqNVu2bOHw4cMF3ufu3buMHTsWNze3Au8jEAheLyUqlAyjcpjr5VioSiE6RolpSZMXj07S44lWGGBpLDyu+jqpibrmTZhWE5ZtnI/hX0ewL5l9k6yOuoZ3mJTqY51zNKZXEXktEJPablg/nMHdDWupsfYA0u3/gF1H6jx9AqEK/pOB42/Rbe9lRtnrAJlcm1yPYWuu02VmKXz9lTganeTPw+P5p7MJqkgfAnUc6G2c9Pp1pmJu6OiQmfmslklKyZIleBIfg+/OUKwaS4h9FM2B49r0nVw937OWy+Voawvv+Q/ls61JsrCw4OHDh7kXpvmxZIAb2r8exK2bFfd3buJMGoCM5lPXM8xBhjo5gtC4VGJuXOT8tfsk5TMUUmRkJL///jvBwcEF/luxYsUbu2Tu37+fsWPHEhIS8t7XoSCSk5NZt24dv/zyy0c5nkBQeLJvcFb65x4eQB29kR87LeFW5gO2jZjOITmk7x1Ms/9d5i0HAhd8MdK55hmIulJtapuVpdf/hlL9aTVA+jVPAtU21K5TAqK30b/PaiAdTx8FNeqWpVQpY7SdR/K/7yLw9JVT0qkOZSUACrw3/EV8j3kMs382nYw2tWdf5fLMmpDli/fdSvSd3pzba7ZwXwUZ3j5EVHGiqiifdbLsufReTLchxtzKnPQb69ilbk//Khoe7F+NT80RdJTdYPeafQSEnGPDmoPceekNnpiYSNWqVT/KFf4afbZJUtWqVQkNDX1RzZjmw3LXjkyPbUSvutro6hlC5E7mrvBHgRz3Me2Zey0TuftoGjduS/+F//HvjHbU6beLmNcMHOzl5UWtWrUKPfasrCyqVq1KxYoV36+g9Ptccnfn8oP8xwo2MjLi22+/JT39LZ83CgQfQ8ojQiJTsv+dHs29sMTsUaUV8YSGpT6/wQFQJ4Vw5exl7j4d+TEz2psTJ68RFBJN9rtbSULQFc5dDiZRGAv2q6K4sYH1p5PQKBJJyDkyqOIGG9afJolUriwfTLe24/D+xonMgL/468xjUlM1NJywk+N/D4A9azkcqYb0J08HF03memAqjvXtcj92kcqQiUF1z4ebRg7UdhnA98qt/OWTxg3vW5R2dEKWzzpdoG7dupw9e/ZFkVZmpPhpaD2oNuUsNFwOtmV4r3LI4zTIb6zF7ZIhFaM2subSiyxJqVRy9+5dypQp84Gv7tfrs33cJpPJqF69OpcuXcoebVvfiVGHwnk2NW1ltxsonj/xyp0cKO2H8e/mHhgnb6JLnbMEZnV/ZYLL6OhoIiMjP+0MXZbG6T/3UnVH+6KORCB4Z6pHm+k/UM26c79ivKUfjosrc+jGEmq4j6Xbme8ZEz0RzxF+LLH6h249tlKsuSNynyMEKHsQ732RW4n3SdhzhdblNTw+Mo0hyY5YxZ1hiOVcPP7ugtnTW0FFyA6mL46ny7LhOD7aySz30kwZXpErS2ZwpsIUZnYqWbQXQvBeZPajOBGbx+TkMntGnYjl1TW1ORU19um/G2EK0GcXYX1ybKJWo1KJEecaDSOOC5sPoGnVHwdfX2Ltf8JW2xqrfuVo889BLMIUVJtQivR81knI7hj06NEjnjx5QrFixZA1mIuHhw7GBhIYuBOfQUYYSQBrU5JTbOjhao/qNxmWli9+to8fP07Xrl0RiV43upjgfX22NUkAffv2Ze3atW+5lwgjCwt0AbR00Rar8pxDav369fTs2fODjieSHrCG0Qs9UCivs3buDsLkd/h7zNynjwgLIDWA4GL1cSncsTQFgo9KUqEdzTUXOf0omXPnU7Ap5sO54BQuHg/CqV3dp1spuLp+DVk/72Lj4uVsW96bMmIxJdt0o75FdVwndKa8GLScRrF58yrWbp+EnecZArNeHEdWpgzKK6e4maEm5sQm/nT3JR5D5KEPEZczL5JzF3zixMVxrmXIxX3nSFADqIk9MYMxK+8iNVIS4H0HaycnZIgx7zyYen5/8E9IZZwcRPmsy74jF4lELFq0iF9++YWsrCyQGmBs8PT3RscIo2c37umeXLsVz80dSzhi1o8BT7ufhoWFsXXrVsaPH//RL8vX5LNOknr37s358+e5e/duoZablpbGmjVrGD58eKGW+zK9iiYk+gaRdOM8B455EiqPJFK/MnZxO5i1yueN+2f6+5BexRlTMYCK8B2zWOUjDFEv+MxIq9CuSQrnTxznTEx9xnfV4srJwxy7WY12jZ+11lYSFZWJVRlTxIC0QgXKvHL/IsLIwjL7Bkimhw5Z5GrJJC1JyRJPiI/xZWeoFY0lsTyKPsBx7b4Mq/7ZVqoLPigpjuOW4Boykrr12tKxVT1a/w7jtsykgU4UPgFaODiVyP4h1W/CoNYi7ho64GSQz7ocI9XXrFmTyZMnc+7cuddGoPC7RlanSQzpOYE/Jrd6XjN6/vx5NmzYIAx2/IF91t8MBgYGTJ06lf79+3Pu3DkkksLpmjxp0iSaN29O9er59yp4b1JLzKVH2XbOhm+rpuP13zXK9p2IpXk037fRR5kUR4pISXRYCiVsK2GW65GgigjvUMxqVUaquM2Fa0bUcvmeNvpqkuISESmjCUspgW0ls3cb2E4g+GikVG9bn0djlxJdczqLv5WxvP88rtSczlx9yB6PWIZtJSOW+txF0dYedUAgd5U6gAiRSE2B5vkUm2Nlns7xdbuo1rU/VXzXsH91BjVHzEd2Yzdr7lWihYk3p57Up1+HysLnRgCA2LQx004EM+2VNWUZeybnOHpSHGb4kDYj+3/V8lmXk7W1NdbW1q8//jff0b2+Dmjn/n378ccf3+IsBO/qs65Jguyh9sViMbNnz85nKz1677nFiqba6PXew60VTbO/APW+Z/uttbTO8W144MABDhw4wLJlyz5w5IDEErMET1LrdaO2UQiXxZ34oZIEVegmfnO7TPCqXjTuM5cdu2fSxXUFIYpbuP00JXtfeRD7joWTensji8eMZ0eMHqGbfsPt8i1W9WpMn7k72D2zC64rPk4POoHgfcic2uIYH4JF43oYVmtKzZRorNs0zzFSvhT74bNpfOYHGrVpS9tZF8gSixBJylDV2ocpraZz6Y2VqFKszFLw07RmUO1yWGguE2w7nF7l5MRp5NxY68Ylw4pEbVzDJaGLnOATIS3fiM5NbRHqi4rGZ12TBNnjJW3fvp2mTZuira3N5MmT37msQ4cOMXjwYA4dOpT3QJX5uHPnDgqFgoCAAIKDgwkICACgWrVqr29UJ7FhzK7DSPX1wOEwO3R0kAIvOuXIqD1gETPb3UXaeRW3o0rxpIJL9iodOyac8n663c+AilvPprWR1WbAopm0uyul86og4D170QkEH5p2E5aHPH76n+asevBimoXee27RG4BvmXfWh9SEVKTGxug8vcVz2H+ffiqQSMTc+unpTnrfs/3W9y8dREaDuR546BiT3TbWh0FGRmS3jU0mxaYHrvYqfpNZYvnZfzO+3vXr17/cudsEX5zw8HBsbGyK7PhfxCelZMmSnD17lmbNmhEUFMTSpUsxNjYu8P5ZWVnMnz+fVatWcejQIZydnd86hjVr1uSqfXJwcKBNmzYcOXIkn73EaOs/bXOho/PqSMcibXR0RCCSIhVrEFt0Ycb/3hyLSFuH7N2kiAvyGEIg+GxIMTB9+bMtpqBP2qUGxs/vyHWMjJ4vT/e8xq14OTuWRGPWb2K+U7N8ztq3by9MYfEBREVFkZaWhqWlpdBGqJAZGhpibW2Nh4dHkRz/i0iSIDtR8vb2ZuLEiVSvXp2JEyfSt29fDA1f3/UrKyuLffv2MX/+fCwtLfH19aVUqVLvdPxJkyaxdu1a5PIXYxZNnz49z21Lly5NeHg4ISEh7z9WUgEkJyfj4eHxQcZ8Egg+fwr8rmXRadIQetobvNz044uRlpbG/v35zwMmeDeHDx8mJCSEdu3afZTv9K+Nv79/kQ3HI9JoXt/k0dXVFZFIxLZt2xCLP5/mS5cvX2bp0qWcOnWKZs2aUatWLezt7dHT00OhUBAcHIy3tzenT5+mUqVKjBw5ks6dO7/3WBNjxox5Xpv05lokwdfK1dWVNm3aYGFhQZs2bd67vAoVKnDixAkqVKhQCNF9jZQ8uOBOqMW3NLUVagEEb6979+7s3r2bvXv30rlz56IOR1CIvpiapJzq169P/fr1efToEefPn8fLy4tVq1Yhl8vR0tLC2tqaxo0bM2XKFGxtbQvtuDlrk15XiyQQCD41Uso36kz5og5DIBB8cgotSVKr1dy+fZv4+Pg3zl32MVlYWPDdd9/x3XffvbIuMjKSyMjIQj1e27ZtefjwIWlpaZw5c6ZQy/5c6OjoYGVlRfnyws+OQCAQCN6Nt7c38fHxlChRAicnpyKJ4b2TpMTERDZu3MiqVavQaDRYWVkhk8nevOMXKjMzE5VKxZw5c4o6lCKTkZHB/fv3sbGxYcSIEXTp0uWrfk8IBAKBoOA0Gg1ubm5MnDgRtTp7Hr7hw4ezdOnSj94z872OduPGDb799luaNm3Kli1bqFu3rjCHjADInnjx4MGDLFu2jMWLF3PixIm3HlZBIBAIBF+fTZs2MX78eLp168aoUaPYs2cPy5Yt48mTJ2zevPmjxvLOSdKzBMnNzY2ePXsWZkyCL4BUKqVLly507tyZsWPH0rJlSyFREggEAkG+1Go1CxcupGbNmuzcuRORSISLiwuGhobMmTOHoUOHUq9evY8Wzzt1WVOpVHTo0IE//vhDSJAE+RKJRCxZsgRnZ2eGDRtW1OEIBAJBofn3339p0KABu3fvBqBLly40bNiQ6OjoIo7s83XmzBlu377NhAkTcj2Zmjx5MiVLlmTixIn57h8UFIS3tzehoaGFEs87JUmHDh3CwsKC3r17F0oQgi+bSCRi/vz5HDt2jEePHhV1OAKBQFAoWrdu/Xx2hWdKliyJpaVlEUX0+fPy8gKgQ4cOuZbr6ekxfvx4PDw8CAsLy3PfNWvWYG9vj7OzM+XLl2fevHnkM8pRgbxTkrRy5UpGjBjxXgd+PTkJjyIIDw8nIuIRsSmKN+9SVNLv4XMjDnVKNI8SCyPOdOKjk/g400apSYuLIVmpItLPj0jVm/d4H0ZGRvTs2ZO1a9e+dpuUqFBi07P/nR7uxcnjV3mQ9mHjEggEgndVvHhxRo0alWvZtGmvToUrKLigoCDKlCmDvr7+K+uejSt3+vTpV9Zt3ryZoUOH0rJlSw4cOICrqytTpkx5Y83Tm7xTknT58mXat2//Xgd+LfkJxtUsR9myZSlTphSWxqaUbzWdUzHqD3O8fKlJ9PmbUSPXcvuVUQ3SuDhnMnsSdXn87wA6ut3kvQc+SN/L4Gb/4/LHyJLUj9ny03csClShdXcN45ffev/436B9+/ZcuXLlNWvlnJzRn5X+Ckg9yIgW/Vnlfo3QzKJ43d+eOs6Dvxf+zrypM9jsn1LU4QgEgo9k3Lhxz6ci6d69O/b29kUc0eft7t27r52rzdbWllKlSnHq1Klcy9VqNQsWLMDR0ZGDBw/SoUMHtm/fzuDBg1myZAm3b99+53jeOknKysoiMzMz3+k+3p+ESkP3cyfkFp67x1DOZx4/TT5EEmqSwm8T9CiOCD8P/CKypwBJC/fh3OmL3Ih5kV2kxoQSFpdOZnwQfgHhpOSsKUkLx+fcaS7eiHlaa6Mm8UEggQ8SUQPqpDBuBN4nPi2OMytnsPbsHcKiknNFqHrwD4tuN2ZIg6dzr2kyeOh/iat34rLLzIzl/oN4ntUvqZPCuR8jz1UG6lTCvM9xzvM+yTlyAWVCEFfOXSY48VnQShKCrnD6+Fm8w1Of7ptMRGgcqTE3uHj+GveTVICa5IhQ4lJjuHHxPNfuJ72YLDc1FK+zp7kSkmMZAGLMOw3A+tAC9sUX7NV5V6ampiQmJr5mrYzmU9czzEFN9PVrBBm2YNCYntQ3/VRGelfxYNtStofmVeWWwL4JvxHoPIJJ/Y3YPHAOVz7hClCBQFB4ctYmCbVI708sFuc5w8fjx49RqVRUr16dkJCQXOtOnTrF7du3mThxIpKnEzk+a+ZhaGj4XhPfv3XvNrVajUQi+eBd/SWGlpSvUAVZhalMOrKRDkcPcTWzKdEjHBnhZ4ZudDSlf7vAWoNpdJrqgdxYm9R0K3r/dYR1rsXZPagKo4JsKJUaS0x8IvotFnN4zwgqBPxB505T8ZAbo52ajlXvvziyri3nRjvzs2Y1Me790DoxkQY/Pmbm7tps2BZJlnIl3X4qT+rpZ9WqKu7t3EN6o42UFsNjVIRtHceYuAaUuHucIJd1HBsfx5zvttD03E76mMk5ObkLm5uc5F9XnewilHdY930PNuo0p770KqPTB7DvHxmax0eYNiQZR6s4zgyxZK7HCixXfccor2q0qq7g6sjR1Fh3lcW13RndeB6P7WtRvdRjTg8uwawLf5I5ujHzHttTq3opHp8eTIlZF9hQ3Z0ePTcga9IAXf9JzOvwN7tHWb242LIatHO8zaJD8XTvW/yDvaZSqfT5mBevkuM+pj2eQ/fT4txVImMV7Nvni+MvrbASA6pITv/5B9tjmzGm2UM2/utPyZ8WMq5hjslOVSlExygxLWlCYYzKpE4O53ZYIkoNoH7AZvcIXKrcIFBThqrlTV7cYcgvceRKKVqt0kei3ZDa6lGcDlFRz+4LnQRM8Fk6ceJEPp8/wftwcHCgZcuWPHz4kIcPHxZ1OJ+1zMxM4uLiOHbsWK7lq1evxt3d/Xkbo1atWmFnZ4ebmxtXrlxBLBbTqVOnXPuYmpoyatQoZs+ezePHjylRosRbx/Op3KbnQ0oJcxNIfEyCCkCDXNSMdWFRnOgTyPTZlyg37SoRUSHs7aFgy4T5nJJnb5ehbsDSm494cGQweifmsuz4A7ZNn82lctO4GhFFyN4eKLZMYP4ped5HdpzG0oFlkTn8xqXjOZ87y/H1i6Z8FcunF1CEcasZ/Lt2MX/tm0m5nX9yVNSKH5reZfeBGNSpp9jlVYMe7V50f1dcXcef8p/ZvXUxi/7ZxsI2JsiVgJYTozZvZtXa7Uyy8+TM9SdI7Ubz138rmDVlCiObqPDzi80uRGnPsH83s3Lddn6teo2zgVmAEvth/7J55Tq2/1qVa2d9uLBiMUm9N7Bq2hQWbxyObM0yjqfnPFMZdnalCPK9UVgv2ruTlKX9wHZUKNeKX8Y9TZAAdcoTLFrWR7FvKduetGF6dwkbVp3Itas6eiM/dlrCrUJ6bqiO9GT/rl3s2rWLLbPmckVLG+89u9jtcZ+snBumxxCjNKCYFBCZYmL4hPj4D9zISyB4S//88w9qtVr4+wB/BgYGDB8+vMjj+BL+dHV1USqVuZbJ5XKKFSuWqxH2iRMnqFq1KiKRiDt37mBtbY22tvYr7/t27dqh0Wg4e/bsO31u3mmcpPdtLf52FERHJ4CpGcUlEAVIKtehvmVxinndIyzTgmYNbNETa9OogR2SzQ+4n6xGF5BUqkUdYzFGzk5UEq8n8uFd7oVlYtGsAbZ6YrQbNcBOspkH959QD0CtRg2gUqHK9xQzSUmTYmDwrDZNglW58ugA6JehtF4c0U9kdOjdmkkz9nDL8CoBdfuxMsfcmcrIR2SUakFxMUBZWg0oC+lbERlZYKkLIENPB7KUMojZy5hmfyAq8Q3mKemobbODExlZYKELoIWuthiVEhAZYZG9EC1dbcSqTB4+iua+3wT6nnsar10NpC89DpIZ6KFM/bBtad7nfSM2rkLliJ08+KYLc9qXR3ZchZZedq2cOikET79YTC2f3SWrSAx/iFInnZAwXao5l8MgLQJ/n7ukW9Sgjq0pEtKIjVKgRySB4VpUdLLF7KVPg7hUHTp1r4RSk8LJ0Ah6DXXFRQ/ExcqglXNDLRlaikzkGgAFCqUuBgafwf2H4Ktibm7O6dOnUSqVn9WE5YKvS82aNXF0dOTkyZMARERE0L9/f7755hsqVapEcHAwAA0bNqR///4ABAcHU6lSpTzLc3JywsjIiNOnT9O9e/e3juetkySZTIZGo0GhUHzQqSbk0Te54pGJ/MF+5u+No3TPbjTQht2ASKaNjhik1lWopPcIj0NXiatdhmNH/VCV+57qJmLuAsrrJzkS3pvmvhe4obakjY0dVSrp8cjjEFfjalPm2FH8VOX4vnpxdM/KUN8LIzQtnqRrd1BgAYBYLEKTmUl6pgKkz85XDwszNZceP/tRVhIaeJ0ktS0mUb4EZFWkeXExMvMfaJ82mt82KmjyPxd0cl7HChUwXBXAA2VbKqu8mN1lFdYrm+VxIY6y6I8MBvlf5gfTFNwHOPCH+m2qzKVUrFiGUnUWsndyFcTxx1gwL5JvDCD8+TZqnsQlYviBu62mpKS8R1s2NQlXfRHVn0tJsZyrZ+5SvU1DlHfW0K3HVoo1d0Tuc4QAZS9Qx/PfkCYsTSpJKcuW/DatJGv6b0DcuAH6/mP437frODg2hJ/rzCXOoT7OJcM5M6khqw9Pp16ODhXqh1fYs92P1IcXOJPkTIsD29kOSCt3ZWp55xevp25lKptt4G6kCkqFEpZhTxubL3LuaMEXYPHixR99ageB4F3t27cPyG5jNGDAACZNmoRUKmXNmjW5kn0tLa1X9lUqlUilUmrUqPHOjbff+pMiEomwtv4/e2cdFlX2xvHPBMNIi6RJiYWioqjYtfZPUddeE1vXXF27dW3Xdu1Y7MRY7EbKFgsURQmRkh5mht8fhICiYiy43s/z+Dx4z7nnvPfOzLnfe8573tcKX1/fbxj1UsXTbf1ouF2Chl4xqrRdws65TdEhNsMIAMTGXZi90J2fxzTBfFkqYuPa/LZ9PLVk8BgQafqzxFGf3q81sJ+bwwAAIABJREFUsOmxljGNilO86ELcfx5DE/NlpIqNqf3bdsbX0kMV0gmLnnOoYb6Z6rVLYKQhRiSSYlGhDFqrF1C/vhyFz/R0+2TUrFuGBT53UbarBiIp8kdr6dhiC6kRCVSasY0GmgAV6N5WyeK/mzDTMbuglFYdzgzHLnStf4liknCUDf9gqHEwB3LeCo1K1LObw1KX/pzXjiYhTIdog7wEKtOg5ujZOHbrQu2LFuiGB2PQez2jpXA1s04iXr4RVO9UPg/t5p3r169jZWX1mWcr8Pa8R3jcITYseo23xkhmt9fh2m9rSRl0gi2DzFHfNsSpb7qATFVR43d3traVcXGkA7Euxzk+tATimEP0rrGYQ73bgroiQ7ZupKthLG79arHcbSS1uuhn9iit0Jmp89pyeeoYrGYtYZBFLj5GUgcGjLZi0OSZUNwf1YAJtNJ5f1UBAQEBgbzx+vVrnj9/jq2tLWKxmLZt2+Lv75/pwK1QKAgICODw4cOULl2a8uXTnmUdO3bk0qVLREZGAtC9e3fKlSvHpEmTPtmv+rNeJwYMGMDq1au/jUiS/48tYSq2vLdQl95Hkuid+X9NKri4crfbUp4GJWBgYUkRTSBdTEnKD+X07tYkxGljZqKV5j9UwQXXu91Y+jSIBAMLLIukr2F23IBf83m8ERXBUDvLVPQgN562CiRaq0QWO8SYtHehYpf9+CRXo+bAozwcCIrYKBRahdHJ+iwVaVGze3cq5rzTYjNaLTpPs7gIYsWFKayV1ud+v4wAnVp02ulHJ4AWPrR/EobY3AKzQiqSUiSgWfv9dZv7kXm00078OqX9vfh8C+IiokjVNUY3Xa8NOpYWtIvoIxwObsGQhlrvvetfA7VazerVq/n7779zqaFFj/0Zto/izKUcxcr7eD52ZNqhUbTSlDNISwIkcC0kGfPGhogBsbU1JaWP0+pLTClRXA4oCAlVUPwnk7TPX9say8IRhMaokRQvS1k9gEJYltIl7GUkoJ+jYzWWvSdSNTeBlNYZlj02caJjDLHooC8XHLYFBAQEvhYikQiRSISxsTFt27ala9eu2WZDtbW1iYiIIDExkePHj2eKpH79+nH48OHMeq6urqxbty5PG88+a2G6T58+HD16lICAgM85/asj1jLFukyGQAIQIRJLkIhFiLSMKZohkN6egKl1mbcCKR2pjnF2gZR2FIMSNljkqIt+M8Z3CmfLnrcRpGW6WQSS6hmbe1WlxR47xvWzJbfHplSnSKZAyv0CdShqY42ZtgTEMuSan/MQlqJT5K1AeouSexsOoTd8BFW+3eop+/bto3DhwtSoUeOzzleHX8MrRIwkqRC6WhnXL6OMrT73fR+jAJJu3+FxFqdtkTitjk1pXfx8HqEA1OE+3IwqibWxGNWL+9yPAdRR3PGLx9LW5D09a1HMqhifIh/Fcn1BIAkICAh8ZWJiYggNDeXixYvY2Njg5eXFxYsXM//VqlWLChUqvOPO0aJFC8zN3+7kdnJywsXFJU99f9ZMkqGhIXPmzKFJkyacO3cOCwuLz2nmG6JDr0Px9PqmfUiw6LOO5bnFw5GUoufqC/ysaYBOgV7+l1L21w0slsm+2VbH06dPM2zYMA4ePPjZoSPExj3YftEZqUFWK6XYDZ1F/U7dqXe1GFpEkiJuRvYepFQZNou6XXpQ92oJCr2OwnrSZlpre7FN4c3CDv/jkFYoT/VHsqPFuxFeBQQEBATyFysrK6pXr87cuXNzrbNp06Z3jkmlUnr37s28efPe68f0KXz243vIkCGo1WoaNGjA0qVLadOmzQ/oDCjhQ77rEm0DvgfXFMk3csCPi4tjy5YtzJw5k/3791O7du3Pb0yqi7HZu07fYuOmzD3nS1xkHFIDA+Tp33/7jKVEQGzagvnnmhIXEUOqfhF0pUCCFyKjNiw+MAmrRDnFixl8/o9BQEBAQCDfCQ4O5sGDB+zZsyfzWEYevZYtW3L//n2kUinlypX75Da/6LkwbNgwihcvzoIFCxg5ciRdu3bF3Nz8vbEKBH4MUlNTSUxM5PHjx+zevZuGDRvyzz//ULVq1W/YqxQdQ4OP1ynybqBMkbYZFobfxioBAQEBgX8PiUSChoZGtrxv1tbWNGnShJ49e5KYmMjFixf/PZEE0K5dO9q1a4evry9ubm74+/uTkpLy8RP/o3h4eFCkSJFcYzb8CBQqVAhLS0tu3rxJyZIl89uc96PVI4vjuwAAqiC8TodRomlFtF5HkaBKRSQSI5bpYGiojRRQh/hy6rkJjWuUyMPgkURkcDhxShBJJMi0DDAqrJWrn94HSQgnKAIMixVBHepPCObYFv3SFElq4l/e43ZADBrFylPJ2vALI7YnEJ5mJMbv+DhmJZ7QJ68Qm1li8u32TAgI/DDExMQQEhKCl5dXtuM1a9Zk+/bt9O/fn/j4vGVN/2orDA4ODjg4OHyt5r5bBg4cSLVq1ejfv39+myIgkAdUPFnXn07/dMSjbjAjKjqz9VV6OAWRmELFGjFp5z4m2NxideedPDr5D8NtP1HmJJ1kdJWs7WlgXONXNh9cQCuzvPkHxO7pR+lBqawJc0U0qBKDUteQ5NYnT21kJ5rzU1vx8zwPopGgUsmw6bKK49t7Y/XGl83TtqAcvJSB5fIwVMbuoV/pQaSuCcOtT87dkhmoeLlvMHW6X6GNux/LGwiz7wICX4qtrS0qlYqxY8e+U7Z06dLPalMIuyogIADJV1nxpy8Ondqnp4JJSzL9MMCfh75/003rAvP/PEGimTNdqt9ixcpLvD+ZT25kJK2+j+eOnhTxXc6MDQ/Sy+J57nueM5fukiVH9QeOA2jRePIBDkxpApmJr2OJC/TmwpW7hGYxLiH4Fpcu3uRl7CsC7jwgJGtKnthjLF/qjdWkG0QnR3FjdnXCj/3JZu9Yws+uZPq6czx4FkKMCkgO5e5Fd/45cw3/qLdpZ5SRj7h2/jxeATmTRwOKCJ76+eEfmuXtVRXIkQnNceq1kyAhldp/ioMHDxIWFpbfZuSKl5cXN27cyG8zvisEkSQgIIDCcz9HX1SkQcO3vl1J4QHcuXWbWzfv8jxWi0rVKqCJPg0a2vPyyF6uviNcPkxa0uqyVHawxVgCCkUyxHuz8CdbStftQLf/OWBTpR+7A1W5H88kgTOz29N+1mkgnkPDqlK9QR3qNOtB9+aVKdNoHreUakIODaZ62Vp07NeZBrXqUKdaO5bczLIlVbMkFkXh5vqB9Bu7khsOqwmIvMGcqn78OcOVlyn3WdmxN1v9PZhRrxy1uo1mXN9mVKjYA9cQJZGnx1PLxp5Wv/SgWYWyNF92+23bqhfsdqlBhWaTOBuVZSZK+Zw7QWWZsGooQg7k/w4zZsxArVZjamqa36bkSvXq1dm9ezfHjx/Pb1O+GwSRJCDww6Mm8u49XhhYYWOUMSSoCDkxm8EDBzBoxELOJlhhZ6UNiDG0tsIw7B63Q/MyDaLGf107rIubY2o/gWsGTejfpQIhrtOYdcWCqdeCCPE/QBfFdn6bd5Kn7z1++gOzV6kkypqx5tZ9vOc3JPn6eTzCA/l78WZeOC3B++F9Lo23IyVn+kBZXWbu3cCwakqubJhC3xaVsK45jlOx1Zm2zIVSMnsmXnHn12JFqDtyDUcvnGLHrLYUf+XBlTv+bJ29gkeOS7nx7AmX1vXBXhyRef/ur+hM/wNFGHdgBwPKZVlO06zHpB0rcLHX5fMCYggUNHx8fAgNDaVDhw75bcoHEYlEzJ07lwULFpCYmJjf5nwXCCJJQOCHJ5X4+EQopI1W5lNbgs3QE7x4FU5EdDAHOkSyafAUDseDSEsLOUkkJuUlYbEY0wZDmf3HYtbuOsPt+0cZWh6eBzwj2bQqtctoIS5Sj9rlJYQ99ee+//uOP+HNB3SZpFQZysrF6BcxRBMlyuQQwsJTMbAqjalYjGEZW0xzjHiqoEscvJxK27VeBIW/wHdzN4xvrmL54egcFV/ju3cuPRo2Zei2u8SjRqV8yfNgJYbWtpiKZdj1mseiXxumn6DkaUAwEtULHgTEIqyq5Z3Ya6sY1qsnvV0GMmhAP3r1mcbhYDXEXmPVsF707DOW7X5vZwVjPFYyrNcv9Bq1FT8lQDzXVg6mZ89ejFjnQ9w3tPWvv/6ie/fu37CHzyTmDjunLeafmLeHxGIxDRo0wM3NLf/s+o4okCJp9erVHDx48IuyxueF69evs3jx4gK9liwg8O2QUMTUCI030URneZonv3qA97VreFy+gM/TWFI1NNGUgDoqijcaphQ1ydtakW6ZpnTt0Y3O7RpQ1kAMSLEqZ4tW8GWOXgsn4ekhTtxQYVGxMvbl33e8EgYfGrHE4rQBLUPoSa0oX6YQIWd3ctDHh4ObjhGQ02kowYN1YwYyaIor3kExJCUlkyLSpJCWBMRiRKnJJCck8+bkKuYd02TAiVucmFgbPQBJKUpbyAjzucjduGjOj6tD2VaL0xuWUWfmJXb2KcShaXM5H5unWyUAaBm8wWfnPsIcZ7F6RV/Mn4UgMhCDlgFvfHayN8iCBmXf7kPUKRzHPbdD7NzyD34KUN5bxbS5f/P3Lj+M6lTJFrMuPugG5078w8W7YWTKLHU84SFRZF1FVsW+IixGAaiJDw8hKnshr8JiUAA3b978hrlMs6C4zOIhk9l8NZiPrXbHe29l+sKlrFh/lhc5vveNGjXCw8Pjm5n5X6JAiqTDhw/j7Oz82dGZM0h4cgU3t6s8/YiHadWqVXn+/DlRUVFf1J+AwPeKbnVHyic8xO9Zxmiq4smW3tSp5UTtRl3483Fpei+aSHO5isC7D4gr54ij3pf2Ksa4y2wWtk9mfRNz9Er346LVb6waX5ti7z1eK29b88WmdJ09n44ah3Bp2pXN0WaYiMXp6WrSkJQZzJK5LVHt7U2N0mWoPfwkup0XMd1ZH6lFBcpoPWBB/fos0m5ME7M7zG9iQ9VxXkjkMQS/1OeXmVOo+/IPHA2MaLIqFPsOrdNbFqFXuDhNJ02ieeQmJvx5i9yC8wu8n8Tbt3hEGRwdC3H/sorBp/7if1pA4m1uPQLbWrXTNxmkkXDrLqLq1TFMCicsOoi/555Bx8EEabEa1LZJF/SqII6MqUe1tlP5+8RR1g9tgNOgA7xUg/L6PBpb12emd8YnlczFifXp7RqJWnmdeY2tqT/TO/NzTL44kfq9XYlQpfLixQskkrQ+FP67mDB4Fd4KUAXuYdoKDxTqcC4sHsq0Q2/TWOVKwl12/jGHpYsmM2Ly3uxlsjoMGtuQ5INj6eDswpw9N4l4Z7dAGtrVezF9qgu1jSXkXNctXLgw9+7d+7gtAv/tIMOy+DOsPlCBXW3y2xIBgYKNxLYjPzsswf1MMOOHfCDJtPolZ88/pkqHDnxqBADkrdn4MoFUkfTdAUezAi6ud+m29ClBCQZYWBYhzXsnl+O9j5CUkeE6S7LrbImvO+0mphOAAo8Dtyni/CdXJ3al2NFfsDuUgJFxVsN1cRx1gIdDo3n5MhqxYXHM9dOttBiE29NWBEZrUcKiCJoP2xAYmopZSSOkiiSUYjly2Xjcn/QnMCAcSVFrSuhLgTIcyTSyD4fC3x+iQFplFreSZn3iTfzRUHDb6waxYri2oC27o9pxsmG9tJLbXtyIK0Jtx3JZvk8Kbnk/x6JhDV5fPMWT3fO4b9SJ6k+GcLpKDapqAqh4tNqFsX4dOXD1V+zkQLIXE2oNYe2t1gy/fhNlVX1OrT7G2M3OFFa9xPeOHPsehhB5hJvKquifWs2xsZtxLqzipe8d5PY90E9VZQokAFnJkig9dnMvcTDFTm5ltVtLBg2tQlLgC8SN35cfMjtJJxezIrwv5xaWxePInXfKta0aM2hhYwbFBXBq2xpG/Dwfw3r9GDOsCaU+8Ykul8tJTs7jzosflAI5k5RGArfXjmDBZQXKW+uYs+sZSQ82MnLOWT41FFTc7UfoOdXlS0PNCQj855HY0m98e4L+3s79XN5MAVQPt+P6rC3jXMrmIRikGIlUijTXE8RomVpTJlMgfez4pyLFsqSYKyv6UK2IPsW7HcWw+3A6W7zHEJkBxSwt3gqkjBYMSmBjkd6/3BgLCxPkYjFSuRZyWfrwKTPEolyZdIEk8FVQh+Dl8wyNBqPZMs+F7v9rQFExgJoQLx+eSSvhWD3LvKLqJV53CuPYxgqz1MdsWpdA7yFSbt+D8o410p4BCh82bYigy9whaQIJQNORWdeuMqNKKtd9HmPbaxqN769l+xMVJPrgG1QOhwoyUq778Ni2F9Ma32ft9ieoSMTHN4hyDhUoJJFkdw2RFqWo0Rsiwq6zJ9Cc+pJXBIcexl2zFz04xNqDt/E/v4m1Rx68d8lMVq8Xze6Npu2ka1i2bpT7PdKxpn77n2leUYNHZ67wNA+aJz4+Hh2d7yFpVv5TgEWSFjaFo7j+MJq7Fw7zj2cgSS9fol22POG7ZrLKV/mR85O56ZtAueqGaRepes6umav+BbsFBL5PDFqu4daZMZT9gPqR2I7m9J31/O+7SOUixux/K7ge9orAu7d4FPqKexu7UErYdl/wSfDC844aW0dHjEt1Y8rgSumzRgl4ed5BXdqRGkYQ6tqXX9Y8gwRPfBWVqVmqGMUMNKk+fAqtgzy5nlQUhxql0gR9zC3uxFXFqXwOISyTIVYF4HtPH3vHuvTrpGTHBl/i7/rgV6IqDoVUBPjeQ9/ekbr9OqHcsQHf+Lv4+JWgqkMhRCIRFhYWJCWl+3WITTA3SeDuX3tRt+lLudSnHFrjS5VhTUgVJXF33RKu6NoQsmUtV5IBdSwhz8LTBJM6ihdxlZly7DC/PJ3OnJPvUz4Kwrxdmdm/I10mHkPdYj5H3KbRIJf83KmpqZDDvTc8PBx7e/vP/3x+IAqwSAKpmQnSx66c125KhQRvdnuVolc7M0zqdqKFtZro8ChiQu5z+1H4u4pcFYRPoDHVykpR3L/I5TBD6nZqAcpowqNiCLl/m0fhwnSjgMBbJMjkmh+eIZLIkMsKqMpQxxMeFsM7r08yA4paWlHCsABFtY4NIfBVQo6DKmJDg4n64Z2XFNzdtJ4z0akooiKz7WhU3N3E+jPREOfB8gEdaTnah+IORtzesIGzr+OIS63Db3vc2dgP9q87xks1JLx5gxpQq1WoxOJs3291+EW2bLhASMx1rr+yo1oZTax69MHCfTNHPO+iqOhAMUks16+/wq5aGTStetDHwp3NRzy5q6iIQ7G01mrUqMHZs2fTW5VibhzLjdTm9He0wDT1Ko/KDKWbhR5WhjHElu5CZzsVsTIzzKRA0hGGt56DpwKIPcm0nqNZvsONW+Lq1MkZ6V3hzbJu7RmxK4baE7dyYNNMetY0f6+vnjrMg7+XreNM4A12z12Om9/b79uJEyeoW7fuF39SPwIFen5YYmZMpGcctSY0JMR1MeKRe7CVqPDbOpHVladivrQnnmWcqRx1lts1N7O/8RH6/V2NLXPqkfTwIP88jyN1y2KOuwXQ+G8jLg5bzfjZ5nTr4kkZ58pEnb1Nzc37GW7zZYO+Wq3G1dUVtVrNtm3b8PPzQ1NTEyMjI1q2bPmV7oaAgMCHUL/eTu/WL5ntMYsqBWRkUz11ZcQWPRbNaI08y/GkU9Ppe7MX/8x0evuAU4eypacz4QuuMrNyAbmAfEGG3a8nefXre0rsfuXk+wocTxMyKv3vemnTnL/sfcYvWesUqU413ZUcPB9J42aGiNWvODl9JCt1VtDurg8PrBxwkIHYxJkBtVYybnMkZUfao6m4jc8DKxzSCnEeUIuV4zYTWXYk9um6e8iQIUyaNCl9vJdRe85lLssN0JGAyx5f+uvrIwHiPL3wi0hi19JQjPuMo6wEEm4/JLVxcyrJAFlnNp9pQ3R0CvJfBiLPOY0hq0j/DUcYqfXx+Q2xaS26j6tF93HZjyckJHDz5k0WLlz40TYECrpIKj2SvcekaGuB/bFdyOVSyBr4X+ZIv0UzaPVYivOq+7wu9gbruhUAkJf/jdM+6fUGASo/5mae1o9FM1rxWOrMqocp8IUiSSwWc+LECVxdXQG4fPkyly9fZsmSJYJIEhDID5SRPPS+QWCSPuWqV6OkTjKvngQjKW5JERmgjuZ5YDImVqbI4wLx9g5AWcIBRxsDJKoonr9QIk/w51mhilQvqeKp7w0Ckoywc7TDLNcJKTUxT325EZCEkZ0jdmaphPqc5JRXRfxD61NMEYlSnoD/s0JUaDyZ9U7piXTV0fh73uCVoVm2eEpxgd54Bygp4eCIjUEBnb37npBWZfTSznTpX5NaNuUwiAhCUW0022fUImbNFDTsu5AWS1WbBv2bI9rshr2DHuoQX25r2NMlPdCqdoP+NBdtxs3egYwNntbW1rRu3Zr58+czZswYpDoGmSEH5PoZ+fsU3PBKod34gXS100EzY8OdSU9WLrIhM9a9RAuDIrldhBztL0iGHBMTw7hx41i2bFk2Z3OB3CnQIgmxZuYXQi5/9wMVacqRi0AklSJOFWPafjpTPtqoCE25HBEipFJxzqXaz2bKlCns3Lkz04HP1NSUgQMHfqXWBQT+w6iCObd2IX8H12dE0xC2bvOlmMtSRjl95paLhKtM/9+veFdsRiXFNYaPqMxf16YTM7s12xueZ88vxiSdmkD7bQ04MTWWHl03IWtQm0I3xzP3fxvZ082DgQ2WEV20GGZNhtI+cB6bNJpSW9OXUeNrsuH8DGq8s76hJnRPH9qs0aBpbU18R42n5rqtVLnkR9STSPZfqYvxhs4sjy5KMbOf+K31bUbfHMaNpeZs7tiFHXqNqZrky/HbSrqh5OGGLnTdJKNB7ULcHD+X/23cx0j7rOosgbs7l3M4VEpiYDT2v87gZ2vhofdhxBjWn8pJv18JevIaibkFRXXTH4GjznIzS02p/XR846en/28UZ7MXMt03nulkp0ePHty7d4/o6GiMjIze23/x1j/jJCdTIAHoWtn8a5uLQkJCmDVrFiYmH99lJ5BGgRRJSqWSEydO0Lx58y+OlfQp3Llzh8DAwC9qo2zZsnTt2jVzNmn8+PFoaX2B5BcQ+EFQx8Zg3KQWivbL+dtxA+Nqu9PW/SnDnSp91gClTpRSfsQGxjpXhOduRLRYzo1X2gzu3pCpyw8T1r0L3nu9qdxlNndW1CG6x0H29jVD9KY8/X76E/d2jqSqavC7+1baym8xvXYyJfs1oYfzGHo+fZHLA01NmN8Dkkv2o0kPZ8b0fMoL3VKU6+iEaUh1fnO2YNt6FTV+d2drWx0SdqSlr1BcW8/alEGc2DIIc/VtDJ36olZcYsXiaHoc3EtfMxFvyvfjpz/dGbnpf2+7SzrJ4hXh9D23kLIeR7ijKyQ4+WSkBpSwNfh4vc+gQoUKH+oYy3rOWH6Tnj+NsmXL5mPv3ycFUiQdP378A9G2JZSfeICVAC1Xph0qN4HD6z7SqKQ8Ew+k1T+Qedph1gEpKWXZvXs3MlmeQtW9Q8ZskomJiTCLJCDwiYgNylH2+W6elGzP7DbFeTopirKO1rkPTrEP8HpmTDW7Iu/deSKWQdiBkTRaKMKouAmxCWrKpKYir9uD5uOns99Pl2u3a9JnpSYvNoby5MZv9DqfLjLKV0aaAhLTEhSXA1J7xqz/nYVLF9Jj/j0Syw9m7XZ7zN8ZKqTYj1nP7wuXsrDHfO4llmfw2u2Uy1pFYkqJ4vJsZylDQkg2b4yhGBBbY11SymPlC16EPuHGb714a1aOuyGrR69maxnddhJT182ijUmB3oMjIPDdUiB/WZqamsjl8n9lFglAQ0MDuVyOWPxltyNjNkmYRRIQyAtqXnv4olGvCcVSX3DZR48atQpBzG32rT3Ibf/zbFp7hAfJoAw4xdrpvzJx7/Nc86ElHl/EwsT+/HP1HMc39sBWpEatBmTV6N4mnv0Tt/C0QTfqymXY2JSkWOsFHDhyhEObh1DTshTFtQFEaYOj8gauW8JpvuoEPo89GKLYxA7P9+2KVXLDdQvhzVdxwucxHkMUbNrhSbJIhEidmrmsL8oxxMjK2KJ/35fHCiDpNnceK0HDGpuSxWi94ABHjhxi85CaWJYqnu1+Rb2Io/KUYxz+5SnT55z8aIoKAQGBz+OrziR5enpy9uxZoqKiSElJ+ZpNfzfIZDICAgIYNWrUxysL/GcQi8UYGBhQsWJFWrdujVRaICdpCyhJXLsWQvnulkhSHxCXGILX8rWUaFuLpLvrWFJoDn1DlrP2SjOWNWpK304XuHAi99ZklephN2spLv3Pox2dQJhONAahKrCWUaF7W5SL/6bJTEdkSKk5ejaO3bpQ+6IFuuHBGPRez0jptbeNSS2x1RrFwAbulDaNJUjSi2VOEu7NqkH78Pk8XN4goyKWtlqMGtgA99KmxAZJ6LXMCa3iQVj5TqLZdD26vMdWqd1QZtXvRPd6VymmBZEpYprJajJ6tiPdutTmooUu4cEG9F4/OstZsZyc1hN3+z7UCBZTvU65grkkICDwH+CLf1sKhYJdu3axcuVKwsPDad++PcbGxmhqFqCYJP8iJUuWzG8TBPIBpVJJTEwMCxcuZPjw4QwePJgBAwbk4sApkB0t2m32oR0A9kw5cwWlpiYydRCrVpemS2c7VBNlmJl9eLgSmwzimHfa30d92vMkTIy5hRmFVEmkSDLWx0Ro1exO94ppbYmLtmHx+RbERUSRqmuMrgygcmY7YEDD2ee58Xs4EQptjAy1kADKrt1ofdA4W/8GDWdz/sbvhEco0DYyREsC4MKhJ31QIUEifutTpNVjP3490v5uOvccvnGRxEkNMMjY822/mPMt4oiISkXXWDdHHBx9Om8+Q5voaFLkvzDwnX3iuSORSAgPD8fc3PyTzxEQyE9evXqFhYVFvvX/RSIpKSmJDh068ObNG6ZMmULLli2FbYUCPzw3btxg+fLlVK9enXPnzuXrD/x7RKwITU1IAAAgAElEQVSpmSYKEjzx8osgaddSQo37MK6sBHXELY4d8+beTU1O3LGgVcXC7/dL0imKTcYebIkcTVQ829yX/819QJM1/+TIOydFp4jxe1rJUkPHGNMs/09W1sNl0HucdKU6GJvmOCaWfDSFi1THkHdciaU65G6WBK3c94m/l8TERNq3b8/kyZOZO3cupqY5DRUQKFi4u7sTFBSEVCqlXLlyHz/hG/DZIilDIGlra3P48GFheUFAIJ0qVaqwefNmVq5cScOGDQWh9JkobniR0m48A7vaoZOxZ7qIPc4z3XHOc2sSSvVczYWfNTHQ+fKxSrtsFfJnyP58Fi5cyNixY+nZsycTJ04UhJJAgcbd3Z1Lly5hZWVFfHw8/fr1yxc7PttTefjw4WhpaeHq6ioIJAGB9zBs2DBGjx7NTz/9hFL5sVyDAjkRF2/NzzmDynwJEu2vIpC+V/T19Vm0aBE7duzIFEphYWH5bZaAwDvkFEjDhw/PN1s+SySFhYWxb98+1q1bJwgkAYEPMHz4cIyNjXFzc8tvU747pJb1cG5Yhre5ymMJCXzFOxnPYkMJzpnwLCGC0Ghhz1dOBKEkUND5XIEUGhr6TfTIZ4mk9evX07FjRwwNvzAVeJYkjwnPvTnlfo2n8V/W5DdHFU/Yk4cEhMW/3YKcHEXIq9isCVNIjgolPF4NJBMV8orY7IWEhsfnuoX548QT+uQpGfkxkyKDCXr+nOdBQQS/iuXD+TGVRD66xtlT5/F5Fpv9GoJCiM48OZmokCDCYpUfLvtk3veASyAiNPo/v3152LBhrFy5Mr/N+P5JOsX0viu5me0LriZ0S0/aLfVD9dSVYdOOkgQkHBhAoylX//Pfrc9BEEoCBZXPFUg7duxALpdz8+bNjwT0zDufJZI2b97MoEGDvrjzpFPT6bvyJoq4Iwxr0pdVbl4EJmeVDioebh3C5EPRX9xXXlE92c74+We5uX4cy7wVgJLAg2P5yaE2HYeNZVCrqlTtvJqb8ZB8ZTJOpRsz2ycp/exkrk5tQN+dMZB8hclOpWk824fM0qtTadB3JzHJZxn901jcDw6m2VSvT7WMl/sGU6tcE2Z7JQNJnBxdBYtSpShVsiTFzAwwtGzGtNNh74qwpDus61KBUuWc+KlFYxxtrKgz4RTh6vRrsCxFzd/OEQuQfJlJNUvT+s9HJHygTJWzj9zIeMAlPsV12DSOpj3JGNBoClc/8CRLeriRblW7sT3iUzsqeHTo0IE7d+7w7Nmz/DalwBMb7M/L2LS/E0IDeBaV9g1TRATyLK4+k9cPwT4t4RnR/h6cu/o4PUt8cnqetIf4h2ZIcSWRDz04f/URUZ/8Rf0xEISSQEHjSwTSq1evSElJwdLSkhYtWnxVu/IsklJTUwkKCvoqak3WeDLrh5Qh8pYXD3Wb0H9kV5wM35qUEOKF64YrREujiHwWSETGG6QqimcBYcRFPyMgNJrg21e5FhD9VhSoo/D3OMv5my8zhUneUPF412aem0vYveM1FhVlqB6tpN/4ADrt8+LScTdOeZ1nonQZw1feRwWIdYLZNvwPvN/XoViH4G3D+SNnYVwIbwpboRcWh5FVMVBEEOgfQq6TaapAjkxojlOvnQTlUEAS28EceuCPn+c+Rlr4Mrf3BI5m05ZK7izuz+ijOvQ99IQ3CS84OaIUt5ZPZ9PDjCdICo/WjWbetZwLGh8rA4gl2P9lmogigdCAZ+kPJgURgc+IFDdm8vohlIn04eQpLx76h2bOKikjH+Jx/iqP3vMkk5vJUeuWpqL+O0XfDTKZDBsbG168eJHfphRwVARv60uXFQ9QqUPZ3qcqTWdeRUEsbqM6suDaYUa2mYNXspIHa9vToPcaju6awuAVt1GqI/C+5EfUkwvs9whFRSqvj09l4Jwd7JzVlpouBwjP/M0o8N81gcGrvFGgInDPNFZ4KFCHX2Dx0GkcCv78Od7vCUEoCRQUvpZA6tSp01e3Lc8iSaFIUypyufwjNT9OkttI2sw6yeUT13j5ypeDB68TmTk+KXnivoRdDzWIvO3Nid86MMdLAagI3NCLLivv4rf8Zxq27svUrbv5o319hh2LQh15lslNWzB2zyVOzu9Ei6lXsvWpjnnOvdu3uHUr+787T6OyzLxIKN5uIoOcytF1Yl8qa6gIPHSAZ42H8otNesQSsTmdNt7A/bdySABp9THMsN3Nr394vyvMpNUZM8OW3b9mF1GqZHtGrnChWO3xLO9ZDNXLvYwbuJZbua2XKZ9zJ6gsE1YNpXxOX1aJLmaW1pSr7szk8c6YvjrJ0WtZpmhUgZw4fp3UugOZ1MoCLZk5TWaf5HnoRcaXS29MbEIxo/usGLX0XRs+VAagCmZb3y6seKBCHbqdPlWbMvOqAmLdGNVxAdfj3BjZZhYnr17CL+oJF/Z7EKqC1NfHmTpwDjt2zqJtTRcOhGd/QCXf8OZ56erYxl1i6UAX/jgT/v57o/Bn14TBrPJWgCqQPdNW4KFQE35hMUOnHcrlhv576OnpERMTk99mFHAkWLdqTOqlMwTHnOdCbGn0fM/zKPYS7g8daFUz/benuMb6tSkM2ruFxctdWd6jJGJxUVp2dMK0Umd+c7ZCAmg4/Mq2batYt3M85T3Pciczvq2MkiWVeJy+R6I6jJNbV+N2PQJ0kwh8IcbiB0rxIQglgfymIAsk+MwQAF81XYjYjFYurVhzIYExo5thnjk+SSlrZ4ueQ0MW/96e4KlL2P84HnX5y8zepMuII5V5MDgKp7Fu/NXFlNjtHajpdgXPi1O50Ggb5ybZIU2oikvDXTCzdmZ36peeHNp7i8QcqeEkZZyxtXQgIwSmTvkm1AewMQEUXH0ViVFR8+w3TK6NFqT5PYgNabtgNqcaDOePVu5p5769SAzbLmD2qQYM/6MV7umFkqJ22AGY2KUdsBzEnjMfuFea9Zi0ox7KG1NY84FqUiMTChPF68gsMzOqcMIjoJClEXoZ91huSJGsWldiQZ/ZTuwftIhRa+dTmk8sA5BY06pxKuPOBDO4yAViS+vhe/4RsbHuPHRoh5M8kjWIMWveESfTEKr/5owVO0DDgV+3baOrQTRbnJ04eyeF9o0yPgUVQV53MKo+hpALG3lu3Y3xDXMJHCMrSUmlB7vvJTK42Em2rnaj5aChVEkK5IW48Qfu1ruonroyYosei2a05stfBdL40pQ3PwrScq1oELuAk+4GhDmNpYPvNk4dC+dexVYs1XrDRgBlCCHJ5jROS3iGtXVJpI9ztiRC39SMQgAyLeSkZPPVkxYtitGbCMKu7yHQvD6SV8GEHnZHs9cEKv1ge1EyhFJGeIDx48djbPzhuFECAl8DpVKJtrb2NxdIiYmJ+Pn5YW9vn2cbC/BwoCby+m3UldpQRCxFp1wpQu8/wHPZQoI6/8XPBncY9aQqbVuZIkZNcmIycq1Yzl0K443uRNp7prUitsu+PikuVoN2P9uizCGSxHol0cjVFilFi5nw+sELlFTIDAynCjjL/hfWZPQgMnVmwewjNBi+AINKOZoQmeK8YDZHGgxnwTuFXxdFaCiRGGJcJMt0k8QUM2MRccEvCFdDCTGonhziz4OJ1P2lM2kWiShUYzKL+xzlf3Nn8Tg1FbO3F/CBMgAp5Vo1IHbBSdwNwnAa2wHfbac4Fn6Piq2WosXB99oq0jfFLO1JhpYcUrLNUsXi6fMaif5gOqn7c3FjI7RzvWopRYsa8SYijOt7AjGvL+FVcCiH3TXpNSH9fscHcdP3MQmmlalRxhCJKoaXL5UYlSyCJiqiX75EZWRCks9JTnlVxD+0EXZmQg6+fxVpJVo6BTNqWShVpi2mqWw5fed6UGXaHLQ5nFZHVgZb/WX4PlbQ0k7N7TuPUcqBLHnSPvYaJzYxxyTBnb/2VqRD33JcX3uINYlVGDZPxt19awmwbUJhn9O8cerD/8r+97MHZBVK7dq1Q0MjbTRMTk7m6tWr2Nra8vDhQ5ycnL7KKoKAQAaPHj365gJp7NixdO3aFVdXVyZPnpwn+wrw620KN64/pXSVCsiAQmVt0fCcz7SztZg6qCwE+3LzhYKUFCDZj807X1K/tQPa2ma0mLmPI0f2sai9BRaNmmdrVf3Cg/07d7Izx7/dF/w/sCtMTIn/tcXi7Gq2PkpfwlIFc3DGYBacfZ3lJooxdV7A7DJ7mb3v1TuO02JTZxbMLsPe2ft49bXdHpJCuedxmYvu25k87wDhJdrQsXaWwV1SijYda1Poygp+W3sBv/sXWDF6OL/P2Mi1uCyKUWTATzMX0E0vhJcxOZTkh8oAaaWWOAX/xbIHpWnYqim1ojcz18OGFg2zShsRIpGa1HdPf5fkm3gFVafPzM5YP/Tm/gc304kxMTch4e5f7FW3oW+5VJ4eWoNvlWG0NRajfLCen+t0Yt6B42wY2ICf5lwjNmInAzosw08JqEPZ3qcjf959mcO3ReDfRYZDy6pE+JtSv5YuFRtWITbUihaNdd9WkdoxdFZ9znavR4uWLZl5MQWxSISkZIW0PGnTLn1khycgNcc49gapzfvjaGFK6tVHlBnaDYukcFKT7rJuyRV0bULYsvbKx1r6z6Cvr8/ixYszBRKkJRuvXr06x44dw8zMjK1btxIbG5uPVgr816hateo3F0gdOnTIFEh5DexbgGeSUkhIeMP19Us53WIqTWzLYuS9Ga2N66mlBQk+13ldSYOjXVuxMz4SeavlbGxgQ0piAzoN+Yk2xim81mrDgg2lsrUqrdCZqfM659kaifVgVk+/R48W9myxsUT28iGRdmPZOs4BqeemtxXFpjgvmMfh8z15/U4raSJq3uHz9Hy3ENWDedRtE8Ssu6tpnMeXV9XTbfRruB2Jhh7FqrRlyc65NNXJWkOC7dCNbHzak19HN6RCMmgY2tNjxUoGWklIDcxipXFb5s5rz8me7/ryfKgMmQMtq0awKaQ+tXQrIqsSy1+vW9BYFzK9tCUlqWDly6Rm0yh19J1Fu+zX9MyT24WrMN6kCdHmy5k+1oBBo0bgeMqZKid78GBPF7I8OpGaGxN7I5Xmyx2x2JrK1UtlODPdAgkKLq5dTqzLcY4PLYE45hC9ayzm0M/vWYaTlErzbQmpnunbIvDvotlgOf4Zv4/Gq3j6KqOkB/vfJjzjnG8ckXFSDAzk6S8q9lnypNXNzI2GVid2+uUYTGW1mXP5MnIDHSS4sMe3P/r6EsAKw5hYSnfpjJ1qIjKz7POl/3X09PRo1arVO8ebNm3K+PHjGTNmDNu3b2f+/PkUKZK3tCgCAl9CfggkAFFqau7v9J07d0YkEuHq6prpU5GcnIyenh7Jyf9CBBJlDKHhqRiaG6C49BvN51uw9fBQrCUKPH93YpblMY70lhObqot+1iSPihgikwthqCvLve3PRRHJs4BQUgxLYWWqXZCn4nJFFRtCYFgKhUuUxDBfVhLUqFQgkeTh7imTSEKOXApJt1ay9H5Hxncxy37/lXFEJ8nToionxRAj0kdfEyCB3V2qcKrnbTa01ATlHWbUHYHW8o6cHxLCbI9ZVBEHsbxZB14vvMrkN2NxWFEdz73d+VqLbS1btmTYsGG0bNmSzp0706JFC0xNTb/KdlVra2tOnjyJtbX1V7D0RyeOfb0acqDSUCqFhlBl3HiaGX+Pv/KvT2RkJOPHj6dHjx5s2LDhy+PkCQh8IsnJydja2v7rAgkK9EwSINXHzDyJE7/VYtQ/Zvy6aw7WEkAdxZ1HhXDoWASxppR3dofL9DH8BvoorW1DSpX7vgcHia451rofr/ftEJPnPMhSeboTdQKBibVw6WT2rkCV6mCQMXsm18/yvZBhU1qXP30eoWhZEWm4DzejStLLXJNLcVFEqwDlEwJeqNPOyeLbIvCDobiBV0o7xg/sip2OpjCTmAVDQ0Pmz5/PuHHj6NChA1KpFJVKxZUrVyhRogQBAQFUrlwZIyOj/DZV4D+Ij48P5cuX/1cFEhR0kQSAnGYzTuA52yB9VgAQm+Jy4FK+WiWQX2hRtqZDHs+RUmXYLOp26UHdqyUo9DoK60mbaW0uJariYoY2bY2FTEqSXIO6kMW3pRRnZ9TlW+ltgQKIuDitf3ZCjiCQ3oehoSELFizA19c381ijRo3YsWMHFStWxM3NjZYtW1KyZMl8tFLgv0ibNm1wcPjw2P+1BRJ8FyIJxFoG784WCQjkAbFpC+afa0pcRAyp+kXQTf/m99lzl06vQknSK0phDRVIJIhxeevbkr9mC/zbSC2p52yZ31YUaAwNDWnatGm2Y40bN2bixIm4uLhw+PBhatSogZ2dXT5ZKPAj8l6BVELGuVWj+Du4PiOahrB1my/FXJYyyunTl1KExXaBHwgpOkXeCqQ0xGibFKWIHMQSydsfhFhCXlymBAR+ZDQ0NJg7dy5nz56lbdu2rFy5krt37+a3WQI/CLnNIKljYzBuUgvFoeX8HdOMcbVfs8f9KXnJOvpdzCQJCAgICBRsMoTSxIkTadu2LWvWrMkWTkBA4FsRGRlJ796931liExuUo+zz3Twp2Z7ZbYrzdFIUZR2tcxU+sQ+8eGZcDbsib9+QBZEkICAgIPBVyCqUqlSpgr6+4Cjh6urK06dP6d69+xf7xwi8n9TU1Fx8kNS89vBFo95CiqW+YK+PHjVGFIKY2+zbGYBtk8L4nH6DU5+WaFzcwOqF+9CZ/48gkv5bKAi/78Wt0ELYOjpQUhtQxxAUmIheoTDuhhlTu3LR/DZSQEDgByFDKHl5efGBCDM/DC9evODWrVv06dMHc3Pz/DbnP8usWbPec3+TuHYthPLdLZGkPiAuMQSv5Wsp0bYWSXfXsaTQHPqGLGftlWYsa9qXThcucCJHC4JI+p5Rh3J0dHum+1ejZcVwpkw0YerRpTRL2Erf/+1D18YUgzrDBZEkICDwr6KhoUHt2rU/XvEHoHDhwgBUrFiROnXq5LM1PxpatNvsQzsA7Jly5gpKTU1k6iBWrS5Nl852qCbKMDOTQi75FQSR9D2giCcmIYVUkQiZtj5a6Z9a0oW5TLnflX3Hh2MtSabekGpsOJVIfbkvj/Sa4bZnEpVkWZuJISElFZFIhr6+kJNMQEBAQODHQaypmRbSJcETL78IknYtJdS4D+PKioi4dYxj3ve4qXmCOxatqFg4bclN2L9T4FHhv7EXdWrWpKZTexb5ZGSlUvLg/DWMW7XDUgKQSrIiBZFYyUPfh5Tv3hu7rAF+VP5s7FWHmjVr4tR+0b9/GQICAgICAp9IYmIi3bt3p1ChQpiamjJu3DhUqq+TTVNxw4uUduMZ2PU3Fk5ohrFYTBF7Z2a63+bIlDaZAgmEmaTvAAk2g/dxZ/B7SkRiUpIVpALq4P1svVaBVjMS8flbSo2fTbMrYIkNg/fd4T3NCAgI/Ido3bo1MTEx+W2GQDpCKIS8o1QqadiwIV5eXnTv3p03b96wcOFCHj16xL59+5BKv0y6iIu35mcnOWh+PBKeIJK+W6SU/2Uwxbr+QufH5Ui++4SS0zbTw+gOv4aWo6Wt8NEKCPyIeHp68vr1ezJoCwh8Jxw8eBBPT082bdpEnz59AFi2bBmjRo1iy5YtuLi4fFH7Ust6fGrMWOFJ+h0jse7Fjqttef4kikIlLTGWA1iwyrPpx04VEBD4j3Ps2LFMp2GB/GPEiBF4e3vntxnfFYsWLcLW1pZevXplHhsxYgT79+9n6tSpdO3aFW1t7X/FFkEkfe9IDShpa5DfVggICBQwatWqJYikfCQ5ORm1Wo2mZlrS0cTERBITE5HJZEjynOH7xyEyMhIvLy/++OMPxOK3TiMikYj58+dTu3Ztdu3aRb9+/d45V6FQMHz4cI4dO4aenh4uLi6MGjUKkUj02fYIjtsCAgICAgJfmdWrV6OlpcXly5eBNF+x4sWLk5CQkM+WFWwePnwIQKVKld4pq1WrFsWLF+f06dPvlCmVSn766Sf++usvqlSpgoGBAWPGjGHw4MFfFK+rAMwkxRISmIi+hQlaJPDc+wr3VTY0q1kQk0wmExXyBpmpMdrvyEs18SGP8A8XY2Zji6kWqKKDCAhLINvHI5JRuHhRNGJekVTIFHODt1vQ1PHhvIyRYlK0MJp5tSzmFVEJKlJFIsRiGTqGhminf7rq2GD8Q8Dcpii6We1WJREbp0RDRwd5zhcbdTwv790mIEaDYuUrYW0oA1Ucr4IjScp+QWjomqCvfk1EvBoNPVPMDGRAEpEvw4lTS9EzNcdAxqcRG0Jgoj4WJllCFCREEKrQwcwgr3dFQEBAIH8YOHAg8+fPJywsLPPY2LFj0dX99OSqPyIZIqlMmTLvlIlEIho3bszx48dRq9XZZpoOHDjAhQsXWLduHQMGDECtVjNhwgQWLFhAs2bNcHZ2/ix78n8mKekU0/uu5KYC4o4Mo0nfVbh5BWaronq4lSGTDxH9rxun4sn28cw/e5P145bhHXue3+v2YkdEzmoB7OjtSPV2I5k+dSgtqzoyYPcTws8vZ9Tw4Qwf6Eztas3oOWw4w3+dyu67F5nsZIVNx794rs5oJIGToxywdhzH2eRkzo7+ibHuBxncbOon2JmE+4iKFCtalKLm5piZGqGnV4Kmcy4TA8TvH0SlSgPZF5v9rOTTIyhv7MCka4rsBdHnmVrXAgv7ejRu6EjpEnZ02+KP4vk6OlqXolSprP8sqTnpNIcHlqVUqVKUc9lHNKB69CetLUtRyqoeMzxztP+hKzk1nb4rb5L41JVh046SBCQcGECjKVdJ/uRWBAQEBPIXLS0txo0bl/l/Q0NDhg0blo8WfR/ExcUBYGDwfjeSOnXqEB4eTmDgW52QmprKokWLKFOmTKZTt1gsZu7cudjZ2fH777+TkpLyWfbkv0iSNWby+iHYq0O55fUQ3Sb9GdnV6W15Qgherhu4Ei0lNiaSZ4ERZDxyVVHPCAgJ5llAKNHBt7l6LYBo9dtT1VH+eJw9z82XSZ9nm+oxuzY/x1yymx2vLaiYy2yI8vpGlgR05ODVfzh46BSeB9vz6I81BLRcyLGTJ/ln2wAqFW3DH0dPcvKf7Qy3l4C0NFaRRznyIt3g+HMc9CtESQlAHCFvCmOlF0ackRUAiohA/EPiP2iuxHYwhx4G4P/Ql7+7aXFh/p+ciAetxpM5cGAKTT7Rzy322HKWelsx6UY0yVE3mF09nGN/bua6mQu7HwYQ4LeS1joaVB53hscBj7k0qxYAokJaKK+d5WqSmtfnz3NHQwutnEvBscH4v0xXawmhBDyLSotzqogg8FkkssaTWT+kDJE+Jznl9RD/0IypaSWRDz04f/URUV8nVIaAgIDAN2XQoEGYmJgAwizSp6KnpwdAbGz2t/qTJ0/SunVrhg4dCsCqVavYt28foaGhvHr1Cm9vb/r3759tdkkikfDHH3/w6NEjDh8+/Fn25P9yW5IbI9t4MvhQE85fe8krxUEOXq/Kb83ScrAon7izZNdDNBxuc+tlFNt+ucGvHkuoIwlkQ6+u+Dk3wmOaO8V/qk+J16c5X3Qut1a3IPLsVH6eeJdKLcoTOmUGpRccZWbtdJWgjuH5/WdEKXOsU4r1KFnBksw4UpLitJs4CFm5wpj21UODh++9BHFRWyxCVrHgTyt6t2pCjXK/c/7GR65bbEmbn2I5ejiIIcNLkXD2EAGObakQEgWqZOxHrsBWL4x6yysAKl7uHcfAkJGcmeGUe5tJ4QTcuYVaHc3d57FoVapGBU1IODOb9oNSWRPmRp9PyDepWdKCohxj/cB+vGnXnGarA4gsb4gEwFIfEowoJAKZnjmWVpZISOAKILV3xP7uFc7diCb+nA9aDtUx8XqRrW1V8Db6uqj56/zvGGzvQ9XFZTl6dymV3UbR8WxPLtZaQxuPjsyQ+BH1JJL9Hs6MJpXXx6cyMKYq5uFnGWg2h8sb22MsBlDgv2saiyPa8+fQqgTvmYlbiUkMtfFg6fSzWE+aRrui+f8uICCQ3/To0QMjI6P8NuOHo0yZMsTExBAUFMTIkSPz25wCT0BAAACTJk3K/L6q1WqKFSvGuXPnUCjSpkmWLFlC0aJF8fPz4+bNmwBUrlz5nfaaN2+OgYEBp06domPHjnm2J/9FUjqSUm1wabWcCwljGN3sbZI6aVk7bPUcaLj4d1ob3+CaxkEex6spf3k2m3RHsE20h+NOY3H7qwumsdvpUNMNFIbMH3uBRtvOMclOSkJVFxru8mRm7UZpjapf4nloL7cSc4gkSRmcbS1xyHR90aF8k/oA2JgAybmIpGK92XxAzrLl2/ltzXCeUIH/s3feYVFcXRx+2V2WIghSBCwUUUBFsReMXSP2Lnbs3Rg1atRErNHEGP1swd57r1HsGgtdsSuKgnTpbYEt3x8gQUUFREGd93nyZJm5c+/ZcWfmN+eee07PX5ezeKA97y7+Icaqy/dcm32E4DGD8Tv8HMchjbhz5CqIy2BvD1Aa+6zWVqP2cu4D51AR9g/zR19CrJCREC+i6rAKvO48SuGZxxXuxyhR0yxP83f0I208l33r1fh1+V7W/7qXZb/oUXvSXk4tao3he/SGqFRDvrNdxgX3/SRcz6BO35qEe70uksTW7Wmpmsq50NEYXkqkUkkfLj5KJPH0Q2p3cYSYv0FkilMPR0zC6jKlawXYDuq1f2Dr1j7ox22mq+N5bmd0o4UGgBRzcznX99wldXRZ3Les5li7UYytKePZCxEtS+dDICkC2TlhMyX/nEMHzbwfJiDwJWBoaIiTkxOPHj0qalO+KcqVK0fZsmWxs7MralO+CIyMjDA1NaVixYpoaWkB0LJlS3bs2EHPnj3ZsmVLdtv//e9/6Onp8eDBA4Bcz7FYLKZ58+a5BnvnhWIjkt6FMsYXf2V1OhqKQGJDZYtw7j/wYNniYJzXdiJs9SJqdW6PiQiUaamkaeqgeHqBKxEJ6M7ohgcAIuzbGv7Xqags9bv0xJ2uJOAAACAASURBVCY3T5J6/m2Mf3CBm9IOuK7vjStyojxXMLj3RDY0Pc14i3c/pEWVuvF92iQO3zfjZkgjJtpL+JjcrOKKY/nn5u/UlyqJPjqc2j1G82vrdri9aqAIZ/9PHZnybwYis6GkbMptGaqC4CuH+FfVGTfPJRhE+bF7ijPDVi3nyM+tGWLwPgMs+M7RjOWb/yAs0p4xjrrsX/pGG0ll2jdL5A/30+hHOPJTdx+2njlB1N1qtF+qDYdy61gNPRNTtACk2miSQc4oJ0mZMhglRBPhu5dnZk0RR4YSfuQ0Gi7TqS4BSCb4pg+PU0yoUd8WA7GC+JAQ5EbmGGqAIi6EEIURpWXeuJ/xpFpAOC3sTd8jcAUEvkzOnDnDiBEjPjpjsUD+KGjQsACcPXuW58+fAzBs2LBskdS+fXu6d+8OkJ1h3tDQMNc+mjZtyqFDh4iIiMDExCRf4xf7KyXDz5fASs2oKgXQws5Gnc2/u5La0BX3ShH8ffMF6XUygDTubdpFSFNX1LRvUcK0LXP3L6I+j9j802rUnez/61T5gusHduGX8oZIktjR/Rcr6ubLi6BEdu1/DNrXilOHx2GrIcG4cg0q6B5HrvzAskOxDd2+T2fE1A3oNFlCVfUPzdF9gLRIHnjdQCVKJ9Q7kESVOhoaYng1tSu2YPjeh3RLVaEm1oUHvwBpRNy7zr+qTMEkKmmOzvU1TJ4Vg694Cz/XkyNLy0BNQwvtD6b2kFCrSX20V+0kzLY7jcuK2J9Lm+rtHAmduIzwmq4saS1l+ZDfuF7TlQU53V5qaqgpVaiAD2W4EJU2o3TKadbuq0b3IZXxdTvM36k1GbfQGJH8Aev69GGjqCmNStzkx19bs/bwcG6O6E7I/OvMqykifNtgekQtZF+tK9yLfUrMget0rdyVCkIqE4GvEGtra0EkCXwx+Pv7Z39u1KgRtra2BAUFsXLlyuz8RznjmDQ1/3uAnzp1it27d3Ps2DEAdu7cSY0aNbJTBOSFfF8pEokEuVz+1vK7T0VGSgoJvutYerYts1rpYWNnhNcmbTasa4h26gF8X1ZH/Xgf2u9KJkazPcs3NEGkX5kfm/VizPcdMc54iXbHP1hvkeOJJ6mK86yFOBfEoPSruDaoxJ8iABHmLts4N3URc671x6nqZiytSpIYFInpIDfmWmWNqSZGXSpB9NbTXkKlbt8jX3iUJn/ZIeZdIknBg4WN6Rg8j8erW77TNMXTzQz6bjNqamIkumbUGvQnM5w0YfurFmL0zKx4FZaU9gBQPmfHiGbsyNqm3nAR907+xW83BjB7UH02KEBd3x7nJbPp+sF4JjW0GzWhjuYe/Bs2prrEJ9dW0trtqBW9kbCmDdGtJqVm4lpetm2JLvAqTFtsXpUKPjNp42rB8UofGFZihnGiHyqn5dSz3ILq2hVsz83GUgzpl91YnjiMkyfHUl4Uz+FB9VlyuCdvn0UxFu164GgSRt0phS+Q0tLSUFcvgJtSQEBA4BsnICAADw8PJk6ciJGREcbGxixbtix7/6vpth9//JHmzZtnr3CrUaMGHTp0yC6MO2nSJExNTbl//36ex1ZTvSfLkrOzM2pqauzcufM1QVSyZEkCAwPf6doqXOTEh0ehMjBDX5rElSlO/G65hSNjrVF4/IzjPCtOHB2EZqIKXT3N15brpcfHkKZlgG5ec/R8rKVxITwNT0W3XAXMdL78YOH0uBBC4kQYlDND7zU5rUQhV4JIgvhTfk2lAgXiPIwhJylOhqa+DhJkxMeroaeXGViWsqc3Nc8MxH99OzSQc3tOYyZoL6fXxTHZnqTg5W3o/nIxnr8k8GPtFdT12Ee/Qp5rc3BwYOPGjdSuXRtnZ2fatm2LiYkJbdu2/ei+ra2tcXd3x9rauhAsFfjSMTY25uXLl8TExLyWcXvChAlIJBJ+//13wZMk8MXwaprsxIkTNGnShJSUFDQ0NF7LWh4TE8PTp08ZOXIkvr6+/PTTT9n7unTp8trKtt27d+PsnHcXSYGulFfzex9bZC5vSNAzNQPZP0xpOJFTpj+we4E1YpS8vP0Irdo9MBRpIMnFyyHVM+Az6aNMS/XL8jVVCJHql8Uq1+8jQiz5DCJQJCZvDh0JOvo6WZ810cvxW5BWrITu/7x5lN6OapIovG/GYu5ihvqVJGLjFICcp09eoNQDUENNTclHJGfNlYCAAEJDQ6lSpUrhdiwgICDwDWBiYkKFChXo06fPO9vs3buX1NTUt7YPGzYsWyQ5OTnRq1evfI1dIJE0fvx4pk6dytChQz+qJkq+0GzDnH88mK+vl5WNWoTJsINc+TyjC3yhSGqOY17j3vRvfI3yWi+JtZ7Jpg5mSGKrsWRsazpYSpHINFFvDIjNqVrBh5ltXLE4P4fGhaSwV69ezdChQ7NXaggICAgI5B0NDQ1KlChBuXLl3tnmXTFGTk5OlClThpiYGFatWpVvzVIgkdSqVStSU1M5depUoUwX5A0R2vp5SPIjIJATkQltf79A66Ro4lV6GOpm/eQH7+VOr0jCZSUpU0odBWIQwbDDTxmsgMKqPxkREcGWLVvw9fUtnA4FBAQEBPKMRCJh0KBB6OrqUqFChfwfX5BBRSIRq1atom/fvhw+fBhHx/ckOBQQKAZIdAx5M4JOVKI0ZbJW1P2niUSFJpAiIyNp0aIFEyZMwMLConA6FRAQEBDIlbt373LhwgVevHg9P19ycjIJCQkMHz6cpk2b0r9//zz3WeDAklatWrFt2za6dOmCu7v7R1XZFRD42ggMDKR58+b07NmTWbPyUn9PQEBAQOBjUFdXR1NTEwMDg9f+K1++PMbGxgwcOJDk5PeX93qTj1ri0KZNG3bs2MGoUaOyi/d1794dHR2dDx8sIPCVkZGRwaVLl1i1ahWXL19m+vTpr62yEBAQEBD4dFhZWZGUlMSYMWPe2rdmzRoSEhLy3edHrwNt3bo1jx494tSpU6xcuZLhw4cjFotfS+j0rZGRkYFIJHptiaLA141cLic1NZWqVasyduxYtm/fTokSeawoLCAgUCxJT09n5cqVjBkz5pM90xIfePLcuA7276z5lMK5BX8iGzeL9p8xLPfhw4f4+vq+d0VZcUNdXR0tLa1c0xMVdOFMoSTLEIvFtG/fnvbt26NSqUhJSckuQvctMmHCBOrUqcOAAQOK2hSBz4RYLEZHR+ezJFgVEBD49MjlcgYNGoSrq+snEkhynpxZz+rF+9H5/dQ7RVJ64DWe27Sn+2det2Rra8utW7dYsGABM2fO/LyDFyMKPaOYmpoaJUqU+Kbfol8tV8yZyE1AoNijCMbzbATlW9fBTATIY3jkfZPnspLY1K6Fha4IZZgPZ4JK07J++XzcPGTEhEaRJAc1sRiptj5GpbTzmAPrDVKiCI4Gg7KGKMMDCMMMmzK6BekpB0qSQ+7i/yQe9bJVqG79sfnVUojKNBLjEu8WzWlxoUSklaSsiU7BzsUXSOKNVUz/24MkdS00RXJSM8rRbYErnXU9WTX9bzySjWk95TcGVJEC8VxfORM3j0QMWk1jsUsVJMk3WPnTajxTStFo/AJG1vl0oR3btm3D0dERW1vbj+hFSdgmF6YoFrF1WNk3goAlWLceQq9Ll/gna0vKMw+u3I9BCYCIkpUcaaj9iPPnzOnb8yPMyAvxt9n1lzulJk3GKUuQ9erVi169evHkyZNvNlmt8NorICAAKHi6Zji9Vt0EQHZ7Db2rWlDZ8XvatqxHxQrfMf1MFEq1W6x2HsLfjxR571rmzqSallhYmGNeriymhvqYNfyJE+HKfFuZuHcolSoNYW9iMgdGVaf6yLerA+aPOC7OaoylpQNNWjanXqXy2PfdTIAClLE+bPhhPGvuy/NrJEMrVWLI3sTc98sfsnmgPaZG5bAwM8Gm22pufyOOd239BLx37Sei3jxWrxiC2fMw1PRFoK1Pgvcu9gVb0szulUTVoVTSXY4d3sXmU/dIR87dVa78tmMHu+8Z8V3NTxv7unXr1nwnHnwLxVN2HZbTuqNZ3h628nRkMln2f+kZSlI8byGqXTf/wj39X5aM+YVN10JJ+0DTZK8tzF68lBXrzvPijUu7R48ebNu2Lb+jfzUIIklAQADSrrHifz7U7tUNM+VtlgyfxHGdIRx+mkDKC3cmWNxi+eyNPDbuSu+6t1ix8gqyfA0gxmb0YR4E3Mdj+0AMfZYzZ/2DrH3JBPlc5NyVO0S8djd/13YAbVr+cpCDv7YClMQF3edhaCJJz7y4dPUO4TmMSwm9xZXLNwlJjOTJ7QeEpeToJvEEy5d6UWGmH3FpsfjNr0vUif+xySuRqPMrmb3mAg+ehxGvANLCuXP5NKfO3SAg9r8niTzmETcuXsTzSRxvScf0aALv3SMg/L8VNWlXVjB3dyrdDwQTeqw/ouOLWHXpQ4+xr4NU/1s8wpZ69bS4/6+C0WfW0kkbSPXn1iOwadgo04sJQAq37qhRt64BsqgI4oJ38Ns5HWqXllC2fiMqfkL3m1wuJyEhgdKlSwOQHrCb6aNX4ZUOimd7cV1xnXRlFJeWjMX1cOi7+7mzgzOGPelsIgJSuLNrEQuW/skvE35h35MMom+d4ITXXXyP/8PtWCXaFRvTuWtXunbtSteunWleWZeE8Dgi77jjGZ3bS8Wbfeb4BUq/Y9RPzUk79BPduw5jwd6bRL/j3aZEXRdmzxpGI2PxWxXFv/vuOzw9PfN3Ar8iil0Bn/DwcEJDQ7G2tkZPr2iSR8bExPDs2bPsZYMCAl876R4HOP6iGj8010fxbD0nfVU0XjmT9pbaiGjFfPcgXPUM0RUr0W/uQMgf+7i2uBktNPI+hljXFCtrO5DbYCyGhPQ0SPZicdcu/PKvDH2NJFLM+rP+5FqcjX1z3d4uu7cUzs3vxijV38iO9eDwuFr88MCGCmoyXoY+IbHaPC5fnkbp42NpNXALL03KU1JDRdJjEf0v+LPYMeu9XMMcyzJwYt1IhiZ0wanNap7EVMFA4cGMETsJyZCzsscgrHzmEzuwHX+GlMFK/IKHinZs8tqG092ZtOm1nKclDFFGyamzyJ0zQ7NMVLxgz7DODL5QjWXuu6lomjVk8+Xci0hAoaeP8rSKDDUd9HQ/U+WCIiUdf08/EkVw44/O7IntgnvzJpl7/D3xSzKkUb3K/z2U0m/hFWRJ8/ovuXzmKXsW3seoV12ejjlLzfq1ePOnlxzsh+edCMTla9LA3iTT86JMJioinZJmpbLbKxIjeanUx0RPQnJUBOklzSj1304iXypJSIt/beGN1Nwc+fU93E0dTVn3Law+1o5RY2sie/YCUcvS7/i+adzYfo2KfaegDyBzZ8mKKIZcWIzd9aPc1hVjaN2Vuae7vueciSgzYhtH5BI0pLn4NN7q8/XfUYkKLRm1uCWjkp5wZuvfTOj5OwZNhjJ5XCss8vj0L1myJI8ePcpb46+QYudJ2rRpE48ePSrSiulSqRRvb2/27/9YV76AwJeAkpg7d3mhX4GKRiIUUVFEo4WhUcnsG4SmgSG6YgARBtYVMIi4i3++psuUBKzpgnU5M0wcpnNDvxXDe1clbKcr865aMutGMGEBB+mdvo0pC90JzHX72fd4r1SkStvw9637eP3enDTfi1yPesaOJZt44fgXXg/vc2WaPRlvpnOTNmbuvvWMqyPn6vpfGdK2OtYNpnImsS6uy4ZhIXVgxtXT/FDWkMY//s3xS2fYPq8z5SKvc/V2AFvmr+BRvaX4PX/KlTWDcRBFZ3Ws4P4KZ4YfNGTqwe2MqJzzkS5Cs5Q+anfdGDhyC6ltJjGy7uesMllEKMPw9H6OerNJbF44jH6dmlFGBKAkzNOb55Lq1MtxHhQhntwuVY+OFUxRPd7ImpRBjJH4c5cq1KufIw5NEczRyU2o03kWO/45zrqxzXAcdZAQJch9F9LSuilzvV7NZ6ZxeUZTBu2MQSn3ZWFLa5rO9SJ77+UZNB20k+h0OVJpjn8TSRnKGCUQHeHL3mdmNBVHEhp+hNMaLoyp/g61kXSW7bdq0a9JVrVsaRNc2txlUueZ3LDqQIvSeXz8iqS5C6T89KljTdNuPXGqps6jc1cJzIfjUkNDg4yMjLwf8JVR7EQSgKmpKdrahVyGHSDlKVePHeNa4PsnCnR0dLLdrAICXz8qkpNTQasE2mogNjHFWC2J0BdRWQGkCp4e/oslu7yIVIKatjaayEiV5SeBrAiTZmOZv2gJbrvP4X//OGOrQNCT56SZ1KKRrTYiwyY0qiImIjCA+wG5bX9Kwnt0mdjCFjtNEXqGBmggR54WRkSUCv0KlTARiTCwtcHkjTueIvgKh/5V0dnNk+CoF/hs6ovxzVUsPxL3RsOX+Oz7jf7NWzN26x2SUaKQhxAUKsfA2gYTkRR7l4X8+UPzrAPkBD4JRax4wYMnibxpdor/Snq1ncB1m9kc2jqMCt9C5HaKJx63ldjUq4exRV9+HV09y2uUgqfHbZSV6lHfCMJ3DmHA389J8fAhvUYDLMqWRV+jLuN/7UCwhy+yMrWpb/HqhCl4tHoYP93rwb5rx1i/fCXb3LfQ2vM33G6lE+N7E3ktPc6sPkEsgCIEn9uaONQygBhfbsproXdmNScydxLicxtNh1qYl9J7PemgqDRmpVO4s3Yfyo5DqKwK5PDfPtQc1xnpnf24HfIn4OJG3I4+yI7/iTm+m6Bm/amT6dIi9kUSNX49wZEBgcxe4P7BOKFcUSYS9jwq69i89JlOhNdO5g7vQe8ZJ1C2/Z2jx1xp9o51VSqVCt64rJOSkopsVqc4UCxF0idDmsy51QcJM/h2czgJCLyNGEMTI9QT4ohTgtiiIz0aaXF1xRTcLt3j/qUVTBr/M3M23CBJBcrYWBLUTShTOn9Pdl3b1vTp3xfnLs2w0xcBEipUtkE79F+O34giJfAw//gpsKxWA4cquW2vjv777lgiUeYN7dWMg6QCVWy1CDu/i0Pe3hzaeIInb8ZkpFxnzeSRjPp1J17B8chkaWSoaaClLQaRCDVVGmkpaSS4r2LhCQ1G/HOLf2Y0oiSA2IJKllIivC9zJymOi1O/w679kqyOpXw39wq7Bmtx2PU3LuaM4Y47xZSekzit1pYfR9oT/e9p/AoQxP5lkc6djes4F6ciPTbmNbGbfmcj687FQdJ1lo/oQbtJ3pSrFs369ed5mZSE6rsp7D29gaEcYM2JEJSkkPCqg3RvNq6PpvdvY7B/dVvXqMe8G9eYU1OFr/djbFxcaXnfjW1PFZDqjU9wZWpXlZLh681jGxdcW97HbdtTFKTi7RNM5dpVMTM0RF1dPUclCQlmxon4qZwYXs8SE9U1HtmOpa+ljCiVjDtr/uKqbkXCNrtxNQ1QhrB/fyJt+9hlCcFE3F0HMmn5do7dElH3u8oFi3WRHWV8hwV4pOehz3QvlvXtxoTd8TSasYWDG+cysIFZrgHgyojr7Fi2hnPP/Njz23KO3fsvcC80NJRatWoVxNqvgmIXk/SKFH83pp+yZ/EkXTb9fh+nyTU4+/MBrBbMpEVBswsk+fOopCOjPnbFsIDAV4Zu3XpUSTnKvecKOtvaMHbDBgIH/sCk5lVJQx0Dh/6sWDmSCmIFD+48IKlyR+qV/NhRRRj3ns/i0z2Z3MqMZSoRxo2msG1aI8rqGeSyvSHSk/np3oQ+83/nTN+ZDGt9ie/aWFJaFIJaDqElth3NX7/dYMDsQdTfoAB1feydlzC7qx6SsKrYaq/mj6ZNkR4eQyvTA/zeqiLbypog1ownNESPxXN/5UgPV+rpz0dNw5LuKzoA1wE1SpYqR+uZM3HaN57p/xvKlV8ckKIkYv8Ktj7OQK46wvReR0BUir57Q9nR/Wt+eZNi/4M7kT/kssf+B9xz23E2jIlZH5sYAAxg3/M3cs/F3+J2Ui3GV3n9USaRSkFxD5+7ejiMbcyAxIX0Xe/D0E7e3CtfiwVaCp743EXPYSyNBySysO96fIZ2wvteeWotyEw6WLlyZby9valbty4gpdGCf/lXUx8dMQzb68NwPT3EQAWDeBIr9cbZXsEMqSmmElAE7OaYuAvrsz1eejhvOkfHuDgyNAcwUrNg/okU/4eoWjpRXZqHPqXVGL7+KD9qf3gskUlD+k1tSL+pb+87fvw4zZs3f3vHN0KxFUnaFUsR6/uQuDvJHDkViN1YY0JK2NEpD0kzFcEXuZz4Hc3fuHDSbvqQUnkgBiJAEcTuBcfoPWvsp/kCAgJfEGKbHvSs/Renz4UyzbY8EqvuLLvShXlhz4jIKEV5c4PMwFdlCOcvPqZm9+7Y5NWRpNmBDSEpqNQkb99wNKoybOcd+i4NJDhFH0srw6wA23dsH3QU2aCsY4/KePVxUI7P9NpDfC+AdK4f9Mew6/+4NqMPZY8PwP5wCkbGOQ3Xpd7EgzwcG0dISBwig3KY6WVZaTmKY4HteRanTXlLQzQeduRZuApTcyMk6TLkIk00pdM4/XQ4z55EIS5jTXk9CWDL0WwjB3M4anCO8USYDDtB4rA8njuB96JUKlCIRK/lmVJGXWbrERVtugXjG2nPIFsNKpgNxrLtJo6aPCe92hTKihPZ5xuJ/SBbNCqYMdiyLZuOmvA8vRpTymb2NnnyZJYuXcqmTZsAkOjo8yrpgGaO6acUD0/uRcvYvTQc48FTsRPL8d95BqNeu9+Y3hWjrf92Juj8oCg9kJV/VswMBP9gn5qU+MioldTUVM6cOcPx48c/rqMvmOI73SYxpbTkMTsvlqB11RS89nhi4dIFYxEkh97B504IyUqANGIjYjPnYtNiiIiNJ2j/Hyw9+4Kk1zzYCoK9n2Fcxw5J+n0uX8+gca+2gJy4qFjiw+7j/yiqYPPEAgJfOmIbhk7rRvCObdzPnpISo2tmTcVXAglQPNzGzuedmTrMLh8JEEWIJRIk7zxAhLaJNbbZAulD2/OKBCtzEVdXDKaOoR7l+h7HoN94nC1zMUSqT1kry/8E0qse9MtT0TJrfE1jLC1LoykSIdHURvNVMK3UAMvKtlkCSeBzIjKsSx3dKxy6mJWAURmJ++wfWflYgtYdbx5UqE1tKYhKd2VEQz8WbwrArrYDGun+eD+oQO3MnXQd0RC/xZsIsKuNQ9aPrXr16rRu3Zr58+e/J3A5HT/PDLpMG0mfKYuZ3sYYERLsJuxiVVf9dxxTcHQrVMTsM/3MXr58ycSJE/nrr7++6TJjxVckiU0xjvEgqWEP6ukFcE3UhX42aoQdGEPPybs5v28aPUfu5HnqLZaMXoR3OqR7LWLEoovcexhGiO8Nnqbe469BWenUZQ85dCqIpPubWfLjT+wOjWHLjL9A8YhVfZsyYMFu9s/phvOKgAKZq1QqadKkCebm5qxfv55hw4Zhbm7O+PHjC/GkCAh8OvTb/c2tc5Oxe4/6EdtM4uztdXQy+Hx2FRwRpp1W4BsRybM7t3gUHsndDb2x+BaCpL8VJLWYtNSZgPENaNiuM20aOvE7k9g2pyHxPv6oO9TGSARQgmbDnVB7rItD7ZIow3zwV3egduZOSjQbjpPaY3QdapNzFrlv3764uLigpvauFA0iynXoiaMmaOT4XWmWKkUeZrmKNWpqavzxxx9Ur169qE0pUorvq4+4Ej/uO4GkhDY4nGC3piYS4nDfE07nlasZWToWox6DOBE5/fXjRCY41rGikmYn7JNOcdC6ceZ2zSpMOeud1WgUKO7x2+6sP6X1GPrnHNo/ltB11UOgYr7NFYlEDBgwgBEjRmRvCw4OZtgwwa8u8KUgRqr5AQUhlvKhJsUOqT5lrAr/rV6gOCDCoOks3O/9QPDTl4jNLCmjm/VYm3iemzlaShxm45M8O+uviZx/fSezfZKZzduUL1/+PeNLsGrSFauP+g7Fk9yKxH6LFGOtK0KjRGZ9J7GmZlZEvhh1iYK0NCUo00hTSNCQShCpFCgAVXIyqSqyV7eITLox+1enD46kpqGJphqoSSSI8rOq+Q1cXFywsLDI/rtbt244ODgUvEMBAYG8o0wmKiKefBYRKRoSw3gWmfLGRgWJ4aHEfiMlSgoViT7lbSr+J5AEBAqJYieS2rVrR2RkJGFhYbns1aXdiFpcGjuEsSPHcqbycDoZW1HLxIP5o0YzZp0fcjXQKmPCkzXzOBFVsGW1QUFBpKWl0axZs3wdJ5VKX6uWPGvWrAKNLyAgkH+UL7cxqMOf3C5GKkkRuJNxrsffSoIpOzObIStv8poeUoazeWAXlt4rRl9AQOAbp9jJbgcHh/d6X/SbuXKgSRqyDHU0NTI1Xhe3i7SKjkdUSh8NRIhFDfD4XoFI/B4NKK7CjIMrAbL/T+XpHFmT+dHc3LxA9ru4uLBgwQJq164teJEEBIoKeQwPvfx4JtOjct06mOukEfk0FHE5KwylgDKOoGdplK5ggmbSM7y8niAvX5t6FfURK2IJeiFHMyWA51rVqGuuINDHjycyI+zr2WP6zihyJfGBPvg9kWFkXw97UxXh3u6c8axGQHhTyqbHINdMIeC5FlVb/sI6R4Os0hlxBHj4EWlg+lrSyaRnXng9kVO+dj0q6n9pc5wCAl8HhS6SVCoVCQkJyOWf/m0oOemNDbGxpH7yUT/MhAkTqFWrFtHR0R9u/JWiqamJtrb2ewIeBQSyUIRywW0xO0KbMqF1GFu2+lB22FImOhYwoVnKNWZ3+gGvam2onn6D8RNqsPbGbOLnd2Bb84vsHWCM7Mx0um1txj+zEunfZyPSZo3QujmN3zptYG/f64xstoy4MmUxbTWWbs8WslG9NY00fJg4rQHrL86h/lsZ+ZSE7x1Mx7/Vad1IA5+J02iwZgs1r9wj9mkMB642xni9M8vjylDW9HumdPBn0s1x+C01Y1OP3mwv2ZJaMh9O+svpi5yH63vTZ6OUZo20uDntNzpt2M+PDjnVWQp3di3nSLiE1GdxOPwwh57WeRNScrkciaTYvR8LVzAm4AAAIABJREFUCORKRkYGGhoFW99aGBTKlaJSqTh//jwrV67kxIkTaGhovF735htDpVJ98+IgNTUVdXV1+vfvz7hx46hcuXJRmyRQTFEmxmPcqiHp3Zazo956pjY6TefTgYx3rF6gG5QyVUKVCev5qWs1CDpGdNvl+EWWYHS/5sxafoSIfr3x2udFjd7zub3iO+L6H2LfEFPUEqow9Pv/cbpLPVSK+vx8egudNW8xu1Ea5kNb0b/rZAYGviB36aYk4t4D0syH0qp/VyYPDOSFrgWVezhiElaXKV0t2bpOQf2fT7Olsw4p27sDkH5jHW4Zo/hn8yjMlP4YOA5BmX6FFUvi6H9oH0NM1UioMpTv/3eaHzd2+m+4DxQ2zY2YmBgGDBjAjBkzWLRo0Td9jxb4Mnjy5AlnzpyhZ8+elCz50dlrC8RHi6TIyEjat2+PTCZj3LhxbN26FV1dIaW1ALx48YK1a9fSvHlznJycWL9+vfAGK/AWIv3K2AXt4al5N+Z3LEfgzFjs6lnnenNSxN7n4lkPglV2tOnWINecMSIpRBz8kRaL1TAqV5rEFCW2KhWajfvjNG02B+7pcsO/AYNXavBiQzhP/abgcjFLZFSpgSQDxCblKacJSByYvO5nFi9dTP/f75JaZTRu2xwwe0tfSHCYvI6fFy9lcf/fuZtahdFu23jt1UBsQvlyr+ebkYeFkWbWMjPBrcgaa3MJj+UveBH+FL8pLvxn1htfVNoElzZuTOo8k1lr5tExD8VSZ86cydKlS+nWrRs///yzIJQEijVPnjxh0aJFODs7c/ToURYvXlwkdnxU4HZkZCQtW7bEyckJf39/Ro4cKQgkgWzKlSvH3Llzefr0KWFhYQwYMOCzTMMKfGkoeXndB/UmrSiresG/3iWp31AL4v3Z73YI/4CLbHQ7yoM0JVHXrxBRtSs17rnyy/HcC1WnnvyTxanDOXXtAic39MdGTYlSCUjr0K9jMgdmbCawWV8aa0qpWNGcsh3+4ODRoxzeNIYGVhaUKwGglnlzlPuxc3MUTqv+wfvxdcakb2S7R24pZ+X47dxMlNMq/vF+zPUx6Wzc7kGamhpqSlV2zVC1NytH2Nqgd9+Hx+mAzJ/bj+Wgbk1F87J0+OMgR48eZtOYBlhZlHvtfBWkWKqdnR0TJ07k4MGDtGnThp9//pn0dGEpnUDxIzeBVFRTbgUWSSqVik6dOtG5c2fmzp37zU8vCbwbbW1tDh8+THR0NDNmzChqcwSKHTJu3AijSl0rxKpEklLDuLDcjRMPFMjurOGvq7pUDNuM29UMTNuNoK/NS3yjKvN97dy9INLqTbC/s5Rhw4czYMhugnXiCA9XABKq9uuM/GI4rfrWQ4qUBpPmU+90bxq17cz3bX7F16o2NjmdNhIrbLQPMbJZO7p17s1msQsDHcXcnVcf2x8u5myIlY02h0Y2o123zvTeLMZloCPa5lWp4DOTNrNv5JqaQGI/lnlNz9OvSVvatZvL5QwRatIGTJpfj9O9G9G28/e0+dUXq9o2OY4qeLFUQSgJFHcKKpA+2e9Y9R569eqlcnZ2VikUirf2Xb58WWVra5vrPgGB3AgKClKVKlVKlZCQUNSmFCm9evVSbdq0SXXy5MlC6a9ChQqqgICAQumrOKCQyVRpKpVKpQhSrRw4TvVPqkx1fqKzauFduUqlCFEdmzNVte5m0vv7SAxRPQ4IUyXJVSpFWqpKJs/cLn/8p+r71otVD+U5W2eoEl9GqhLS3t1fRmKkKjw6WfXqsIzHy1ST/riTW0NVZHi0Kjln/wq5Sv6B22RGYrQqNlXx5kbVy8gEVe5myVXJsS9VcW8eo1KpjIyMVIAqJibmnePdv39fNWLECNWpU6dUEydOVKWlvefLCwh8JgICAlTDhg1TnTlzRjV+/HiVTCbL03E+Pj6qMWPGqPbs2aNav359rm3++usv1fHjx1Vubm75sqnAnqSVK1cyduxYRKLCS7WUHP6UwOwEazJiQoMJCgoiODiUyMQPqMS0eCLDI4grpOJrsphQgoOCCA4OITQilhTFh4/JnRSigoOJSs5HzqbcEs2lRBNeWF+ORMKeRfL6CJ8+kV358uVp0aIF27Zt+3SDCHzxiDQ0MpfGp3jgeS+au7uXctJ4MEPt1AjZ9iOzL0fht6IPP2yPfXcfOmWoaG1KCTGIpJpoiBU83+RCrbZ7sZ869I3ivBJ0DI3RfU94jkTHGBMD7ex6dWnyJgwbVTW3hhibGKCds3+RmPdlI8k8zAD9N6u4S3QwNNYld7MyC5vqFbCavOBREihuFNSD5Ovry4YNG/j+++/x9/dn0KBBHzwmPxRY4Zw4cYI+ffoUmiGKkP2MbliZVvM9MzfI3JlU0xILCwvMzctiqm+AVRtXzkbkJjbk3P69JeXLlMPx1xt8/KUuw31STSwtLDA3L0dZU0P0zRry04nw/HeVuJehlSoxZG9i3kd/lWhOEcjOca4cl0HKwRG0+PVavgrwviuRHbIzzB6ykps5T9RnSmTXt29fTpw48f5GOUViShBeZ05zIzD5k9olUPxI9/Mko8s0RvaZwuLpbTAWiSjrshfvsxtZtf4oy/uXykdvYiwGruaS31WWtMrPcblTwq4mlb/w8EtBKAkUFwpDIHl5eTFnzhzE4sLNKVYgkZSeno5MJiuk2i4Knh2djpOjC7uC3xRAYmxGH+ZBwD089v+Ipc9vDJp+nLi3DPJi6y5/tPRK8Hj3Bs5m5U9KCn3IvScRWSJBSVzQfe4HxaEAUkJvceXyTUISI3ly+wFhb1YIAMQ2ozn8IID7HtsZaOjD8jnrs/clB/lw8dwV7kS8Llvetf01EkMJCMkSTSnhPHkeiwIgPZpnz2OQtvyFdWMcUIZ7437Gk4cB4VleHzkxD69z8dojYt/ybCmJD/Ti4tkr3AlPA2RZieweEhCeSGzQc6Ii73Pd6xlJ0pb8sm4MDllJ9QKuX+Da44TXEtmR9AyvC+e4HpB5vgoLExMTYmJi3tvmv2zESRwd14ohq47h+aywvGhvEH+bXa5LOBX/aboXKDiich3o+Wbl0I9BXAJ9HWF1ZU4EoSRQ1HwugaRSFazmWIHuGAqFAnV19UIK1pYTdDsYu+mrqLp6FMfe2CvWNcXKujJS61+YdnIznf45zo20TjjlOIfJ5zez96kNw1d34PTo9Ww6uoh2fUsRf3wSjpPTmH/XnXGG7kz9rgveg69zpOZanAZu4aVJeUpqqEh6LKL/BX8WO0rfHBxTK2vskGNjLIaEdCAZr8Vd6fLLv8j0NUhKMaP/+pOsdTbGN7ft7XI5f6FbGTJMydqLP6O/bTC1lthx/M5SahybSI/zA7nc8G86eozmcMsr3It9SsyB6zhZqXh5chYj42thFnWekaYL+HdDN4xFkN9Edr+us2ZFRw/GeY3lYb83E9mB/OF6er+RYG//jw4UxtoCqVT6wRVu0qxsxMrwm3g+1KXVjB/p41iwsvOKwJ2suOHI+D6WvHn5JHttYfGRS7hvjGDIhMkF6l/g0yGxakLXr6JyaCIPPJ9jXMcew3e+lqZwbsGfyMbNor3e57TtP6GUMz3A+PHjhcU4Ap+c6Oho3NzcPrlAev78OXfv3sXGxibfaWiKwWuVBk1mbqeJ3I9f/35fOwlGpUtB7EtiXnNtxHJy62HC7UczoF8v9NcuY9HmPTzvPQaLnoNpP8uFPXsD6FdhB8diGjC1nw57hm7iheMybp8cgXRHD+yH3st1RGXAGrpY74Ckl0Qml6LN0t4ow3biOu8qlrNucGZ6WS6MrE/XKQvpUa8OK3LZ3qtFw7f6FVu3p6VqKudCR2N4KZFKJX24+CiRxNMPqd3FEWL+BsRYtOuBo0kYdad0xergNtRr/8DWrX3Qj9tMV8fz3M7oRgsNyG8iO1K2swLI8MglkR1pXF6x5O0EeyM20km7oP/G+UN27MdMkdjqIjdCIkk/dAjfWpNpY5b1hEm5w67lRwiXpPIszoH/ze/538HKeILuPydWrgKUBG49RnDjyty5rcK8qhWlcjykStR1YbZDJZKPLcouiixQnEkk7FkqepalyflTVCSGEyE3oEypHC85KdGEp+tgql90mXoBkD/hzPrVLN6vw++n3i2S0gOv8dymPd0/s0B6RU6h5OzszOHDh4HMF2KZTIa6ujpyuRxt7c90E/iKiI2NJSMjAz09vSLNHF0cEYlEHyWQPD09mTt37gcF0ty5c+nXrx8HDhzId76lAoukgrquCk464eExYGCMYY7zoQw7yNaTL1FJtjOgwSFSQ1Ukx29h+/2RzKzajqE9TGm/Zz3Ly59E1mIxfcxfsiRKhX61SpiIRKjZ2mAiyl0kiUyaMXZ+Dypo6lDGoRlN7PSRe2zleZoJLRrZoi3SoEmjKoi3BhJwXz/X7U8T6r/dsaQy7Zsl8of7afQjHPmpuw9bz5wg6m412i/VhkO5WaOGnokpWgBSbTTJyBF7VbBEdorwXBLZkcGLF7kk2EsHCuH+mPffjRiLjsNov/wSKZMn/SeQAJn7ElZEDeHCYjuuH739+mHKEDwO7+NWqgpkdzl9XZfWWgfY52dLVxsragv3qC8X2RlmD7mJy6m5/Of0VRK+eSBdo/7g+uB7TNhckj/ndEB5cAQtPMbht6J5oXhA34syjE0uU1As2sqwsm8GX1vTekgvLl36J2tDCs88rnA/JmtyW1SSSo6NqKDxiPPnzOnbkyLjlVBasmQJJUqUyN4eFxfHkydPMDMzIyoqimrVqhXqgp2vnUOHDhEUFES3bt0oX758UZtT7AgNDf2sAim/Qr9AIkkqlZKRkYFCoSj0IKk3kYXf5fq/acgCD7PwYBTl+/SgUfa5VPBs93bOp1Wjz88u1NAA5A/YM28T2zd7MGWxI02G9sV23RL+uKdH163dMJWmUcVWi7DzuzjkrYfaxhM8eVfQja4trfv0f61Ok6RCZWy0Q/n3+A2i6plz6h8/FJa9qOFQJdft1fVF/PNWxxKqt3MkdOIywmu6sqS1lOVDfuN6TVcWlMjZTg01NSUf1hU5EtnN1SRkVVuabfdgXP/3J7JTt7FBb5UPj9PbYa/MTGSnSVaCvfp/cHB6ZUTRp1j0WwjldD5kQ95ISUlBS0vro/qQNnGhjdskOs+cxZp5HV/fKSpL/S49sZGrSDzzjOC+o3FurA2ikpirf9SwAp+BxNAAEnQrUlYXUsKfEKVhiUUpMenRzwhTNeWXdY4YZFaFJS7AA79IA0yVAGk5ism2oALwKobvYbQhDvVtKCUGZXwgPn5PkBnZU8/etFAElOLpLg7LW7PWLG/CQZ4uQyZ7JZI0yVAqkXneQlS7+ztWsn0+7OzsWLdu3Vvb/f39cXNzw8XFhcuXL7NgwQIhe34euXfvHkFBQYwdO5YWLVoUtTlfLEUhkKCAIkksFlO6dGkCAgKwtbUtSBd5REHg1qE03yZGvWRZanb+i12/tSb7eS2/y9Yd1xA1/R+/TR1F5ktcLGV9D9Fv90bOuDrSvvogBjZcwrSgbgxuqw8i6DP/d870ncmw1pf4ro0lpUUhbwmIdyEy7s38xafpObkVZstUiIwbMWXbNBqV1cMgl+0NpSdz7Udaux21ojcS1rQhutWk1Excy8u2LdGF/5bmi82pWsGHmW1cMRn2PqsyE9lNHNmM05VMSAwW47LMEe1ywVmJ7ErSO5ejxFXHMq9pL/o1uUZZbYjJENFGLSvBXt/eNLpsiW5UKPqD1jGpkO6Hjx49okyZMgXvQBnLi6Qa/HriCLv6dWSB+/esbZfjUad8wfUDu/BLesHl83HUbXWEXbsAiR3df7GirubbXapUKvjcjlGBXFAQunUIw5RrufizPtsG12KJ3XHuLK3BsYk9ON9rDOFT/RjntxSzTT3ovb0kLWvJ8Dnpj7x3NF6vYvCud2USb8fwXV4gZ2rnv1Fv3QgNn4lMa7Cei3PqZwmTdAJ2u7Ikuhv/G1uL0L1zOVZ+JmMrXmfp7PNYz3SlS5ncbhJy7uw4g2HPXZiIeHsqeG4Lbp04gdfdm2j8cxvL9tWo2LgzFV/rQ0loeByR993xjO5Hg3cHLhUZ1atXZ9SoUbi5udG+fXtmzpzJkCFDitqsL4Lk5MyVucHBwTx8+LCIrfkyCQkJ4cCBA59dIAGoqd4z/+Hs7Iyamho7d+58y706bdo05HI5S5YsKdDAhYMShVyJSk2CJOf5UiqQK0EkEeeyfC+d6ysnsjPSkeEz+lD2+ADs+z9j4t3LTM1RRVupkKNUqSGR5P4PoUyJIDA4BX1LKww1Prz9o76jAsQfSrQCIE8iKjqdEkY58rQoFSh4f54WeVIMSRL9N/K0yEmKjkWl+/78MflBpVJRp04d5s2bR7t2uUS0Z5GyvTt1PMbh9z8rVrceQMrKi8ysnPWF4vcwuPNpHAbXJ/S0H/YLVjHQ6u1/I9m/s5h8ZwjLR70dsP0KZcR1dm1x48/55zAaMZUfhgyjY5VPH3Ph7OxM27ZtMTExoW3bth/dn7W1Ne7u7lhbWxeCdUWL/PYcmk01YtdOQ6Z1/JNHos5sPlGb5a2O0OVEYzY08WCchzPHGy+k0j/HGGWmxH+2I0OUa7nWajO1V9TFY18/2N6NSkd7cnfvqxg+b8ZsMeKXoYEMXTadrg4iAl/oUsPBLNt7k35tCvUWV+XSge/Z09GB/e38OTXUnwnONxh6wJU3y6cBkPYvUzrtoc2hFbTSBtnhwbS48t9UcLMuefQcKNNJk0vQkH68QDI2Nubly5fExMRQqtTHpzvIySuPUo8ePXjw4AGQWcxaqVSiUqmQSCRoaubyJvINs2LFCh48eMAPP/yQJ6dCREQEGRkZmJiYoK4uuL8B1NXVKV269GcXSPARMUmjR4+mTp06zJo1Cz29Ioo2RIRYkstNRSQmt82ZSLAyF3H118GsWTIKcYaICgPW4Gz5+kkXiSXvzY8g0jbBOpff+7u2FxwReZ7RlOhgbPLm4eJ3ioT/DjNA/+2t6Bga53HgvHH16lXi4+NxcnJ6bzvt/ge41z/z88RzV17fqefMpnMdiYvLQHPASN6VS09pNYgZtd4tkABEJg3pN7Uh/abm/TsIfFokldvTLPEP3E/rE+H4E919tnLmRBR3q7VnqXYCGwDkYYSlmdEyM5gOa2tzJI/f7OntGD5F9cms+3kxSxf35/e7qVQZ7ca2HCJJUqYMRgnRRPju5ZlZU8SRoYQfOY2Gy3Sqv+NOmXR2O7dqDWJB1j34vVPB70MkRaOo59rywCuP0ooVK1578Dx79iw7BENHR4dy5cq9p5dvi5yepLywY8cOoqOj6devXyGl2fk6MDY2/uwCCT5CJFlaWtK3b186dOjAyZMnv6DCtiJMO63AN2IeoSExKPTKUt5AiOb91Dx48ABnZ2eWLl368UGfYm30P3Dv0C5boTDizAU+N5LqtHMMZeKycGq6LqG1dDlDfrtOTdcFlOBIZhupLTZ6y/B5nE47eyX+tx8j1wRyFJPNbbGi4uZONkc5seqfuWiGrKJts+14TGpMm6zLX1TajNIpp1m7rxrdh1TG1+0wf6fWZNxCKXf2u/HEphWlvM+S4DiYTnYaQAzHdwfR7Oc6mULrQ1PBXwnVq1fPNW5py5YtxMfHk5KSQsWKFenRo0cRWFf8uH//PsHBwYwbNy5PMUkXLlzIrnNZpUqVz2Dh18GnEEjwkSkAli1bxsiRI2nbti1btmz5stz9Un3KWL3tPxEoXFQqFZcvX6Zv374sWLCAXr16FbVJAsUaKbXb1SJ6YxhNG+pSTVqTxLUvadsyx0uYxJ6x85rSq18TrmUG0yFqo4b4VTFZVwuOV3q7Z7GlDdrTRtLsdCVMEoMRuyzDMaeGkZhhnOiHymk59Sy3oLp2Bdtzs7GUBXJLdoc1f2mxYEgYy92u0mlZC5Qh+9mf2JbFdlm30UR3XAdmTQWL6vJd5W8rsNnFxYUtW7YAEBAQwObNm6lVq1YRW1X0JCZmJg5+8uQJRkZGH2yfmpoKwMOHDz+YU04gk6SkJDZs2PCWQFKEXsBt8Q5Cm06gddgWtvqUZdjSiTjmw6fzUVexSCRizZo1zJs3jwYNGlCvXj369++PmZmZkA/iG0alUpGamsqjR49Yu3YtycnJLFu2jJ49i3B9s8AXg0az5QS8zPqj5SoCI1/t6c+BV/OwrX/jgk8SMUkS9PU1s6bGHTj8dHBWDF7j7ClbtHux616mOHe66MfPUdGklzDCQPsNt720EQv+/RdNfR3EDGOvz3D09MRABQziE6nU2xl7xQykpqaAgoDdxxB3WY/Fq27yOBX8NZNTKJUsWRIvL68itqjoSUhIADJFT1686K9E0p07dz5YnQAypzoVCgWWlpaffLV5ceZtD5KSxHhjWjVMp9vyHdRbP5VGpztzOnA8ju+aP8+Fj37VEYlEuLq6MnXqVHbt2sWePXuIiYkhIyPjY7v+YgkLC0NLSwt9/W/XU6WlpYWZmRkLFy7k+++/F/KqCBQ+Eh0M3rzEPhiDJ0HnrcC9HPv0X62d1eS/UMsUPDzvES3bzdJwYwZPtQO5PzvPGNFrt8nrsYt5mAr+2nFxcWH79u1cvHhRuO7J9HIABAYG5skzJJNlFtJ69OgR0dHRH2y/Zs0aZDIZI0eO/KaD5h8/fvzGFJsI/cp2BO15inm3+XQsF8jMWDvqWecmexTE3r/IWY9gVHZt6NbALFscFZo/WEtLiyFDhgjLQoHhw4fToEEDhg4dWtSmCAgIfCzpfnhmdGHayD7Y62hkiTA7Juxahd63+x70Xvr370///v0/3PAbIL95ks6fP090dDTTp0/PU0zSjh07kMlkLFy4sNBXM37xKF9y3UedJovLonqxD++S9ZmgBfH++9n1xIZWpbw5m+DI4A76XL8SQVXnGuzt9QvSIxvokqU3BZkvICAg8D5E5ejQ0xFNNHJ4qTQpVUpbuIEKCBRnZDe4EVaFulZiVIlJpIZdYLnbCR4oZNxZ8xdXdSsSttmNqxmmtBvRF5uXvkRV/p7aORNIF531AgICAl8AEiuafB2VdgUEvi20u7DJu0vmZwdX/vXO/KgMXsXqSr1xtlcwQ2qKqQSUocf5ba2SEQucKZ/j7Ud4ERIQEBAQEBD4Zkjx8ORe9F12Lz2J8eCh2KmFsO3H2VyO8mNFnx/YHvtfW8GTJCAgICAgIPCNkI6fZwZdpo2kj70OGllz6C57vXHJpXWx9CSNHj2aQ4cO5aNi/OfnxIkT7w1SL862CwgICAgIfJuIKNehJ46aZAuk91EsPUlPnz6la9euRW3Ge2nfvj3Lly/PdZ+aWm75fgUEBAQ+Hz4+PsJqp0+IkZERFhYWRW2GQL6RYNWkK3mNMiyWIulTkvL0KufuqmHf2hGrvKSUiPuX3ZfM6NnZ+oM10AQEBASKC61bty5qE75qRo4ciZubW1GbIfCJ+eZEkjT5HKsPVmV3nmpPKgk9sBNf88X0/tSGCQgICBQCtWrVIi4urqjN+Grx9PQsahMEPiPFWCSl4O82nVP2i5mku4nf7zsxucZZfj5gxYKZLSjx3mMVBF+8TOJ3zanyxjdM8n9EScdR6AKKoN0sOFaJWWNrv6Obp+zzMKLrgBKQ5MlmNx/UeEZ0XVcmNX1/4TwhJklAQKAoOH36dFGb8FWzfv16hg8fXtRmCHwmimXgdibaVCwVi+/DOO5cOsIpj2fIQkIoYVcNLSA59A4+d0JIVgJpsUTEpgGQFhNBbHwQ+/9YytkXSW/0mcZNnxQq1zVABIiNGtOrrTXI44iKjSfsvj+PotKyW8vv7OFuhZ7UlYIi9F9O+UmoN+RHeld7/zydEJMkICAgICDw5VOMRRJITEsjebyTiyVaUzXFiz2eFrh0MSTiwBh6Tt7N+X3T6DlyJ499lzB6kTfppOO1aASLLt7iYVgIvjeevt6hIhjvZ8bUsZOQfv8ylzw3MeMvDxSPVtG36QAW7N7PnG7OrAhQAGnc2Pecar2qIAHENj/w15AQZo7ZT7xesT5tAgICAgICAoVAMZ5uA7GpMTEeSTSc3pywnUsQ/bgXG3ECW/aE03nlakaWjsWoxyD+ibV/7TiRSUPqWFVCs1N1FPf+YuiOOmxe0ATZw0OcCkpCtXkJJ489odnsctnHSOsN5c857Xks6cqqhxlgep5D0Q35yVIMygg2T55NauNaVDBT5RrArVQqcXV1JSXl/+ydd1RURxfAf7sLC4IKooKIAqLYO2LvJbHGT6NijImx1xi7icbeIvaaGDVYErvG3iuiAlJEEQURBKSJ9Lqwu/P9gV2wYkB9v3M4h9133507s2/f3ndn5t40/vzzT7Zt28atW7eoUKECI0aM+MAjJSEhISEhIZHXFGwnyXYMu4/oYGgAtY7sQF9fB1Cgq6MhUaUFrQqVRgd9XQVCowEEqanpCCB7wkvLw9tJlG9WDQD9qhM5/SgtOcNA4zefHQDI0NPXR4YMHR05Aog7cpSsNnMwlwOY8cOyFaQmq9HrbpDjoMnlctLS0li6dCmQXfl52bJl/PPPPx9ugCQkJCQk/hMiIiK4ePEiO3fuBGDnzp20atWK6tWrU61atffWn56ejlarJTU1FYDU1FRSU1MpVKgQcvnLsxeXL18mNTWVhw8fAnDq1ClMTExo06aNtOQjDymQ80bly5fPTiYpU2JooAAU6Os/rjhXhI5D6nJh5ABGDh3JqSqD6Vq7LmZucxk2fATrvdXIKERps7usm3MMs+4zmda++FtakMat+Gr06vhsjhElhkWeOkhHjhyhbNmyz501ceJEChUq9OR1pUqVcHBweMu2JSQkJCQKGkZGRowaNYrTp08DkJCQwDfffJOrQ+Lo6IhMJuPUqVMAtGnTBhMTExITE3OUnzlzJoULFyYoKHuZSP369alVqxZarTZH+aNHj/LFF188ee3g4MCff/4pOUh5jXgFvXr1Eg4ODkKj0by8IG31AAAgAElEQVRKLH/QZIj0jGfsUieLh7GpQqNRi+x3NUKt/u/NGjdunAAEIP7555//3gCJAk+vXr2Ek5OTOHr0aJ7os7GxEYGBgXmiS0JCInccHR2f3N8B0bt371xlk5OTRYkSJZ6TnzVrVq7yDx48EAYGBs/JOzk55Sr/8OFDUaRIkefkb9y48V79k3iZAhlJeiPkeujrPWO+ojDFTQyQyxWPwmNyFPmQ/XHSpEkUKlRIiiJJSEhIfGKMGDGCEiVKANm7mKdNm5arbOHChZk4ceKT18bGxowePTpX+ZIlSzJq1Kgnr8uXL0/fvn1zlS9evPhz+nr27En16tVzlZd4N/J0TVJKSgrOzs7ExcWRlZWVl6o/Kpo3b46lpSVbtmzJb1PyDX19fUqXLk3Tpk1R5Ie3KiEhIZHHGBoaMmnSJCZNmoSDgwNVq1Z9pfyIESNYtGgRDx8+ZOzYsRgbG79SfsKECaxevZq0tDR+/fVXdHRe/RM9duxYVq5cSXJyMtOnT3/r/ki8njxxkgIDA1m9ejVbt26lVq1amJubo1QqX3/iJ4qJiQkqlQpnZ+f8NiXfSE9PJyAggLi4OEaMGMGgQYMwMTHJb7MkJCQk3osRI0awZMmSV0aRHvM4mrRgwYJXRpEe8ziatHfv3ldGkR7zOJoUEBAgRZE+EO/tJJ0/f56ePXsyaNAgvL29sbS0zAu7JD4Rrl69yooVK2jQoAHnzp2jTJkyrz9JQkLiPyM6Ohpvb2+Sk5OlSgFvyLhx4/D19cXX1/e1sqVKleLrr7/m5MmTb6Tb1taWLl26sG/fvjeSt7GxwcTEhF27dr2RfEFGqVRSokQJGjRogK6ubn6bA7ynk3T+/Hl69erF7t27admyZV7ZJPEJYW9vz99//83ixYtp1aqV5ChJSBQQrl27xqZNm7h06RL29vYYGRlJU+NvwfXr199YtlSpUm8lb2ho+FbyAHFxcW8lXxBRqVSEhoby888/4+DgwLfffpvvMxDv7CRlZGTg4ODAjh07JAdJ4rVMmDCB1NRUhg8fzqFDh/LbHAmJz5oDBw7g6OjI0KFDmTNnDkWKFMlvkyQknhAQEICTkxO9e/dm69atmJmZ5Zst77y7bdeuXdSpU4fWrVvnpT0SnzATJ07E1dX1SR4QCQmJ/56DBw+yaNEiNm/ezPfffy85SBIFjooVK7JgwQJ69OjBd999R3R0dL7Z8s5O0po1a57brvjOpEYRFPyANAAyiIsIIzQ0lLCwCB4kZ+ZykorEB1FERkYSFRXNg7hU1O9vCWkxYYTFpJJz6q5HLcdHEhaZQObTN4gMiyY5Lwx4vYGEhcWQqtWSHBFAQETyu+vSJhMREEBE8qt6m7cYGBjwww8/8Mcff/xnbUpISDwlJSWFOXPmsHHjRipUqJDf5khIvJIhQ4bQunVrVq1alW82vJOTJITg2rVrtG3b9v1a14SzZ3gjqrSdi7sKyDjJuDrWWFlZYWlpQSljE8p9OYPT0S/8kGec4KcaFpQuXRpz81KYlShK0bLtmOeScyZTAG28JxtH/8i6W7l5M8nsGmiL7YBd5O56qLj0a2PKWTVk4rlsKZXLVBradmZFgOYtO//2JO8aiK3tAHYlp7J3WE1qDt3zVuc/NwapexlWsyZD97yHo/UOtG3blmvXrr1WLjnyHg+yPWfSQq9y6oQrwakf2DgJiU+c/fv306hRIypVqpTfpkhIvBGDBg3i+PHjuWYq/9C8k5OkUqmA7Fw474rm3kF+ad+YftvDXojcKKg4fD+3A/1w2zMGa8/5/PDLYRJe0pAt5383EH/Pf+hjcIGFK46R/TuqIsrXmRPHz+AaGI8GFTFnVzNz3Tluh0SSqAFQExfgyvnz7txNeNbBEaRH+ODifJ1IVS7GZwWwbtwCXNNyOJYaiuf5M1z0jebx6dqEUG75RxAT5o2L9z0iQ24RGJVCYrA7F67cJVGrJSXEgwuXbhP3xJQX+/AsBrT5dR/7prVFmxjGLb+b3LyZ/efnH0EKgCoKX+cTHD/jSmC8Bl4cA702/LpvH9PaGj42nFDP85y56Eu06nm7k1PucfXCJXyjMnIZkDfHyMiIhISXP83nyeDUzAGsvpYJKQcZ1XYAaw65c0/1+EpREfBXH+r22ULMfxcI+/TRhOF+woNIbe4RXW2kJyfcwt4ycvtUX2hYGBHRCWQUkM/tcfQY8iA6+xhtKuE3ruDs4sHduNyi4fnDrl276NOnTz5aoCExPAB/f3/8AwIIDAojNt8vhgzio+J4/7vbM6SG4X3uGMedfYkuWJfAR0eJEiVo0aJF/q1lfVU67tzKkqSlpQl9ff33SvWdcWGu+HbU78LppxpCz+ZHcS5DCJF+QPQz1RVVJrkKlRBCiHRxbJCF0LUYLI5lPHPyIznLHkvEnn37xK6/pop25kaiyW/XRZZIF64z6wvjwhaiag1LUVRpIXpvPix+qaEUMplc6Bi2FiuCYsSpSfVEMX0TUcbCWOiZtRXLfB4Kpy56QmFRRdStVFGUKaojjJsvEteznrNanBlhLXRNywgLvcKi4dxrIun0cGGpV0/MuakWKe6Ool1ppShkYiqMlYVFlQE7RLBaiCSnLkLfsIwoW0JH6NYeL6a21RfGFWuJGrZWoriykKjYqo2oXd5KlNRTinKDDwmRUx/+iRBJTl2Enl5n8VdCknDqoif0Ov8lUv/5WhTV1RVKpa7QkcmEovRAcSTBVcysbywKW1QVNSyLCqVFb/HPvcvPj4HvRtFFT090/itBiBR34diutFAWMhGmxkpRuMoAsSNYnW13EVtRs1ZFUdGisFAUbSTmX3tuQN4ad3d3Ua9evddIaURCaKCITEkXkS5TReO6Y8ThOw/Es5dA0vY+ot3cm+Jp5ZlUERXkL27fvi387wSJ8HjV008tLkJEJz1boyZDxEU+ECn5VG2nYJYlUYu7a74UVl3Wi4jUA6KfqfxJqQOZ3FBYfzFdnIrSCE3kRvGVVVux0v8tav6kP68PZEK3uJ0YtS9c5G/Bo6ffo+f/fw/iz4lpjUsIHZmO0NGRCZmBrfjG6Y5QC42I89ggfhz1h/B7v6/Qe1GnTh2RmJiYfwZkXRVTalkIu07dRLdu3US3ru1EXdt6YtzxuHwzSRP5u+ho97NwU71e9vWoReiBcaJZ5Tqi88Afxci+zUVlu6Fi7/0PdaWrxe1Nw8XUf+PzRtvtTWL41H9F3mjLO9avXy8WLFiQL23nW1kSveZT+XvVIGoVeVUxPh1KmBaD+IfPRFgeoyHy2FyGDx3CsJ8WcTbNhuo2hoCaYs3G8PvhC5z6ew5dyzzgips+M5YPwkpZiymXTjCSrcxdFUD9Zd6EBF1kXf9ayGPjAZAX+4q1PrfwWtyWDLfTXIl/2SqFdX/mDrfEe/FY/rj7yDBtJNtmzOGS9XRcwyIJ3NebzK0TWXA6+/lEZMho/WcIkadGUkauJbXo12z382FZRwVBIZVY7HObzd+VJNzlYs59uOSX4wgZ9NlDYmYSHovbUkKnNF1mTaKtohjNxvzO4Qun+HtOV8o8uMKlwLrPjcFoy8cfvZbIbTOYc8ma6a5hRAbuo3fmViYuOE0GINKVfPm7D7euLqSVyovzV/6LbaYZHBrThXlXQnE75kr4A0/+/deLuCcPnJlc84jD1tSLucMHMHl3EBrVRaa3bE6P4T8yavhAujesRuNJx4nRZk+R2raZi8fjR0XVZaa3HMD2/IneFkxUl1m1whO7Xt0xl0NuEd2kUt3obe/DqtUX3/LJ+3Hk9y53fP5lqMV11i3cRPYstZbkYDfOnnEl8JmobmqYN+dPnODCtTCezLS+FCEFTXwItwKjSEkMxv3CFe4matGmhOBx4RK3H904UqLvERKThirWH+/roSS/dD95Gp1Fm0DoLX8iklO4d/UCl3yjnulrGhE+F3G+Fk7yg7vcuB3JswHl5CMrWXbVhqneCajivZlrH8ORFU5cTY7h7OqZrDt3m5DI7AtPmxyM29kzuAYmPIoUpxB9L4SYNBWx/t5cD00mryfxMzIy3msG4H3RxnrindKBOXv3sW/fPvbtP8npyeYc3Of+SCCewCtnOX8t/OmYa+K47XKK01cCiX8yIBpi/a9w9swlbsc+imtqEwn1DyMh8R4eLp6EpYEmMRj3C64EJQNoSQi5S1RCBNcvu3I3IfuGkuXlSUhlO6opc2n/OdR4z+vMiAM5H9UErGXQBD967L7MoQ0rWb31JJvbuTP/Dx8AMmNu4XL6HJ6hj69oLQmhd4lKSyPy+iVc7yY+mVnRxPpz+ex5rkU8bUsT68+Vs2e4dDsWNZAW6c62DZdI0EnOZfxerf9ZXaRF4r5tA5cSdEhOfZOx+O8oVKgQ6enp+dL2O6cAyK0ycd6SSVRUHJiUpPhL6TsUVBh5jGsLG6DUxnJwsB09hk+jXce1mHvuZv7KycyxroheKmg1z9uquR9KhNqEdhXNkCst6bdgMZDMpmUgt6pIJT05SqOiKHlIZpaWl2YlZYVo8OsS+h/+ivlz7iBEKVCHcjdEhVnrJlQykKPXvAlVFVsIDkqEQoCiMg0al6K4yUNAhm5ZG8rp6BFgqERhYYONvpyIooagUQMaHr6mD0/REr53JF9PvIzVz0fYNKgiysTLeO6ez8rJc7CuqEcqWjRaIMcUKGpC74agMmtNk0oGyPWa06Sqgi3BQSQ1ABRWVKqsj1yvOCZ68FCtJocReWPe6rpRWNFlUCdWXkhj/LgvH/14A5r7uHmG8KBiFeZNjuHrb7bi17Eh6NZl1M6jDC0JJP/L97XWcujnVlgjp3DEFn78rSPnZtqTfz8RBZdMt70cvl+D0a2elk1QFClFufJVUJb/lclHN/HVscO4qjrTslUtwh13c3lRS1rrvUUjOnoUKlQIspToykGhXwglam7/2ZMvxl5Ap0whYpOqMO3oQfqFjaVpvyMY1LAkzfc69N7JlcUlWNWqPYvDS1NOcR9/TUecrm6l7YEh1P3lHpXKChIiQnhQvAmNDUIJDL9PZOHv2eu5hIeDqzDa3xaLlAdEx8Zj2HYJR/b2e8a4NM7M7c4w8TsZ22SMqjua2xVtkGU8JOJuMjXmOOM82ZTDI9vy/eaHmJUtip5I4Y68L+euL6LxowIDepbWlOYI64cOJOl/7fly7V3iqpqgcZvCkG3hZKlX0+OHciSs0afnF2O5oFOGQrFJVJl2lMNDfRhcZTT+thakPIgmNt6QtkuOsHdUTd5mmF9HflaJz/L2JMDWjjqP8wSm3mL3EX+sWlRAG3eW6T2n4FuzA1WjpjHL1pFzM8uy5dte7LToTL2EWYxfPoRD27/i7rRuTL5ejbbVMrg0Zj699h5keKHN9G+zGcNW9lir3DjysBZtrfQppPLgkOIX7jpVYmXPjhws8wUtyj7k9PnSzD+/EiuPm5jWnYpe3Fl+fbH92U1e6IGWzOQY4nOcIszE468NxPbexIjqj+8yetSf48pluQ5Rh8fQfWYg9TrWIGbaFEynH2bZFwms/64TxywbU9XcgICDN/li/0n6hUyl1/QAqrc0w/fHOXTeeYKBMTPoNvk61dpWI+PSGOb32slCw6Xs8NfF7roP2sZ3Xh6/GaVy0H+aQZHTX9C1lwMtLrN0hz+6dtfxvqvHurHTXjMW/x35ec2+02+dvr4+Wq32ydqkvCYj6iZXXJw5sfVXFuyLoWyXHjTJ4S6henCbq66uXHG5gEdwMkJXD72sM6xZcAS9IcfwOTaFJkUfCcvlyIQKVZoKbVlbrJXReDj7kpJwnklNK9NpyY1sOZmMN/k4ZMZfMNuxD0Ujw0kUgI4NVSoaEOFyGNeYNIL3H8NbY02NmsUenaBET//pcD/3ocvkPHcNZOTShxxIvjKHXoO2kND4Jya1E9y4fIOg42tYcESPIcd8ODalCU9Of2YMMp88kelgU6UiBhEuHHaNIS14P8e8NVjXqImxHECOXA680ai8nqSkJIyMjN5PSYobV5PaMfJ7O4poNYhCBhjIAKEiMSqU0NBgfJ1dCStVk6qFAXSwHz+LijtH89vV/H4mKohoifO9yX1jGyqUyOmW8GxEV45JeRtMom9yPeptHpQ0BKzqgGXp0lhW7cleg29ZsmgQNmpnVs0/iuHwQ/jdcue3mm4sWn6EO87nuWdYhbbfjGfF3ztYPaQO+tocIqR+2TUitalF+Xq7Hz7LOqIICqHSYh9ub/6OkuEuXAzIBATp2iYsvxlB8NEhGJycx4pTr7gWRDrKL3/H59ZVFrZS4XX+CjH3/mGJ030aL72K/62LTK6exYv5qZXNZrN7wyjqqS+xYdoAOtQsT8NJp0i2n8HyQVYoa03h0omhOK+az1HD4Rzyu4X7bzVxW7Sc42nZ7WqbLOdmRDBHhxhwct4KXmXmx4WGIM/rPLyxhq8a1KFyOUsqNhvNpbpL2TCyLG4LJ3Ch9Qb2LJvB/I0/U/7EDlCH43s7E4t6/2PM75tx7GSOWp2EotY0tu90ZNKYsXS3vMfNUDUZnp7crfANjk5/8FufmmiozaQNf7CwXz30MzMh2Ytr8Y2Z8PufLFmxmQkVznHI/SFePmnUsCvB1Zzaf8b2exv6UKd6Xfo6+XJ8kj3Va7ZmhsuzC44S8bmRQt3GVZ+PPugoUWZeYP60W3yz8wArZy9g05w6nN18ivR0T7zumtL+lz9Z/dsUvrJKJSnFg+XT3Gm/aQ+rFq5l0+JulMpSkaSoxbTtO3GcNIax3S25dzOCytUrUtRuAEt+/iLn8ctJf4Y6B12h6FSuTsWidgxYMo6S/0x+xVh8XrxTJEkmk1G1alVcXFxo06ZNHpukIXjLQFptVaBb1II6XZeyfX47CucgF7TpB5pukiFT6FDEvC4/LJ5C+yIKYtuWYu/CtlTYaoGZQp/EiDB0rKtRyWAtji1aoO9xitnTDtBjRn2M58rQs/6aVZ2rwIW3sVNOya7zWdD9JN/vB+Ql6T13ESd6jqet+XKEvCRNJm5lciMl3AF4M+cLAN06fJFDH15Gzc1/d+CeoEZ9bjZfn5sNus1wPN2ftqX2srBtBbZamKHQTyQiLAmd1s+MwYXhT/vRey6LTvRkfFtzlgs5JZtMZOvkRiiPvs14vBkXL16kSpUq76VD5e1KcI2WNNTXEn7yMjrtZmOlCAf1dTYM+oYDCg1pD8KRt1xIMQFpgNykK45zT9Hyx9/odKJF3nTmk0GQmpoOhQwfOZsvHn8+oiszMECfDNIz3qaEhYJKPx3lYDcffuo9jRvGdWlWswhkhXE/Ws39bf2pfVgOWGBmmkaDkQv4yW86m8b2ZEmWAeW6LubQ+po5R0gBmW5ZbMrpoBdgiFJhgY2NPvKIohiiQf1oNkZRsR4NjOUY2dtRUb6e8IhXbZdUYFWpMvpyPYpnh1BRRUYTI4ypYWuGXC6jUkUz5M/NgmsIu/gvLqIrf7gvwSTGmx0THRi0ZiUHfm6O9RO5LMLuR6O+v43+tQ8jByzMTElMzA73VqzXAGO5EfZ2FZGvDyciFfIy/CnyrfRICl7ekXRYepPtvQyIOTOOr2YYMXxcJ6xlt9l2MZqkIlPo7pYtLa/eAZQNmLLxR2bP7EO9RTWZ5LSBNqF7+O2PNdzZbEEZcy2+V0szqKoM/99vUenruVTUUXPD+zblu8ymnEJDkM9titceg8pnLUF1u9LJTA5aFekqfQrr3cAjsDz21e5z7pcc2n+CAutB2/AelInbz81YXucc2x0Mnu+eVotGI0f+bMReG4PzlgNkWYTiWrITE8tlHxSqTLJkcjJ9PfGv1gvHyjqQ6cW18Mp8qX+J3xWtWGGb/fNs1WEUVupA/h7xB2vubMaijDla36uUHlSZOK8VaGt2obgIwimH8ctJf8dyYThPfFFXVbRxh7murUkX43uczemzyEfy75p9j+m24cOHs2bNmvd0knSoM8eHjDmPX3/FpmgNm153mv6r5frt8qfjvShEKUtK6GSSoZaj0FdyKLgT9xIMKGtdDL2aJwgafI+7MQpKly+LkQ5wMIMfHivptZPEXi9q1qP1mmCezoyWps+uCJ7uFRnENt8+LAsOI83YmnLFH4W/fjhIxhPFpgw7kcGwR6+6/R37ZL7Xeok/qiW598FAf8BTPU9sHUyWYw6D4N+Ze1GCUpYl0MnMQC3XR6Ec9swYFEcvY+AjYSMGbfOlz7JgwtKMsS5XPDu8/5zdvdj58oC8FZmZmaxfv54zZ868hxYNYR53SHyoi9Mab7xutcNxYXV0CAdde8YffjTdpg1jXefGTNndmpEAyDDr5sjcgy350dGYmu/Vk08NBcXNSqCblEDCM8Gh7Iiuiozg/dkR3W+yI7ra+HiSdM0obfp2JSzkesZYt5jAxmVe1P/uFwYtbIrzz7bYWulwvbEjzr9X5/TsZQTUrcsN560k1Z+L57ba3F38DR0W7+PMAWcWHNFjrNcVxkaPpa7zgafKn4sAy5DnEJ5X+5ziaGhf2ng546stRYfyLz96PWdvdgj1iV4dm6pUKhTJ2e3/4mEk468jd19aM5R2ZR3jp8fhpdjMz/XVZKiykOkVwkABcrkMoVKRpgJbWyt0rjfG0fl3qp+ezbKAujQyTWYvanxOHSW0bxu8nH3RlurAa8x8K/T19VGpVPlTgDzzOh63rKlrVwiQU7LVFEavb8S0Df04PsoAQ8NSdJi9h98aQMCmCazVbc3FOb051XgTSw99x4/L29PhT2dqGC8nsPMezoyzJv3iOBpeVWJvkoCHj5baU8yQk4inTya1J5gjJw0PrziqDbQkwvMa9zPrkQWo/JzYHt6CX818mFe4DsOMDIh7qf32OXRCjqF5RayNc7j25cWxr1eE1f+eJ67Nl5jItTw4OZMxqwuzbG0h5FkqMgWgjWDvZleqdZpFmpcTcrsemMtBfccTv1J1mKKjQiNXoJCBNno3P464Q5/xqSwP7MyeM+OwTr/IuIZXUdob4f1XMLYtq6GUxecwfl8Q6zXqJf2/+Dvh8JKuEmR5exFs25Jqega4vdFY/HdkZGRkT9XnA+/sJPXt25epU6fi5eVF3bp189KmPECfktbWT/43eNRLHeOyVHi65AKliTVV8rosjNwAs/J5kYMk5z68+ekleXq6wZMP+sUxeIocA7PyfMjsKevWraNq1apUrVr1NZIG9N3rR3YN7LGcufjsMQU2Yw7iN05DerqM4YaPbvYvzvyqU0nNkKGj88z0kdyMbo5zOdhsKHseNqD+e/bnU6KIfX2qph3EL0RDVyvIPaKr4bbvbVKqdKH+K6aBc0dO6V5LWbTfhe8Xj2Z5t1OMcxzBpcG9KVNEi55lZ+b1KEfZWAU3Jn1LzQ0l0UtKpNKAv/hfiyQultr3UoT0TZHpBbK0vhE/PNSlQt8/GN9Sj4sr3sJys2+Yu/AUfaYOot2FpnxpbYo8XPbMmgUFlYYvZb7rd8z8oQEbNaBrXB2HJTPpZqRDZLVKGKx1pEULfTIOOjLi0mB6lymCVs+SzvN6UEbhA8jQC1xKfaMfeKhbgb5/jKdlHi5IMjc3JyAgADs7u7xT+oZoo7y4rq5BlzKPHAy5KV0HtmPaZCeujZhF7zEt6TXiC7qUzOKhQRccN5SnqpsVUyd3o39DK+J94OtZ9amc3oik6T8y5KYxqSnBpBl9RXGtD96hlWlZXQkqH66FVKJRDSVkeuAVWAH7ulo8Nj2kpu5hvum0ndQ4fTqt3Ii97488qNaX8rplKP5S+1Y59EKH6j9tZUGOPdSh7rhlOPQeTMNGFahiHEtYZj3GbZ1Fi4rhDLf4hu8c7lBF5UuQ5Qyc+pbg+ogIqn6ZPT0Xf80XVY0fKWNrQX+rbxn5/T1MQgMwGbmFhpVv0ihpOj8OuYlxagrBaUZ8VVxLWloSXuuXcbrDrzmMX2mu/fKy/rIVknPQpSArPI0kr/UsO/slQ99oLP47AgIC3nsG4p151da33FIAPGbPnj3CzMxMeHt7f4CNdxKfEps3bxalS5cWfn5+H6aBjONiSNmiwsymgrC1tRUVytmK+v02CN+MDHFmREXRbVPyI0GNiNrznbA06CDW59Ou4wKZAkDtLxY3Ly7arQl99bZ8zX2xpp2JaLLwlnjzJAAaoc7KElnPnqBRi6ysLKF+3Jg6RcQ+SBDP7cLOeCiC/G6KO5EpT21KfyCCg6NFukYjstJTRbrqTbZWP93eH5/6QIRHp75b6gHVZbFqxCgx7W8fka55KHb3KSX0Gi8UgTkMhCr+vggKjhAJz233zxLxoXdE8MPHySzUIiX2gUh43Okkp0dpOeJF6oNwEZ2a99vGN27cKMaPH5/net8IdbpITlG9MPYqkZKQIp4MkypBxCY9vxdfk/5ABPnfFdGpT9/LeHhPBEWlCo1Qi4z0rNd/nipXMdmuk/gjSiMyEhJEem4n5ND+25Ml4kP9xZ3wJJH14vsh/iLoQfob6MgQsffuiohnU5dkPBT3gqJEqkYIdUa6yNIIIbISRGRE/NPvzZvan5MukSUSIiPEk+wpeTIW709cXJyws7MTsbGx+dL+ezlJQmQ7SqampmL58uUiISEhzw2U+LgJDg4W48eP/7AO0kdGgXSShBDxRwaLyo3nCd9XeD9qvwWiScX+4kD+3K/ekWSxqauBMPjKSbzXHUoTKQ6MqiOMdXWFQWF9oVu0qhiw/d5bOIuvM3OT6GpgIL5y+nD30YSEBGFnZyfu3bv3wdooiGii1otuTaeLq/mYo0ri3Vi+fLmYOHFivrUvEyL3FVEODg7IZDK2bdv2ZH4+J9zc3Fi2bBknTpygffv2mJubo6eXl5tWPy60Wi0ymSxfty3mJ0II0tPTuXPnDm5ubvTr148xY8ZgaWmZ36YVCBwcHOjQoQNmZmZ06PD+CyLLly/PyZMnKV++/Htq0pCZoUahr5dztggATSYZGgX6yrdbj/QpkZkQQXicBiOLsph8hLe57du38+eff7JlyxbKloF4IowAACAASURBVC2b3+ZISOTK9u3bWbduHX///TdlypTJFxveeU3SszRo0IAdO3YQERHB8ePHiYuLIysrKy9Uf5Ts27ePsmXLYm9vn9+m5BtmZmY0btyY3bt3Y2ho+PoTJAoACpT6r3F+FEpeJ/KpozQuTbkc1/V9HHzzzTdotVr69u3LmDFj6Nix42f9UCtR8Lh//z5bt27lxIkTbNmyJd8cJMgjJ+kxpUuXZsCAAXmp8qMkKCiIhg0bMnDgwNcLvyUqlQofHx/c3d2JjIxEo9FgZGRE7dq1qV+/PsWLF8/zNiUkJD4tvv32W6ysrHBycmLhwoU0b96cYsWKvXLGQCJ3UlNTOXfuHMbGxjRt2jS/zfloUalUhIaGcv36dbp168aOHTsoVapUvtqUp06SxIfDz8+PFStWsH37dmxsbLC3t8fKygqFQkFcXByLFy/Gw8OD2rVrM3bsWLp06YJC8Zk/8ktISORK06ZNadq0KcHBwXh4eJCcnPwfVVL49MjIyMDFxQULCwu6du2a3+Z8tCiVSuzt7Vm5ciUGBgavP+E/QHKSCjhpaWlMmDCBvXv3MmLECAIDAzE1Nc1RNisri3379jF//nxmzZrFP//88wbb7SUkJD5nypUrR7ly5fLbjI+aq1evMnPmTMqWLcugQYPy2xyJPKRAxlZ/+eUX/v3333zNsvk6jhw5wvjx4z9oG3fu3KFu3bokJSXh7+/PjBkzcnWQAHR1dXFwcMDNzY0RI0bQvHlzNm16bWpOCQkJCYn3IC0tu8xxQYl+SOQdBdJJ8vLyolu3bh9kd1ha0CUOHbpM8JvWQ0pwYceBlzPrdurUCV9f37w27wnBwcG0adOGMWPG8Pfff2Ns/OYrRWUyGYMHD8bFxYVp06axZcuWD2anhISExOfOYycpv7JCS3w4CqST9CFRpp5h7b5ITN6oFpKWiL3b8DIolfuW6A9ASkoKX3zxBZMmTWLYsGGvPyEXKleuzMmTJ5k8eTKnT5/OQwslJCQkJB6Tnp5drEqKJH16FGAnKY3rf/yEo0smap91zNsRQsbtjYyZd5ZXlaV8HSnXAyjauBlF3kRYE8RutxJ0a2EIKe5sWvw7mxdPZumFtPew4PVMnTqVxo0bM2rUqPfWVaVKFf766y8GDx5MSkpKHlgnISEhIfEs0nTbp0sBdpIMqFAsHi//BHwvHOC42z0ywsMxrFyD1wc0NYSdP4ef+sX3VVzzTKOKvQlyQBO6g9lrPHPVovbdyU2bntgrQRPhwnFvHeoPGEPvGnlYkvsFrl27xq5du1i6dGme6ezQoQPNmjVj3rx5eaZTQkJCQiIbabrt06UAO0mgU8oUnTvbOG/YjmppV9npbkW//5VEDqRG+OLpG06qFlDFEx2fXeFUFRdNfGIoexyXcfr+C5ETTRge90pSr7IOmbecuZLVhF4dyoM6gZj4RCJvXScg5nGlVBWuu0Oo0Su7OKCi4miWDghn6og9JBp9uGFbvXo1P/30U57nO5o5cyYbNmx4EhaWkJCQkHg/YmNjuXHjBjdu3ACy8/yoVC9W25b4mCnQTpKiVEni3FJo1KM+RoGXkf/vWyoqtETuHUHP8Ts4u3syPYdu447XEob/5kEmmVz9bQi/nffBPzIcL9cgNH5L+WGqMwAZ/v9yPDSFW5uWMGbCDiLitjJlqRuagDX0afEd83bsYVZ3B1YFaiDlDP/GNqKHtQK00Wwa+yMHEi2wMX9FyYb3JDExkb17936QhJw2NjbUq1ePPXv25LluCQkJic+RlJQU6tSpw+rVqwFwcnJCX1+fCxcu5LNlEnlFgc6TpLAdw+4jOhgaQK0jO9DX1wESOLkziq6r1zLUNJ4SPX7gWHz1586TmzWiXjlb9L+qzsPj+yjfrBoA+lUnctrjkdAw0PjNZ8ejl8r6A1k8qxN3dLqxxj+LOM+jZLWZg7kcwIwflq0gNVmNXneDDzZoLi4u1KtXD1OjcNb/OBHnJCMaj5xF+7AlzDmUQoORcxhq/0arqXKkZ8+enDhxgu+++y4PrZaQkJD4PLGysqJnz57s2LHjyXudOnWiRYsW+WiVRF5SoCNJyPUwNFAACvT1lY/eVKCro0Gl0oJWhUqjg76uAqHRAILU1HQEkJ08QI5Z95lMa/+6qSsZevr6yJChoyNHkMat+Gr06ljsGRklhkXyxkGaOHHik/Dss3h5eWFnZwd6NWli7s+eHV6kmMiJ9LhESIXe9HkPBwnAzs4OT8/c12BJSEhISLwdEyZMePK/QqFg4cKF+WiNRF5TIJ0kExOTVySTLELHIXW5MHIAI4eO5FSVwXStXRczt7kMGz6C9d5qZBSitNld1s058o4WGNBk2HCavmKjwpEjRyhcuPA7ad+/fz+1atWiR48ezzlLvr6+1KxZE1BQ+bt+NNfzYOuY/sxNGsPfUxq+2Y68V1C1alWCgoLIzMx8T00SEhISEpD98NmqVSsA+vfvT7Vq1fLZIom8RCZekdbawcEBmUzGtm3bCl7hQ62KjCxd9PUe2aVJITZRTjFjPZArkKNFo5GTH+XLBg0aRFhYGH369Mnx+A8//PDkf5lMRvfu3ZkxYwaTJk1i1KhRdOrUCbTRbOhSgWE+Dhzw30Anw7c0QhPMtp82UXTxLDo/sxnPyMiIkJCQt0pOKZG3ODg40KFDB8zMzOjQocN76ytfvjwnT56kfPnyeWCdhITE23L06FF69uxJYGAg5ubm+W2ORB5SoNckvRK5Hvp6z7xWFKa4yXMC+eIgAXTs2JEDBw5w/vz518oKIXB1deXixYvI5fJH0TMtsWeXsv+hJcYPXDh3TUWnJo87m0rYNU/upJlRu0ElTBQaEsPDUZewpLgeaBLCCdeUwDTDg5On3KkRGEXr6qV4HBTTarVS4VsJic+MSZMmkZWVld9mfLIIIWjevDmOjo75bconS4MGDejdu/d/3u7H6yQVYLp370737t1zPe7i4kJgYCAWFhb8/PPPDB48GD09Pa5evUpERASp3ssYuFSPnw8u5e/6Xdi1+SyzmnTAUH2b9d98w1/yFjQxvMaYae34c/9grg35mvC5V5hTR07U1v70iFnA7roX8YsPIm7vFbpV6YaNApKSktBqtVLCMwmJz4yMjAz69+9PREREfpvyydKuXbv8NuGTpXLlymzcuFFykj4XypQpw08//fTEOXpMnTp1cD+5hYNzrxE3woOGeg/YVwTCd81j1Yg2jEv6g5XJgzh6dCRl5Yns/6EBS/b3pM1LLSiw6tiDxmaR2E/MdpAgO1FljRo1pEiShMRnxuNlE82bN0dHR7rtS3w8uLu7f5A6rm+K9G3JB86cOZPjGi87OzucnJzwDn2cBLMyS30zeZx7O21nFJllvsBUDmBI+XLFiI1KfE6HNvclZri6umbvnpOQyBENYe6niS7bjup6UcSkqHn2apIVMkEn9BKhpm1oUPZtbh0qEh/Ek6YRyGRy5MrCmJgYvsfNJ42YsFgwsaCk4YdbK5kWE0YsJliUNMxxh4smI5kUtS6FC+v/p7Ud34cvv/wSpVL5ekEJiQJCRsabVqP/MBSw1difB7ktgm/YsCGxsbH4+PjkeFxZwZYifh4EZALaGDyuxWNpa46uIoX4BA2QSdDd+2gBkCGTaXnsMwkh2LRpEw4ODh+gRxKfApqgdQzutYZrZHByXB2srayweubPdvBeZD5rcRjwOwGat1CccYKfalhQunRpzM1LYVaiKEXLtmOeS+IrTtIS77mR0T+u45YatPGebBz9I+tuqSF5FwNtbRmwK/l9u/wKktk10BbbAbvIuRUVp3+qSkm7qbhKm0UlJD5ZJCepAKFQKBg8eDBr167N8bhOnVHMaXaOvs3a07l1FzaYTGVKZ3NadavB2ZHt6NyxM8tu6KMrAxSWVLPxZOqXM7iYyZMMsM2aNfsPeyTx8aDi8qoVeNr1ont2BlUUFYez3/8ud+9m/91a9zWluvXG3mcVqy++7dOdgorD9+N/NxB/z3/oY3CBhSuOPS1WnRqK5/kzXPSNRgWgiuHs6pmsO3ebkMgHxJxdzcx157gdEkniCw6aNjkYt7NncA1MIPuQloTQW/hHJJNy7yoXLvkS9cRcLcnBbpw940pgwjOK1HEEuJ7nvPtdEl5yADOJDfbDLzDqDYtrq4jydebE8TO4BsajQUt8gCvOroEkaLOPh11z5uLNB2hzsEebEMot/whiwrxx8Q4jf5+jJSQ+b6TptgLG0KFDqVGjBsOGDaNOnTrPH5Sb0WHhOdqlxJIojChe5NHH138Xvr0eEJVRlNLFdNGgADkM2h9Efw1otVnUHzuWX375JV/ndiUKMJlu7D18nxqjW/EkOUTqfbwvXiBODqDAvGEPylVqSata4Tjuvsyilq3Ry13jS2TE3OWGjxZtgi+hyQbUrFcNPSD16iK6/e9XXDKM0UtJw7zvBvYP9GHWtnCy1Kvp8b2M0bHbCM9Ss7rHD5Rz+/aJTvXtP+n5xVgu6JShUGwSVaYd5fCECuwfVZfRtytiI8vgYcRdkmvMwdl5Inp/9eSLsRfQKVOI2KQqTDt6mHFWLvzyZS9WBhlSXBuDut5vnDwy8FELGu7vHETX/ueosfwkOyqUel0vcZvVgvaLwyldTsF9fw0dndxYoXbkf0Mf8MuNC0w0O8IvX/YlePxV/rw0lA4v2DPUZxR1R3lTslAUUWWncNV9FrWkO7WERL4gRZIKGKampjg6OjJgwIBcCyXqFC7+1EF6hNzQlNLF9UGuQPHkU5WjUMhZuHAhZmZm9O3b98MaL/HRoo3z5eZ9Y2wqlHhyU9DGXGTD3NnMnj2b2bPnsdktEeQmlLcxIfrmdaK0b9OChshjcxk+dAjDflrE2TQbqtsYgjaSbTPmcMl6Oq5hkQTu603m1oksSZ7M8kFWKGtN4dKppcxYPggrZS2mXDrBaMvHFqpwXjWfo4bDOeR3C/ffauK2aDnH0wAE6cov+d3nFlcXtkLldZ4rkc6smn8Uw+GH8Lvlzm813Vi0/Aghm+eyKqA+y7xDCLq4jv615MTGZXdOc2sVDoP3UXzSPv4eUuUNnEI1xZqN4ffDFzj19xy6lnnAlUu3Me3al07GXuzfd5eE0/9yOrUhDl0jWfuSPccBEBkyWv8ZQuTpiZKDJCGRj0hfvwLI999/z9GjR3FwcGD37t3o6uq+s64tW7bwxx9/cPnyZSmKJJErIjWVdAphaCCDR8u15eWHsPfaQho8t843EwMDfchIJyP3PQI5oKDCyGNcW9gApTaWg4Pt6DF8Gu3ajuZuiAqz1k2oZCBHr3kTqiq2EBz0qvVKj8ki7H406vvb6F/7MHLAwsyUxOw5LRRWlaisL0evuAl6PESdEcL9aDX3t/Wn9mE5YIGZaQL3QyNQm7SjopkcpWU/FiwGSGYToA6+S0RRDYVu3yVZa8/r14lreOi5m/krJzPHuiJ6qaDVaMGoPX27mtJt/z9ssjlNRvNZ9DAPZ/hL9jzqt6IyDRqXoriJ9Bz7MaHVagte4uVHFGTbCjLSiBVAZDIZW7duRa1W07NnTxISEt5ahxCC33//ncmTJ3Pq1CksLS0/gKUSnwqK4maU0E0iIeGZ8FBGFDevuODikv136VoooCU+Pglds9KYvuWWLtWD21x1deWKywU8gpMRunro6dlQpaIBES6HcY1JI3j/Mbw11tSoWQy5XIZQqUhTZYJcjkyoUKWpyHyyZkiJra0VOiXa4+jsw+HpX9Om+/9oZPbotiaXZ9/gHj8b6Npia6VDifaOOPscZvrXbej+vyZY21qjjPbA2TeFhPOTaFq5E0t81NktNJ3Nxe39KbR/BvPP57CEWxWN35Mxusz1u8dYs+AIekOO4XNsCk2KPhY0oNX3PbD0XM7MQypa9/4fpfRzsqdRtrhMiZ7+5317TnZdw6h+3/PDoKEMGzKQfv1ncCBCC8murBnVj+/7T2Cr39NV84lXVjOq33f0G7sZPzVAKq6rh/P99/34aZ0HKbm2lDecP3+eGTNmvEfZpwz8N/ahbp+txOaFQYk32D5jCccf+d1+fn78/PPPxMTE5IX2z4bP+1tYgFEqlezZswcLCwtq1arFyZMnc6ll9zL379+na9eu/Pnnn5w7d44qVap8YGslPnqK2FO/ahr+fiE89kE0wVsY2LIZzZpl/7UasBE09/C9nUKV+vUp+kqFL6IhaNMPNG3UmCate7Piji0/LJ5Ce4OS9J67iO6q9bQ1L4rtQGdsJq5hciMDrKtVwuC2Iy1azEfHuhqVDG7j2KIF8289tlBJw3GOjCh1hN5lilB16E4iTWwok5vzpmzIOMcRlDrSmzJFqjJ0ZyQmNpaU/W4205qF81t9Y0q0XUNUra/pXC07yC4rWowy7aYytX0cf/2yAp8Xfv+0If8w5MkYtWTYLjO+aFuKGwvbUqHuJNwV+iRGhGU3X/87HKqlk6z8kt5dTJHnaE+ZR5plfO5xXwPjJDy27yG6/hzWrhqAeUgkMmM5GBiT5LGd3WHWtKz8NMxZuFgKNw/tZ/um4/hlgvrmGmbM/4d/dvhRomkd3q3S5pvh5+fHjh07mD179nukWNCnlL6WIrY1MHpPe1KvbmbmomWsWn+W+4++LtWrV2fSpEkMHToUrfat5so/b8Qr6NWrl3BwcBAajeZVYhIfmCNHjghbW1thZ2cnnJycREhIiNBqtc/JJCUlidOnT4s+ffqIYsWKialTpwqVSpVPFku8il69egknJydx9OjRPNFnY2MjAgMD31OLWvgvbi6Kt1sjQl/xddfcXyPamTQRC2+p37O9FxWniqjA2yLoYcYzb2aJ+NA7IvjRe1nxoeJO8EOR8dLJapES+0AkvOnlrk4RsQ8SxPPiKhEb7CduhyaIrHfswlPSxYPgYBGdrhGarHSRmq4SIjVW3Pf9R3xvoyssBh4Sya+1J28ZPXq0mDBhwkd1T0je6SCK6dYW072ThO/pC+Ke+skB4VBMV9T81eu5zyppx7ei1RethJl+G7E6PFRs6vOF6N65vFBajxRnXr5o8pSBAweKW7duCSFU4s72n8Ww1e5CJdQieOd0sfKySmgenBeLR0wX/4a/6rc0Q5z7qZEYeChVxDsvFUMGLhCnHzwrrxEJIb7C59o1ce25v+siKC4HvapLYkLNLmJ93PNv//rrr+LMmTN50Ov/hn379okjR46IX375JV/al9YkfQR07NiR9u3bc/ToUTZs2MDkyZORy+VYWlqiUCiIj48nNDSU2rVr061bN9asWSMVsJV4SxRUHDiZ7hvmsfXWUKZUyykco8F/6zZCuk5iUOU8Tp8oN8CsfKUX3tTBuGyFJ7vtdIzLUiHHy1qBoUnJN29LYcjL4kpMrKtgkpP8W6NPSWvrJ/8b6ECmy3zatFlJtE0Plk7q8HxUI0d7Pncyue7uTbIcXB27sjP+f5xs1Tz7yHV3vFOK06R+lWcW1WbiczUU61YNeOh8iqCdC7hVohf2QSM4XacBdV9YcZ8a5o27bzSKsnVoWN2M7NiPltSYaDKLmlPssbwmmQcPtRibFSEr12NGeHl5UblyZQAsLdVc2XmT9OEWnNy8lkMdhzGyTgb37stpY/qKyRtNGO43SmA/PpILG0Mp32cyrUo+K68l3G0/u33SeX5OQUGlbhUpZ/dme03r1avHhQsXaN269RvJf+5ITtJHglwup3PnznTu3BkhBKGhoURGRqLRaChatCiVK1d+rwXeEhIYd+R3n9aocy1bo6DiuNPcUOgj5Wx+O5RNF3NbtTi/zfh40Ebi7hGCbssVbFpQhA1Hq1NaDqAl0t2DEJ2ajLV/5irUhON+oxj1l9pwf+Yd/lpXm3UHdNjuBFW7NqDIE7kwDk76lsnnjGjS1AqVz1jGVZnHgbXdsdB6saBNcw52uYDHPHuUgMp5Ci0cq3LugD2rczt2ZDhpaWlPTNEpXZoSSbFEe+3innkLFA8iiDpwAr1+v1Az1ZsNjpeoNn0UjV70aZLd8HiowGh4L7SDndnY2vAFATkWDf5Hz4rqF5wkOUUt3/zeX7hwYWld0lsgrUn6CJHJZFhZWdGwYUOaNGlCjRo1JAdJIk9QKPXRe0WQSKHUR/mx1OCQ+HhJc8fthpaK9etT0qoP04bXfPREn4a72w20tvVpUAKitg3gu99DIM0Nz8zaNLSywMJYD/sfp9E5zA2vjNLYNbB6VDZGQ8DaQUzw68Huy4fYsHI1W09upp37fP7wUaON8+Kaui5Gp9ZyJD5bPtzzBvq16mKckPsxEznPrUOSm5pjmubLn7u1dBlQBRG8n9896zCqa0nkhVQEhyZSRAFok4kMieFxohfVNXfC7Psz26E8/ldvoX5pULTcv7KX7du3v/C3kwuBOS8WF0LwgkdFamoqxYoVe6+P53NCiiRJSEgUaJIj75FuZI2pwTNvapKJilZjUrrY06hWWixRmYUpZfw2KS4lCh6Z+P61njMJghLxcSRp4XEmhEzfv1h/JgGKXGHlkB74HA3ky4MluL5hA2cf1mK4aMrEXScoUh/2fneEcK2ctKQktJREnunBXxti6b1pBNX1HzWlV585rpeRK3XIOu7BnYr9WFNlI4u2BvHVaFM8PMOo0r0aMq+NuR5TArVq1SIwMJAKFSqAjjklk70R7VdS33oz4vJFKp2ZibUCtOFXCTW2w1YHSDvIj52vMtpzOc2VGkLcrlOszmRM2yZgvnImE4yHMXZsZ6yePJToUM1hOgveoLKUNvoK2zev48w9b3zmr8RswCC6VM3+Anl7e0uVF94CyUmSkJAowGRwauYArvU7zuzGT5/WtVGb+L5bDI4X++M3fhNFF8+i9b4htHYbhfeqVm+VCVyioKGk+uiTPBidw5HqozmZ04H6p4kc++j/5tkry77bHcJ3z8ok+nAjpS4/Vn3+Z09HqQQ03PW8iVGtkTT7LpkFfTbgOfArPPzKUneekrvbcztWCIDRo0ezevVqli9fDsomzHNxQd+4MAoGsctzMEZG2Z5OmrsPsjq90QPSrvsj2rSnphJAQcXJ5zgKwPdsv9gL9PXf+QdabtaIbyc14ttJz7+fkpKCu7s7U6dOfUfNnx/SdJuEhET+khxBYPijHERpUdwNic9OQ5AZy72QFNr8up4RtR4trU0I5Mq5y9xJyt7CrIry4OQpd/wDo8heFaImzv8K5y8HEP82RXglPnm0Wg0auZxnZ4u1Mc5s2nCBSG0yXl4PqF6vEno2felvfQKng274ZtbAziLtFceytdnb29OiRQvmzJlDVpagsHHhRw6OPkZGj112LYkxycRcP4lbnBaN6fesXtyenPYi6LyHg5Qbd+7cYdq0aaxduxYdHSk+8qa8dqRMTU3p3bu3lK1ZQiKPuH//fn6bUKDQRGxhwCAtf57/GeOt/am7pDKHfZdR+9BYepztxZioSbiN8maZuRM9ev9N0TZ1yfA8ynV1b2I9LuIXH0Tc3iu0Lyd4eHQ6QxPrYh5zlqGl5uGysTuPNwhlBu5gxpJYuq8YSd2IXcw+VJapIytwZdlMzpafyqz/lc7fgZD4oMiL21OvyGr+PR9Hmy9NkGsfcHLmGFYXXkUv9XU8bttgZ6cEuSndhjRi9SQn4iqPoZbsOgdyO/ZMyLJbt2506NDhFQ6IHIuhW9mv1kFPKQeTCk8XlP8HlC1blqVLl0q/5W/JK52kxMREatasSalSryvqKCEh8abUqlWLAwcOMGPGjPw2pUCgKN+JNmISZyKGU/xCMrZFPTkfkEzyCX/s/tcQNgJk4rr+D7KGHWPTMHO0101oPEBL6Q49aGwWif3EbpTbtxVdu9Fs2fINxgmb6Nb4LDeyutP60Q+Z0tIS9ZWd3EwfjsXJzaw91JFhI+uQce8+8jam+TkEEv8FOnUZt8yB3oMb0qhCFYxjw8isN46ts5qgH7mM67q16F0i26M2bDmY9jInDtWyo3Dk6VyPvZhQVV9fn1ciV6KXT1tDX2ubRI680knatm0boaGh/5UtEhKfDd988w3lypXLbzMKBjpV6NQyGceTJzCO/j979x0VxdUGcPi3haULgg0bKPauKFhi188SjQaNGrvGHnuLibHGFnsvsZfYu7Fhw44CIqIoiqAgXXpbYHfn+wNU7EZFwNznnD0Hpt13Z2dn3r135t66jO3gzpZTR4m4U5lvFxkRtx5AQ0hIClZNLZADcltbiisfvLIhGWYFC2EIoDLCgDReeuZHWZjC+eKIDLvB7kdWNFSEExx6iJP6vfi1SiIe6+ZyueJkhr72bLbwdZBj0XAyTt7DCfR7isLKhsLPBgq3HsXZm5kWVVZlqnsiUwGo/I55wtfunUmShYUFFhafp3s1QRCEN1NSpXVdgkctJrT6FBY0V7G07yyuVp/CTGM4BICKsmXMWOz+gNTWldDd8uKBxgCQIZPp+KARe+QFsCqQxMm/9lC5Q1/K31jNwVXJVB86m/xyLQ/9A4itLfo3+OopzSlWRnS2K3wYceO2IAjZTmXXmhqRvhRsWAfTyo2pHh9KyVZNM92zoaTSz3/Q8Gw3GrRqTevpF0iTy5ApilOxpDsTW0zh8usdy7xCiVX+eDyklvS3t6GgdIX7ZX+mq40CdKG4BphjV1rc0CoIwgvijCAIQvbTb8RS36cZ/zRlhX/481nd93nTHYDmzDrnTkJUAkpzcwwyfuJVPehHHy0oFHK8e2esZNSJHd6dXilERb2Zl7hkYI6JAvrtdqe/mVn6005J1/GUVaeLaGkTBCETkSQJgpCLKDGxeLWpRM5bR1J5dW0T8+fjphmYvRhrXRcbQXyED07Xoujm8HXeYmBlZcW4cePE001CrhIfH0/Pnj2zrXyRJAmC8J8nLzKQrQc1KLPr0aMs5ufnR/HixSlevHh2h/JVOnDgAPfu3cPR0ZGyZV8dqFn4VFevXs22skWSJAiCgBzVV5ogAaxcuTJ9HC8hS5w9e5bIyEhq1qxJy5YtszucnN7IiQAAIABJREFUr5Kx8asD/n4Z2ZokXb16FZ1Ol50h5GoVK1bE3Fw8pSEIwrsVK1Ysu0P4qhkZpY+Llj9/flFb95XJ1iRp8uTJJCUlZWcIudrSpUuxs7PL7jAEQRAE4auUrUnSqVOnsrN4QRAEQRCEtxL3JAmCIAiCkCOlpqYye/ZsoqKiKFCgACNHjvyi9yeJJEkQBEEQhBwnKioKR0dHzp8/T548eYiLi+PAgQMcPnyYwoW/zIDUosdtQRAEQRBynBkzZnDp0iV27txJbGwsR48excfHh/bt23+xpzVzYE2SBp9jGzj7WIdMLkfP2IpqzVthVzD7Q9X4HGPD2cdoZTIUKjOsHVrSvGLet2aauohrbF17lIB8TZk0oOEHlZHguZ9td4vQoZMD+YknNskYMxORywqCIAj/HTExMaxdu5bu3bvTuXNnAFq3bs3KlSvp2bMne/bsoVOnV3vV//xy4NU3maurRzBi5lr27t3Bqt9+pG7tQRyKzP6uApKvrmbEiJms27uPHSt/5YeateizO4g3R6bhxpL+DFx3k8QP7uBWR9S5pYyZc5Qg9TVmNa7N6DOp719NEARBEL4i+/btIyEhgZEjR740vVu3blSrVo2JEyd+kdqkHJgkpTNp+Bv/nDqHy+ExVAg5jZNHGJeXDsKxRVNaOA5hxdVoNJHHmd5nAn9M6cUPo3fjF3GZpYMcadG0BY5DVnA1WkPI4Sn0mbyUpcM70uaH8Ww9uplfurTl+8F/4Z7wUYHx2z+nOOfmya4eWnbN+QsvDaQ8PMDUXu1o9X1/5p8NIfXOVhYdeIROE0OMLA9Rl5cyyLEFTVs4MmTFVSIjjjKl+3h2PdJC7ClmdB/B5nvajEJ0PN69iI0uflxYMZt/wrI/QRQEQRBetn37dho3bszevXsBcHR0pEmTJoSGhmZzZLnf7du3MTExoWrVqi9Nl8vlDBs2DF9fX+7du5flceTYJEl9ezfTJo5nyNgt3DOsSGXzKxw8nUiNQeNpqzjChBn7iU704ez2hWz0LEIth1IoLh3kdGINBo1vi+LIBGbsjyLO24ntS3YSYN+GovcW03fUKWy+r0HszvH8eTTxEyI0o26dSsh873Iv6SZzOvdmn74jQ5onsKrrcHbrSlC2sD6yfGWxL6Xj/MHTJNYYxPi2Co5MmMH+4AecP+DEnRgJXcpDLh44gefTZ0mSDOPiNlgqVRQoX4miBjn2YxIEQfjPat68Oa6uri9Ny5s3L4UKFcqmiL4e9+7do1y5cm8ca7BZs2YAnDlzJsvjyLFXX11SFMEhUcgq/8RfTlsYUKU6DWsb4rbkd1ZdiSItKZ5EHaAoQfuxUxnfuQZWVRtS29CNJb+v4kpUGknxiegAZemW9O7SiW9Kq7D8pjN9vm9G1fxpJMR/SlOWjuRkNagM0I93xeV2KrE3trFs/z2So125El6Db8qYoCxSlw6NKlO1YW0M3Zbw+6orRKUlEZ+UUTskATrdK012MvLVtMNGpU+pJm2pZvZ66YIgCEL2yp8/P0OHDn1p2uTJk7Mpmq9LYGAg1tbWb5xXvHhxSpUqxdmzZ7M8juy/G/otjOwHsWZDBwwy/k/aO5Rei9T86XaWcktr8j93CQkZyAwxNpYDSRye2ItF6j9xO1uOpTX/h/uz9kqFAoUMZMhQKpTIkPHRA2FLKcSFhhCY7MnKrdcwbrCM2mbmHDA3RNFvFVua3GP7wVjsq+uRsP9Z8IeZ2GsR6j/dOFtuKTX/546k1EMpxREVrUGtfkSIFiq/VJAMmUxCjNoiCIKQc40ZM4bly5eTmJiIo6Pja81DwscxNjZGrVa/Nv3QoUPo6ekRHh6Ot7c3Go0GpTLrUpkcW5P0KlUZO6rgzKx2Tfn1vAb50zBCNenz0hMeFWXsqoDzLNo1/ZXzGjlPw0LRfOY4pJh99C1ZhBLVu7LP8CdWL+hOIZOWjJ3SGJ/fHKhQrw/Lb6aQJ0+mXasqQ3po7Wj663k08qdEpH5Dm2+SWN+hFHYTXJAbyZFnTtxU1pQqnsCevs344+bnfheCIAjC55C5NknUIn0+z/pFetWFCxf49ttviYuLw8fHB5VKRdu2bbPsJm6ZlAOHhtZpNeiQo1S8nMNpE0IJTjKlSD49UtUSekb6KF5egoTQYJJMi5BPLxW1pIeR/stLZGncKfHEphmS1yQ9q9Wlql/EoE0gNDgJ0yL50EtVI+kZoa9IJT5Gg7G50ZuzVXUEj8PkFLS2fF6jJgivsrW1xcnJCVtb2+wORcihWrRoQfny5bM7jK9WUlISFy9epEWLFtkdylfj+vXrpKSkUL9+fTQaDebm5syYMQNvb28qVqz4fDmFQoGbmxvVqlXLkjhyZHObXKF8Y9KgMClEMZP0vw2M3rSmApNCxUhfxIA3LpKF5Pqm5NXP9L8qUwwKEwq9FrwKU3PV2zdokJ+3NMkKgiB8sHLlylGzZk3c3d2zO5SvkrGxMc2bN3/jTcbCx3FwcHj+d+/evdm/P/3+lQoVKlC7dm1cXFwAGDlyZJYlSJDNSVLXrl3f2OYofJhp06ZRuXLl9y8oCMJ/nqenJ3PmzEFPTy+7QxGED3bgwAHCw8NfmtavXz9cXFwoVqwYU6dOzdLyszVJ6tKlCxqNuN/mYxUsWDC7QxAEIReRyWTI5bnmVlRBeH68SpKEVpveTU6HDh0YPnw4S5Yswcgoa9uMsjVJ+u6777KzeEEQBEEQcoHr168zbty45/83bdqU06dPc+fOHX7//fcsK1f8pBAEQRAEIcfT6XTPX9bW1nTr1i3Ly8yRN24LgiAIgiA84+DgwKxZs57/v23bNqKjo7O83NxVk5QQQcSnjCQiCIIgCDmUWq3m6dOnH7x8UFBQFkYjQG5KkrR3md9pFCc+ZSSRLKSJuMU/G5exZPVOLgWkP7GnfXiSjUe8ef78XqwHBw57kaR9yMmNR/B+MQOPA4fxSsqN5Wu5f2Inl0Je7Ro8Ed8zm1g0ew5LtjrjnwS66OtsXziPefNevOYvPYL24Qn+WnWYO5kedEz0OsjqndeIED2OC4LwXvG4rBhKr5696TdwEAN+6kWfKYfS57isYGivnvQZuxXvZ9eP2KssH9qLHr1Gsdk7/eGhRJflDO7Zk14j1uD2MYOff6Jbt24xYcIEUlM//CLn5ubG77//Lh6AykK5JElSc3nJGNbe8OLM5mVMX+5CKkDCZZZO3cn9u38zdcpMJvb7ka5jtqZfbHWhnF88jJ7dejBwxlEeZ+ExpPZYyPdtfuV8gjn55LeZ164F01ySSLmxkdFdejDDJT370D49zbLVF4hNvsHG0V3oMcOFpPQZnF62mguxH5cRZG/5Kbhv/JN9vpl3sJaHq7vRbdUTzG1tMLo3jw4/rsFfZoSZZT7ymT1i34rzJFjkI59lHlJubGbS2OEscHp2Zorh6NwhjPnzKEEiSRIE4b2MMI9zY8feMOz/WMmyvlY8Dknvs8jIPA63HXsItGlEuWfd0pnkJeHOEQ7u2MQJ71TQ3GHFlFn8/fdOvPN9Q3WTLxu9Wq1mwoQJzJs3j8KFC3/weu3ataN27dosXLgwC6P7b8slSZIBtezKUrD9n6zpbYbXkUsEajXcXj6TcwVqkefKLrZ4WNJj+Wr6pixh6t6n3FvSl6nBHVi0dTW9E+Yz6UB81oSmC2Hb1K1Yz9rNvGE96DZgBtuWdcUoIhxQYNOgKGfGzsbllVoahU0Dip4Zy+xXZ+SY8pO4t3cyfX9w5Id+szgeoEEXcoS506YxqnMvllxPeUdQGu7fCKBkh/706tSF/tNXMatlXpJMKvFtrz706V6XYnnL8b8efejdrSGgoFjDivgfPksCQNRxTiRWpqbozkUQhA+SzC3P+1DWHnvDu1zSDubUX+lPTyff8uQ+ZahTz+rFBS/Jk9uyWtSyUBMRFkPg37M4Y2JHAWURHOqV4suN05Du5MmTNG3aFD09PVJ9d/Lr4BW4poL20W6mLLtKqi6C8wt+ZsrB4NfWbd26NceOHfvCEf935JIkSUew+x3Mq1dHz6gspfHnfsDfzDxbk19/KorXjXi+G92HcgYmlLQxIznmNrt23CbZfxNjfxrK6pvxJMd9YjLyNmneeDywpbaD8fNJpt8MZFxbG0CGqtJI5jd1ZuxsF5IyDQAjU1Vi5PymOI+djUvSJ4wMk0Xla7wWMGS1PsM27mFF51Cm/7yWRzF3OLztPtXG/opj+XdlMPo0GTkc4xUNKF+zBd3G7kbdoBWV3/GYgKLcd9QOOMyZBB2Rx51Ia9oSS9F57RemJfD6SdxCdKijggkMCCDg2SswmCi1jhD3k1wL/LfVsmqiggMzthNIcFgM6lxaQ6hLDMLr6gUuuT0k6jM0/SdFBBIYkUgu3R05R+otrnvEI09yYW67Dsy/KSP99JHKreseJFjaYV/+xQko1dOVAJvG2BWAp367mL0rH50qPSXQoDoONV4Mm6CJDyMs9vXjXZcYgPu54xx39iT4+W0CiYQ+vI+Pj0+m1318Q+LRJoQTFJ6A9qWtpBAdEkRkEly5coVy5coBoCpeHM3V09xJ1hHmtJmVR24QiSnqR0+Q2xR4LRa5XI5SqSQ0NPQTd6LwJrkkSUrG3TOFStUtkavKUjaPHzunbCHP8FHYKx/hdiuBPOYK0D7in7MStesZkKqqy/C1m9i4bhodWwxhXJcs6nhRZkoeQzVJyZmmJYUT/OwMKtOn1i8LaOo8ltnumXsXl6Ff6xcWNHVm7Gx3Prrf8SwqX33Tk9QG7ahsoqBAg3bYhd/gdhroVWpCu1rlKGb6rkNHB8U6suLSXdwPzKJb+WCWdOrFxsB3XAqUFfi+zhOOnA7g2CkdTb+1zC0H51dD67eG/p1WcBM1TqOrY2NtjfWzl00VRp5QI/NcSee+q7ivff/2nlM7Mbq6Tfp2ihenSCEL8hSoybADwTkwOdAR7b6e4cPWcPeVa2OM82Tq29hQtUFTGtuXplilrmzy1YIuGvf1wxm25u6/HFA7nt0/laZ0391kUT33f4Yu5Dpuj/VoNHoTs/t147tGhdPPH7oQrrs9RlnFnlrPR4DSEnTdi7z2bSlZSOLBhjUk9R6C8tYdqGCPg+mz5VJwGlWXPjsjXzpONXfX0LHO90zffpITW8bRvEYH1vpoIP4Sq8aOYtTIAbSrXZvvBoxk1KjR/LHvLpcn1qVk4xm4ZUqsE5x/oU7JxsxwTyUhIQF9/YzkTFmYwvniiAy7we5HVjRUhBMceoiT+r0YUiURj3UTWX715Zp8AwMD4uPFUZQVcsd1SBdPdHQol/YfI0hnRLmSgZyM+ZHfWueFBHduJptyb/qPdGo3kCvNZjOiek26dZP4q0snOrToyZa0cpQ1fn8xH0VVhf/VC+Xgbt+ME6SGuyu60HLq5RfLGNnzy4KmXJi8Ht+XLi5G2P+ygKYXJrPe999cdbK+fL38+dA8eYwa0MU8JkhZkIIKQKZ4f1W01oeFbVox1ysNk2J2tB7wK73LhXI/5F2XEAWVHOsQtGUiR2TN+Ta/qEb6slK4smwJ7nadcLRKPy0oygzmoM9DHj58yENfd+b/z4hC33ehlucyll/8t2m9gjKDD+Lz8CEPPA8wsMgt1vy5KSPZ0hHvf42zZ1zwjcl0HGqiuO/ijPP1h8RoAV00/l5e+EfrAB0xj2/j5ReJBh0xAXfxCY4g0OMSHo9DCbjrQ3BEIB6XPAhUA7p4/K+d5YyLb/q2nq8TT8IjV85fvk2oGkiJ4Ozyqaw5d4/HIbGZ4o/n6NJFuJaciEdMCtEeM6gVcZQlG11JiTjL8qlrOHfvMSGxWiCF0NsXOHniDC6+0ZlqDzRE3XfB2fk6D2Ne/b6nEunvjbdvKOIB3n8v6fo1vHRlsLfPj3XXSQyuonw2g2teOkrbO5CPULb37cGqxwlcc0+lWm1rihQxR7/WMCa1CeTaDTWF7RywfnaC0/jiftuMajUy/2DTcG/PRgK/X8++tYtZsuE4u3vGsm23FxrTFkw7cJRj+0bjYN6ACfuPc+zYP2weUgB3Ly2FNL48ePZjVuPF8ulHSSxUhZqVVRQuXPjFiPfyAlgVSOL2X3vQte1Lecmfg6vcqT60HfnlhqT4BxBr+vJZODExUYzAkEVyRz9J8kL023uN7xINKKDz58D9Ygz7oyfWcki95U5ojTGcXN6AFI0xZkbph3OFwbtw7htPPMaY6mdlLmhAw9/ncK5HJ+qfLE9J/PFJacq8LQ3hwvrnSxnZ/8LCLkdpceWV1Y3s+WVhF46+NiN7y9dvMpyftg6ibYftmD0NocioTVRXbn9zCLon7BnaiOsmMkBF3V8PMX5SI37s6sA521LkiXtEpO0Yttg9+ymnQE/v9UGMlRUdqR3YnNsdl5FfdvQj94fwUVKvse+fJ1Qe3hjzZ9M0ycREhBOuAJlxMQqVAAwa0bhqEHP3XGFeoybov2OTr1HqY2hoCGkq9OSgMDBEhYZ7f/3A/0adR1nUkMi48kw69g+jrS/xa4tOLPUzxlIXgabmHJx2mvN7rUFIq8I40kcPp/H16Pl0Jn6neuE0tAZDPfJjGBpKsTG/U3TJXDzyGxIaWozfrm6hyIzWjDqvpKhhJHHlJ3Hsn4F4Dq3B8HtlKClT8zT4IfGV/+DMvHimbQ8iTbOcjr1LkHBmeEbw+hS3KQxH1zLwpzjat2zByodRVLDQcu23AWwPSkOzvCO9S7gzK7onLecHUbiEgic+WlpvdGVHF31O/9qCTkv9MLbUEaGpyRynnRn7WsuTXf1o1+cclRc7sbNUoc/2sf4npN5mw9ozxEj5iI6KQ4dFxrklldsb1nImBkyvLmVAR0+O+bZgb+QGpp59StXBEt+M281JU3vY14OjQTrkSXHE6SC/HIh3xyO0At0qZL5MKinmYEfSsN+YXmkOIx2rUHHCac5nWkLj48ad/NUZY5YxIdkd94DqNKoazn0/DVSXE7h1GofzVsYqrQZ2pmDbtCk7duzA0dERUGKVPx4PqSVL7W3YLF3hYtkzTLVRgC4I1wBz7Eq/iCk+Ph65XE6ePHmyeEf/R0m5ifq0NLFxA6nbcg8pOWNS5P5x0ujdYZI2WwOTJElKk2KCfKWHwXFS2ldTvlZKjAqXYlI+cnVNvBTq90Dyf5r8/mWFj1ayZEnJ19f3k7ahDVkhNTMoLP10TC1JUrJ0qFcBSQ4SGS+96pOlm2mSJElq6cSAopJBw0XSow/90iUfknoVkD/fFnJjqXjd3tKK63GSpD4jDbFWSeXGXJKStU+klf8zkQr03Cv5LWwoGeZpIa16rJVSvDZJE8Yskc4+WC+11deX2myIkSQpUdrVKY+k32SZFKSNkza21ZcUxXpJ+4OeSpHBG6W2+gqpWK/9UtDTSOnpmSGStaqcNOZSsqR9slL6n0kBqeeBUGljW31JWXGcdCVZKwWvaCbp6/9PWhWmls4MsZH0n7/fTOI9pc2jv5PsippKSplMUpjXlMY5PU1/Dzb6UvXJN6U0KV7yObNd2uHsKwV5bpZ6lNSTrIecljR+C6WGhnmkFqseS9oUL2nThDHSkrMP0mOwrSJVNDWW7KdclxI+6VN8u+HDh0tjx46VUlI+9sv836M+N1Qq23ih5P/acZ4mPTm3WBrQ2FYqUqGdNPnoE0mTaW742jZSiT5Hnl+j0q7/KlVvMEva/1t9qevOBEmKPioNrPOjtGpuW6nsgBOSWpIknU4ndenSRfL3909fJz5ais84/pJjYiT1s43H75F+6rFeCs9U3pQpU6Tjx49/5nefc+zfv186evSo9Ouvv740fevWrdI///wj/fHHH1lafu5obntGvykzzp5n28/VMMiYZPH9XBb8UCAHtBsqMStsS0kr02yqnsuK8uUY5c2Pmer9S76RwoSCJUphY2nw/mWFbCUlJpKMIcZGL5o5FeXHcTkhhZSUFJJcp1BVCSDDyMgA1Mmo/9XzBgrKjjiBj/OftCyQhta8BvWrmEJaIE/CNDzZ3odqFZqy5HERCupieBIQjMbCljIF5agq9WL2/OE0Lpj+Ldfp0u8Q0Wq1ZA5BUc6BuoUssTBJL6+cQ10KWVqgH/iEMM0TtvepRoWmS3hcpCC62PSmNIV1WcoZyDGztEAfDRrNW+6S0gZy8cAlpHaruR4YwRP3jXTNf5MVSw+9uiBP3fcwq3tjmv+8hduJoNPq0D4JIFhjgW2ZgshVleg1ez7DG6c3j2j8HxKs0PLk3kPic95NWv9RWoLdvZBVtqNwpotLqv95DrmEY9VoBGvO3uHyn0U5PGA8B5/fDpTCTbd7lLarQfppU8dT95skV3bgm3KFCLp/n2uL5vG4029YP/ShVM3q6JE+8PD69ev5+++/uX37NkoTc0wyTuQGZmbPa2x1sRHER9zC6VoUAKtXr8bBwYGWLVt+iZ3yn5T9uYUgCNlOYVmQfHpxxMRkvkrLUOipUKlUKBXPThU6oqPj0CtYmAL/8jlpub45Ng3Hsn7R9+D0K/3+9CBFVZrS1krytZzLBc9/mNyhKY7t62FT2gZVmBsXbicQ4zyeb8p9y4I7KgxUOkIfPyIx8hrX7738eJlMpY/B8zOaDJW+AXJAVbo01sp8tJx7Ac9/JtOhqSPt62Q8JSSXp58EM90CJ5fLkFJSSErJvP0krq4Zw8BBk9juGkisWk1Kmgx9QyNAjlwmkZKSREqcEytmH0V/wHE8j/9GvYwWEKVNaWxUYbhduE1CjDPjvynHtwu8AFB9M52LO/pgeHAKs5zFzbc5QzJu7v6Ur1mFzL8RNZ4bGDXlAAFaAH2KVa9GUYWa5Ge3W2r9cfUyoqpdvoyLaxo33H0pV7MaZqVtkXssYc6FOkzup8DDy5TqNS2eX4SNjIyYOHEilSpVemtU8iID2XpwLj86WAAwaNAgWrVq9bnfvJCJSJIEQQDTWthXSMLH+zHvfIRA+4jb9xIob2/Px90BIadwp4XM62DOjfnDWexTg9Fzh1DoaBeKmlZg4K4QLEoWp1iP6UyqH8Qce3PyNVtBaNUOtKnRmE6dbLg70wGrar9zL08+9OSyF/mN7OWb/Z/9q6o9mrlDCnG0S1FMKwxkV4gFJYu+LcNTYlOxLEb35tKw4YtxolCUZfDCWbTW7qG3Q2nK1huGk2ln5k/9HpQ2VCxrxL25DWk435j/NSuE15/NKFVjPNcVBsQGByIv1oPpk+oTNMce83zNWBFalQ5tyqfHmScvRZtPZGLLKDb8ugTPHDqqwH9K6h3cPCO5PKMR1apVo1q1atTsvg6jlqMYbbKK5jWb0f77ltRp8RcWU6bzQ96M9ZI8uR1RBftKGamVxhf32xZUs8uDqkxpjM9ewHLUWBw07riHVsCu7L+t95ej0leJC/cXJJMk6RM66REEIbvZ2tri5OSEra3tJ2xFy/0FTah7sjMeJ4ZQWJNEik6JgcHLJ2Rd0EpaVtlOs8vnGV/uQ6uSdGg1OiSZEuWzVXRaNDoJmVyJQg5oE4mK0mCS3yzTL/dUoh49JEJRGNtiZhnNyBoSIuKQWVpg/C+vFNrEKKI0JuR/b/uxhpjAR8QYFcPG8tVb01OJCQoiRm5BUSuzF03bmhgCH8VgVMwGS301EY9CkQoVJ58yFbVGjpFBepmpUY94GKGgsG0xzL5gu/yIESNQqVTMnDkTlepj28+FdDqSwv15HKWgQAkbXjtEhM/qwIED6Ovrc+nSpdcGuM2bNy8eHh78/vvvWVZ+7ni6TRCELKagzE+/4LhuJlvvDuS3ikYYvbaMFp+t23ncbjz9PjhBApCjUL6S0cgVvDRJYYxF/lfXU2FhUx6Ll6YpMclv8eqCH0RhbMFrRbyREvNipV485fdKTOZFSrw+T2lOsVLPphqQ38bm+d9Gmc6yKgsbyn9c+EKOIceogC3lX+/XUfgKiSRJEIR05q1Z5dkEjeJtCZCCMqNP46UwQNRF5E5+fn7o6YnxfoTcIzw8nGLFimVb+SJJEgThOYXK4J2dhb5vvpBzNWvWDCcnp+wOQxD+NWtray5dupQtZYskSRCE3EWXSESEhrwFzXLgCSyekEfJmNkUeKm5UhsfSpjGgsJ5X9TBJUWGkmpSCPMvcE9LZGTka4OgSpKEp6cnANWqVcv6IAThIz148OAT77n8eDnvHCN8lIT7x9j89xl81Xmp2KIbXZuUeMM9JelSY2LQmJu/df4b1iAmRoO5+TvWSPTi8Blo+l1lsmoEmBj3/VwxakPr8qKx579M93QrvdsEMePqH1TPaWcw9Smm9r1JrxPTqfv8MNURuqkn30fM5Wofb0ZsysP8aU3YP6AJ14Z6sKxx1mdJ27Zte22aTqdjzJgxSJLE4sWLszwGQciNctop5jWp4T54R5lRvlwhCPXnqYk1+RLuczfOkkpl8uf8N/AFJFydxnfDPGkwciDtCz3l7OIOtLmyiiO/O7yesKScYGTni/Q+MhP7D8w1Uk6MpPPF3hyZaf/We1F0sedZvRpqtqn8r586+jBawk4tY12h5iJJEl7QROHj6sEjtRnla9WkuEkK4X7BKIqWwFIF6GIIeJRCgZIFMUh4hKvrQzTF7LAvZY5CG03AEw0GSb48NqxMreJa/N09eKjORyX7ShR6Y+4ST7BvHKalimBKEqEPI9C3sSavIpXIRyHICjfl97V1sVAB6IjxvYZHuAWFdAAphLo5cep6ZXxDa7+I/6oPkZZVcSiTN1uaMnU6Hd7e3tlQsiB8vNDQUPLmzfv+BT9Rjs8xFNoLTP0pgEnnJxD0myOHO1xlRb4tjF1dk70bHd/yBMqrPqAmJLfShbJzzhGqLrzE1AbpPVs3dMjLgLqz2dGuL+Fn8zJ6RH1UwYeYtc+CTuZbOHPLm6h137O0jDN/3dIR5H4TbYXuTPylJfon5rDLbDQj6qsIPjSLfRbtMd9yhlveUaz7fgtDaqYnKEneu5jx5x580grScMhUhpQEXawHa4Z24maoJa0mzGFQTQ3zdwcgAAAgAElEQVRX1vzBqnMBJChscJw4ix5FLjJv+S20AZe5EmNFq7EzGVzG9fVplUPYNWM2u++qyffNEKaPqv/8Lafe38O02Xu5n2BAqY4TmNq5/L8bQ0zIWbTBnFs9j7+DGzKieQibt7hTpN8iRtU1ffd6SVeY+t1wXCu3oEqqC8NGVOMvl6nEzmjD1sbO7O6RH/WpX3Hc0ojjk+Pp/uMGVI3qYXjzF2Z9t57dXa8ysNFiYgoXoVCzn3F8NJsNes2pp+/OqF9qs855Gg6v5uPaYLb07YfuL2cmmG+lT40FlPvnNouqHWFUx7P0dKrDqrbXGOqxCKuNHemyLQ9Na6hxP3YLTZdIXC96E+0Xxb6rLSkhPeXY5IHE1rAi4uxACs28xEbHZ8/fpeK7cwoLIh1Z8nMNgndP50ixifxc6iqLpp7FduIU2hf+PL9Gvv32Wy5evPhZtiW8EBcXR3R0NGZmZpibf9iVSvhwpqamFC1aFA8PjywtJ+cnSZblKWV4i6iwk9wMNyTxaRL+fkk0H97mAxOkD6sJybXS7nLT14Y6dpmG/jCtR91S43C/eYvb54sybER99GK8cLpgw+9bf6DOWleG9KuB5q9f+evijzjvWknQxHaM2lSJmU9Pcb7QMEbU1yPGy4kLNqPZ+kMd1roOoV/NZx2k3WbxiI2YLdjHrryHGD51P75TQRsch82WzYwO/I2mcw7QcYGMv+/X4c8dC9Hf15MWy5zpOMUXpw0udDuzna1PfqPpzD04rk55ZdpOytbYwTqTJRzYW5iLwzowZs8+JgGQwuXlCwn89ii7Wviwcu41HnQoT6UcfyQLb6OLjyV/szqkOi7lb/t1jK93knYn/RlWt8o7T1C6ZCUVRqxj7PeVIeAIka2W4hFuzOBujZm89BBh3brguseVal1m4LXsG2K6H2BP30LI4irw0/+WcLK9PZLWgQknN9POwJOp9VIo/lMzun8/hp7+T3hjiqaw5dumEuPPBDPY8jzxpfPg7nyf+PiT+Ni1p65BFKsAUl1YuzqNQcc3MchKxy2LuvTVFaZ1x7oUDKnFuO9LsH+rHnbDt7DlR3NiNn1P3bNe4NgkoyAVxYtruLrrDsmDi+C0eSVHWg/i5+pqHj2R07TA50mQIiIiOH78+GfZlvAyDw8PLly4QI0aNahfv/77VxD+NS8vL2rUqJGlZeT8S4vCioKqMM79/QC73rXwdtvIjrztGVtdBUn32DtnLrtux2NWsy+Tx7dA9YE1IV8NmQFGKjWJSfC8bU2XREKSISaGsnetCehRs60jJVXmFGlZmZBdnmhKfkCZKTfxSKnLlArGKJVdWbkedMHLUZatz/9KGGFqWg6rxBiSinbAsfgCJvy4m8RoH6KsOiAByrL1aW5jgKlJGaxSEknWKV+ZFsn16/cJ1c6gj4cMKTKe0Py+pBoC6FPzx46sGtmAGsvK0ajzKDqJ7mdzNbl5ecoF7MKvuCMz2hbFf2I05extURKLx7q5XK44maF1Xq8rlKsgbP9ImsyTka9oAeKTdJSVJAzqd6flL1PZ522Ky63a9Fmuz5P1ofh5jKOXc8Z3okI1lGmgKFiMogaAsipj1k5g3qJ5dP/zDskVBrN6a1WsXjtdKCn/bSPi5zpx0jyMumM74L7lFEcj7lD520UYcSB9MU0IISlWNLWQA3JsbYujfPDKpmRmFCxkCIDKyADSXu5qW1m4MPniIgm7sZtHVg1RhAcTeugk+r1+pUqiB+vmXqbi5KG8Ydd82H6Xy994r5LwecybN48LFy7QokWLlzpBFHKXnH95URTEyvAm9wt1oU25fISfi6DhwEaYosFrwRBW6w9j454VdA6dzs9rHxF9+xTnH6YBUnpNyOOS/PBDHUq3Hv2iJuRroqpO26Zh7Fx7k8SMSfGuq9kT24I2FVTIUtUk60ATEUFUxrBcMqSMgUF1RIaHoQUSgsJQFbJCpZCRqk5Gh4aIiCh0GWuQuWN2pSUWaSE8SQE0t/jrl5W4p4JMLn8xRIQEcXvGM/ZWXWZs2cOWEbUxlDK29spyr0/Tw7JAYeqM2MiePXtYN3Mcg1qVycjoU3kcZMHgw55c3joIxeYJrL//zoE0hBxPx9Or7ug1aEYR6QmX3PLgUMcQMCTFP4BY0zffqZN8bD7zkvtz4so5jq3vThmZDp0OUNWkW9tE9v22Cf9GXalvoKJUqeIUaTOX/YcPc3DjEGqXsKaoMYAs/SSo8WD7pgharjiO24OrDEndwLZrKW8sV1mlNXWD/2LxvdI0/rY5dWI2MutqKVo1znQHoKosZczu4v4gFVBzy+sBGgCZDJlO4kOGOZAXsKJA0m3+2qOjbd/ySP4HWeVenaHt8iM3TME/IJa37BohB9Bo0gd0Uypzfl1E7qYl+NwKRvWbyEEvZ1aN7Uu/RVf4XKMg5oJPz4ROWzzoZGyMSjeOI+f0MDYCUHPTM5UGUytjolDSoJ0dv++8TVr2PCWYjQyoP2UF7foPoXEzK0rmicY3tDA/rVpNvVJ3aJwwkC5dL2CeGoBOXguUxSkpTWDU9EZsLaTl4baR9PC1IMLPgp83VadESGMSBnah6wVzUgN0pK9SEmnCKKY3Oc2M5kag35ThvbYwuF0ntho+JbHuDLqobrweWYlymN3ZycIpl0kI8kWKi/7AA1dFo5F9OfXzd3TcYUFkgBl9N/yQkdErMdNeZnCnf7AtruZxUUcWlhBXitxNjYtLCBW6lUAh3SMhOYTrS1dTZnhr7gSYY1f6zacpVZUGVPpjEf36O2Mck0SYSQzmoVqwVVGxWzs0C/6m2XR7VCipPXoG9l27UO+CDaYRwZj3XstIpcuLjSlLUMZoFAMbnaR0wXgCFb1YXFfBnT8ccIz4E5+ljTIVbEfrGpFsCGlIHdPKqKrH89fTVjQ1BZKeba8SP//RkE7dGnCliBFEpSFvIUNRvCIl3SfSYkpB+r1vtyityB/vgdRyKfY2m5GuXKTsmanYKEAX5EqAuR1v2TVCDiCSpC9EF09s/mbUSXVk6d/2rBtfj5PtTuI/rC5VPseul3IttXR8oIPU/594SZK0Uuj69lLdiS7SvbmNpW9XhUlaSS05D68oOW5NlJL395bqT3CRUrI75Cymjnws+QVESeqXpiZLT4PDpUStVtJotBnTtJJWq5WClrWSWi17LMWEhUlxmsyrPJWCwxMlrVYjvVhFK2mll2kSIqXIBI30LmnRTyT/kHhJI2mkFHXaa9t4J22iFPU05o2fmzY5Sgp9miC9u/T/hpIlS0q+vr7ZHcZno1Wr0z/z+D3STz3WS+HvWjY+SHrgGyIlaCRJm5IsqTMOCM2D+dL/ms+TfF46QNKk+KfhUtw7TgRp8eFSaGTi8+Mq7cFiafTc2x//ZtLipcjo5JeP+8zfq3evLMVHx0tpkiRJUrIUE/Pimx2/5yepx/p37Rkhu02ePFkCpJkzZ2Z3KF+/NE9pSr2W0rLHWinFZYLUoPd+KUGSJCnmhrT2t2XSFfX7NvB2Ob+57a30aTL8JxL/bEuHbo502laYUQNqUqJBYxJWd6Fr1x9Z4q5DRkZNyP5RTD+V9N6t5mb6FsUpUSzvK096GWBplR8juRyF4tnHLUcuB718JSiRzxDTAgVerrY3sMQqvxFyuYIXq8hfa5tVGFtgYfzuWhyleRFsCpmgQIFKX/nv2nflRuS1NHvjzfZyg7wUtDQWvT9/heT6+qgAXWwE8RG3cLoW9fZlTQpTyrYQxgqQqwzQV2h5vLEXNVrtptL4nyjz0gGixMQyP6bvaHVXmuSnoIXR8+MqRdOAfoMqfvybUZpgYW7w8nGf+Xv17pUxMTfJqO43wMzs2TdbR2xEPBG3nLj2rA1dyHHS0tIAUZP0JeieXsVdrwHNikg8ueRGHoc6GAJ8hmbpXP3pqSr052/nbkRHpmH8bPTw4pNwdhlCSLwhBS31kVCgkE/i4t2J6OS5OCf87OTk77KCFdkdhiC8hbzIQLYe1KDU/zf3Eiqw7rmS8z/oY27y6ac343LVKf/JW/nc5BQZuJWDGiX6KnFOy6lEc9uXo3ZxIaRCN0ooJO4lJBNyfSmry4xkQKlPb5bO/Z+e3Ii8rw7tbWCJlcGry71eEyIIQk4mR/WvEqQMCmPMTT5/NDmKXMXH7BrhyxFJ0pdj1H4jbu3T/6466QyXNfroqyBhryey6l0+qR89kTcIgiAIwmf2LEnS09PL5kj+Y+T6GT8gPk+ztEhxBUEQBOEzE/ckZbfP0ywtPj1BEARB+MxEc1sO8BmapUVzmyAIgiB8Jjt27MDc3Jx169YB0LdvX0qVKsXjx4+zOTLhY4gkSRAEQRA+k44dO2JmZvbSNAcHB6ytrbMpIuFTiCTpi0jl/onj3El9/5IfJxHfM5tYNHsOS7Y645/RHVRqTAyv9gyVeHMf+24mZpqiI9ptP8d9NFkVnCAIwn+Gnp4e48aNe/6/Uqlk2rRp2RiR8Cm+isZSTcQ93O5GYmhdhUrWpjmsg8EEbm/6mU6johkf0IqKn/2xXS0PV3ej2+maDOpUCo3nPDr86MOeHdYs6HyR3kdmYp+pTJ06kUQy3+kvEXJiMRtsWtKq7FdxOAiC8IrTp0+j04mOJ78UGxsbzM3NiYmJoVmzZvj5+eHn55fdYeVqhQsXplKlSl+83Nx/VYw9yLCht+k6rjS7F/kwZmFfiuSg+jH1sVlMvVWB+uWvZVEJGu7fCKBkhzX06lQQeYe6FPvrCt67t3DmljdR69rxZzFnNt0IwdvPhv4d1YSoNJBwiy1T5nAwwIgi2ngkGyDpLrtmzGb3XTX5vhnC9FGNKJiD9qUgCB9n/fr1dO/ePbvD+M+QyWS0b9+ebdu20blz5+dPugkfJzU1le3btzNr1qwvXnbuT5L0LLGMusShRz8wY3FZjLI7nlcYtJ7F3hY+zGmWVUmSPk1GDmffgAaUX2BDzfrN+aHfQNrb6rNvgytD+tmhXjiabff7s31SI8ycBrNQ/iMtro9ku9Uy9s+Us65dM4LQcHPuINaZLOHA3sJcHNaBMXsOsq2zZRbFLQjCl1KgQAGcnZ1JTk5GochZde1fKwMDA+zs7PDw8MjuUHK1oKAg+vbtm23l5/okSa2rzuQDC1jXczDTC59gTt3/Wje0OijWkRWXepIW6MGF438zr1Mvog92y7SMHpWatKNWuTwEnwZI5datNOpOKYuRgZzvWlTiErG4ut4nVDODPh4ypMh4QvP7korlG8dOEwQh91m4cCEqlfhGC7nHgQMHsrX8XJ4kRbF/bH+86zpikq8cZQr8B38haX1Y2GYAmuWnmVTZjtYDihNxsh33QrXIkJAyFpO99OtRQaECOi4HqKGSipDQCHQFDMlfoDB1ftjIX61Nibn6N//IyuT2A0QQBEEQPlouvwZa0HX1DhKiEpF364bRfzBHQlGWfpMa8WNXB87ZliJP3CMibcewxd6WbdIkRk1vyMrXxrHSp9Gwvmzu/x0/bDMn4V4CJlWNaDWqL9sHf0fHHRZEBpjRd0MX8fijIAiC8J8lkyRJev9iQo6nTSAsIJTkPEWxsXw2uq8OnU6O/G2ZjjaRqDgZ5nmNXiRDuiSio9MwtjQTzWy5hK2tLU5OTtja2mZ3KEIONWLECFQqFTNnzhTNbUKucuDAAfT19bl06ZK4cVv4BAoTCpYo9crEdyRIAApjLPK+uooRecW92oIgCIIgWlMEQRCEr8fZs2fZu3dvdochZHBxcWHLli3ZHcZHE0mSIAiC8FU4efIkJ06coEOHDtkdSq6RFO7PfR8ffJ697vsT9upQDc/oEokIi+XfjM9Qu3ZtZDIZ8+fP/xzhfnEiSRIEQRDeLN6FFUN70bN3PwYOGsBPvfow5VAwEI/LiqH06tmHsVu9eT7iUuxVlg/tRY9eo9jsnX4pTXRZzuCePek1Yg1uCVkXqk6nY/r06UybNg2ZTPbR21H7rKdrja5sjfwcUcXitWMKC07EvnWJlPsb6FqjK1sisqNH9BRcpjWjQYfBDBs2LP01fBK73jJMle7pVnq3mY/XvxzFqkePHpw8eZKIiIjPEPOXJZIkQRAE4c2MzIlz28HeMHv+WLmMvlaPCZGZA0aYx7mxY08gNo3KvXjIwyQvCXeOcHDHJk54p4LmDiumzOLvv3fine8bqr/2pO3nc/PmTUqVKoWhoeEnbcegkAE609JUNnv/su+U6MrmqfNYtGwtZ59o37qYvpURsnwVqWmRXZdjJVUH/80JJyecnJxwOrGN4dWVgIYon6ucOXkOt4DXs1tdrD+uzqe5eDuUlOdTE3jkeo4zV32JeeUtt2nThoMHD2bxe/n8RJIkCAIA2sDrnHQLef6/LjGQmxdOc/KcK37Pzni6ENxPXiPwY8ZD1sbg5+rM6TNX8A5Lef/yWSQpMojAgEBCY57Vf6iJCgokIDCEmA8ehDqV6CB/AiPf9T6SiAgMJCIxF4+ZlnwLz/tQ1t4ew7uX0A4+xV/fGQHJ3PK8D2XqUM8q02UkyZPbslrUslATERZD4N+zOGNiRwFlERzqlcrScTVdXV2pVatWxn9ags+tYFS/iRz0cmbV2L70W3SF+A/YToqHKwGla1Em4SKLBvZjzpmXaz90sQHcueWJp+fLLy//6JdGxcS4Fr2mTqZfvfwoeHvNVupNN6JKF+DGjMH0/WUPfm/Pp/4FHbEBd7jl+WqcXvhHv348piXH8DQigoiICCJjU9CRxJWpzfjf6O2cdd7JL83qMsb5RRucLnQ3fZr1ZY2TMzvHNKfxlGukanxY17EBXRcd4cS6ATRrvxjPTF8Pe3t7XF1dP8eb+6JEkiQIAmj9WNO/EytuAqTgs7UfdsVKUqNJa9o0c6C0TTX67fRHiwzPlZ3pu+o+/+Zcrr6ziT41bCjt0JzWrepTydqWplPOEP6O/EEX7c764cNYc/djMjId0e7rGT5sDS+vnsShgeWwtramfL+9xADa+0toU8Ia65INmHbtA7MkjTeLvi1Pw6lXeGuaFL+bn0qXpu/uD7k050ypt67jES8nyWUu7TrM5+azZqzUW1z3SMDSzp7ymZ6RTvV0JcCmMXYF4KnfLmbvykenSk8JNKiOQw39lzeeGIjHueOcuHCbsEy7XZcYQUh05r2qJT48jNjUd8/z8/MjT548GRuJJzZ/M+qkHmTp37G0GF+Pp7tP4v/eQ0lL4HUv8tWqSsj5MwTYdqV34/wvLaELusbBPXvY88pr7yU//v0IbVqeXHPncbhE+RG/UNltIVu9NSSFZdwndN8X/+CYF82ZKdGEhMe/9N1LiQ59QyKuI+jawddi3LNnL5f8Xo0yjZsrfqR1q1a0atWKNr8eIUaXjLLCCNbtWsb0iRMZ1kiLh0f4i62HeXMvpTj2zbozdfs+FjsWJ+XCMhbEdGfDislMXLCJn1WrWXLyRWJlamqaKwf5zfVdACQ8cscj3Jxq9raYZncwgpBLpVxZxhJ3OyZtt0J7dx79hmwjyXE9t5Z0o1yqC7M6f8eff6yi1/dz+b5LLSZMWc7F/ktpZPD+baO5zcKfhrBd3YnNt5fRtVQil+Z2pcO0XoyteotNjWO58wSKVixBXmJ47B2I1qokxs7LmbrGlY6tQ4gtkxd1YCTyAoWQB3oTZFiKisVNUaAj2v8OTyhKxRJ5IeYx3oFarEoa47x8KmtcO9I6JJbyxV5uO5EZGqFxOcsVdRfsnJ3x0jPC6KVrRyIB7q48SMpHJftKFHx2fU8K5pbHIxQ2mepEdNH4p78B0kPwJlBrRfmXr63o4v1xdfVDKm5HrVLmWVqr8nnoCLnuxmO9RizZNBvTdceoVDj9d7Uu5Dpuj5VUGVUrU39qWoKue5HXfiEln0zlwYY1VFtzCOWOjVChHQ6mL5YLPDyebr+cw6zeN1ineDJqdHlmHlqJYxEdN2Y3pcHhtpx3m0ktFZBygd8azqXCuUPUWv62eUfRarUvxqWTm1O+XAC7/IrjOKMtRf0nEl3OHtv3XvHiueb2FIXZYDrp+nNhfROMX1lCXsSB9j+UQfNKD4PyPMXR+9f7OIFrrnE0/7kndqbBnJMMMTJK5eLkRvS6YksVKwWauEAC5d+x7NAcGnv9Tt3vXel95gJTahoAKVyZ3IiFVa9ypF/m/lzkFHFozw9lNLwcppw8xV+NUo9aY49z/GerTLUm8RC2n5FN5iHLV5QC8Unoyr7YkrLqGNZOmMeied35804yFQavZlWRJ4T6eTCul3NGvVkFqilTIWNEVaVSiU6X+2pVc31NkirpBFNmniEph+57XZg7/+zN+KVx6BpBn6UqNTMt90/s5FJIxg7QheOyYz3HfdSfZ/Mx7uw/dvfFLxlicN9/jLsf3CzxFqkxxCQBiV4cPuxF4iduTvgUqVzb9w9PKjeisbkW/8MHuKb5huGzu1PJXIGyQD1+PxFE9J251NcHs0aNqRp0mD1XPqzJTPv4OMdu6Phm2Ey6VjBFripEg3ET6VQknOOHLhJ5aAS1ag3nYDygdmJ8vVr033qOxdO2E5R2l+Ude7P53m76l6/MN7WrUaVhU+xLV6Dd8lukkMihEbWoNfwg6auPp16t/mw9t5hp24NIu7ucjr03vxaTsqo9VWMvc84jhgvn3DCyq0XJZ2fDRFfm/a8Mpet3oOt3dpSq/hO7HmnRhRxksH05HBx74dikJ5sfZ3znEg8xolYthqe/AZzG16NW/x0v1ZJp7v1Fh4qVaNVvCB3sytNivgef6RuahZK4fs0LXRl77PNb03XSYKpkJBlJ16/hpSuNvUM+CN1O3x6rgCSuuadSrbY1RYqYo19rGJPaBHLthprCdg5YZ+Qv2vsr6TfWm457rnBk3VKWb3Vic/PrzFrtCboobtzUUMPsFCuPRqcvH+SOl0FVapjHvH2ehRxLS0sSE1+cSXRPr+Ku14BmRSSeXHIjj0MdXr9bSUd8yGMinh3KKTe5HliLPtM7Y+vjypsqMXVPrrJvxw52vPLadd6XN54WJSnTAFGvlueBi39lGtU2QBfkxBVlc1pZKwA9agzdhZPTKc66uDK31F5WHokBQG4SzJZhc3B95wGk48nVfa/FuGPHLs77fsDJO/Eo8+cl0//EFc4dW0/3MrKXEhyNx3Y2RbRkxXE3HlwdQuqGbdwsXoriRdowd/9hDh/cyJDaJbAu+uImtISEBCwtc18nfLk+SdL5PcaghgMGod643/TjDc2t2SryyHQm7/Tk3r173LsfRNxn7988BfeNf7LPVwO6ME7+0pERp00oZ/shP/HfTxt2imXrrr1oUtCGcWrZOq590i0lKZwY2Zl5t1PRxZ5n9erzxOawz+0/RRfF7TtPMC9ZinxyLeERUWBYkEIWckg5xTiH8lSoWo0qlTuyyleL3MKWkhZh3LkV+kGb14ZHEIkRhQpbvjjhKAtRKD/EP31K8ptWUtZg8uJ+WKuq8tvlkwwvLgcpGV29xdwJ9ufYACOcZi7h1FsuFMoak1nczxpV1d+4fHL4a/PleevwTdkALjjt5fTVNGp+Ux2VDEBHyPYp/HHZhskugYT47qdL6lbGzT7J/R0L2RhQm/nX7nH33C9U/aB3D5DChWWzOGY8mCPed7k+pwrX5i3mxNses84hUm9vYO2ZGKTUaKLidJlnsGHtGWJI4OrSAfy/vfsOi+J4Azj+vcIdUkQpAqIo2FAxakTsqEQjGg1iN/YWjSWxRc0vsWLDEhONsWGJWIMaW4wlGkusYMOGFUGQojTpx93t7w9sICpWIM7neXyeY3dm970TZt/bmZ3p0GokgaVqkRHki++B+yQnSzT89nf2LO8Hm5fwZ4QeUh+QdQgNgSt8ie0yncHOj9ooNa7eJzg2uSZkniHwekV6TfyEK4v9uKWDtMDT3Klci6qyF+xTgYuLC7dv334cZvqJE0RWqY2DQiIpOY3If+az+EAMen0c53esZtHCLVzKSGf7sNZMe9jNqgs9SVDxmriUaEZr2z1MGj2HnaHZv9kqq3ZmwoyZzJyZ/d/U3rXJ1urqozm+9ieW7L/N2Y3Tmb/jMqnkON+dQK4n3ufoyoVM8r5I81lf46wEkMhIjCIsLIyQi4c5cceGj6pkJRzK2qOYXHEjX88MeEGiraRq5wnMmJkzzqn0rp2Ha4NhddycLzKv/wAG9OjLhjsmJEQ9+XtXOlTE6I+BNGnVDs8uq1D06kljt5FMdd1DlwYt8fy0BePPOFCr4pNbd7dv36ZWrVovP3cBU8i727RcCQjHvk55YrcOYGJUX3w/cszvoJ6SwcWLSTTs1ZsOFY2xr2CL8btKS/XR7BnbE++UEfgv9cJekcqVjVOZ8fsV0i0bMnjKCNz0fzJn6RkiL9+i7IBOaI4Hows7yrEEW1qOnsZXdVTP1Gn0glOmXtnI1Bm/cyXdkoaDpzCiiYr9s38hSBfG0WMJ2LYczbSv6mAQtJqJPtuJMKuJczEDXDvZsnp/EJfjfPGcCfrEsywZ2olzURa0HDeTQa5v+liJ8EqkFFLSoIixETKUlC5jhyL5Jldv66BCBVoNGYXj8aWMXH6b+xkSyIwwMoT0tLzdC1HY2GAlSybkehg6KmZ1M6Xd4Ea4hPlHNg+/3evJ+qKqQ6d73jcJBRVd6lBMbkbtWhWRL4vgbgpZ3T16fdagWZ2O51bPdqgyNKxvy/xVs4iMcWZwfVM2zQPQEnYzlAxrdxpUMkKudqNBFQWrQ25y0zQGyaIZlW0VyGXOVLWTcfGpQz76pq3T6XJ0cWRyJzwabfg6+tTYiRywsy5BYoIejAru91SV89fsjXk2wUTlzNd7Y3h2jyt/R454+NoNc4Ae/oT2eLpMIucvJPPxsCrZLz5KFSpAd/M0l8yqM6RRD5JmfIHv6X58HniZ0h9PQ3Vz/XP3FSFrPp7x48cjSRIymQyjtisJbJt1+Orj93NUq0at0hA4YxDb6s2mT5XzZGYEcVX6BI+PsjoNFRXH8s+urDo91x+hE+/9cDoAACAASURBVIYYvu5VUm5NvW5jqNdtzJNtqSeyn89xONsvj0SXlobsK+OHXZcZgJYg3/503aZAlxpDhLwJPsUlSAXk5njOmsq+JsOY+dkeGr9meFnUuC+8invOzYrKDN0ZSLtb0chty2JTREd6pgK5ugF/Phx7XfPgWcbdi0VjbIn5w4VT28w9SMvkWOIlU6xMsy9/s23bNoYPH/5G0eaHgvsXmhf6+5w6l0Dy3kHMUU9h85Rm2BSkd6SPJijoGv+umMGPEzvh9vk8zr6Te+zJBEzvTF8/ic+Hfoa9ArTnZjHI14SRv23il7Y3GT/Kn/sPLrF9zTVqjP6OtvY32bviBDbfrcNvmJrl0/yJzKXOc6cK0Z5j1iBfTEb+xqZf2nJz/Cj876Vxde8KTth8xzq/YaiXT8M/8iyzR/hj/8MaVg234Pjqw4SW6UjHehVoNbI/LirQ3X1A2VGrWP21Gt+ZfxD/Lj4i4fkUFlhbGvAgIQE9cuzadqeF2Wl+HvQDG0+nYV3JktR78U8Gi+rjiX9ggHXJEnk7vL0nnRsbEzBvEJM3BxJ86SCLh/6PTbFl8OrSGOMihqj0UYTeTiH25CmCH/UGyOXIpAwyUjPQ6AC0nN+3i7CMaPYfvojexpFyJgqKGKrQR4VyOyWWk6eCeVJdhpSRQWpGbt0LSj52q4NR2E0iyzagkZ388XbHyhUxuvsvO0/cIzVkK3+d1VG2Wg2cKzigijrBgXOJpAQf4Nij7jZFEQxVeqJCb5MSe5JTwTnPp6JChTIoLT2Ydfg8Oye055N2balnXZAaq/dEr0enkyN/ekCW/h6HV/lyKFJP0pkzxDi7UEntSPc+ZdmzcjsnL2qoVsuO1BfsU5A1MLhZs2b89ddfz55XrkatAjRn+COoDO0bWuDY1J1KshL0/GUOHsVyiVX5BgnS8+hynE+uRKmQozYxzrFWpgG1R+3k6NFjnDh7jWNf3mXs//wf75VZezFraiU2DpuV7Qmyt0puQsny5bAxVoBchaE65yg6JSZW1o8TpMdbTSyeSZBCQkJITU0tlHeSCvdfqSaQ05pPGVxHRuh9XcG7LSYvxcAtl/j3j+Us27CdUca/sezfd/AbrU8mvbo3B3yd2DhoJgGpkBgQwLWoQ0zt04Uhy4NJun2ZGxowcHbHs7YTpU3kKCs1onlZQ0ydKmKbkULMydzr5CoxgIBrURya2ocuQ5YTnHSbyzc0oKxEo+ZlMTR1oqJtBin3AjgnNeKzioaoy39Oi4+eHdqorNSITx2MMK3ihG1KAkmi6+09M6W2axVSr14mVAfy0r1Y8vtkGsYuo0fdKlSt157xx4rS5rvxdKugRHf7IsHJlXF1LZq3wyvKMWi5H6M/us2PnV2p7OzO13+q8PppAzObm2Do1olOZa8wrY4tNX4IpqilAXKZDGXZqlQyCmZW48ZMv6IDZKhv/IirWWk6+Bfhi2mjaKI2xK1TJ8pemUYd2xr8EFwUSwM5MpmSslUrYRQ8i8aNc1sUU4ZRAzdcDBVY1Wv0eKwNyLHqMpXZ7TJY1syWohX6cdjxWxaObUCZbpOY2Dia2Q2ssG7zJ5mPvpEZutGpU1muTKuDbY0fCC5qiYFc9tRD3yrqjpzFYJs/6VLKlCoDNxJp7kipgj9y++2TW1DbxZQjfxwkTg+gJ2bvJIb/ch2lmZagwGAca9VChZwSXl9S7+xsVt5wolZ12Qv2PXlqbtKkSRw6dIjdu3fnfn5FCezVF1m3cD5Ldt9CZ+pIedv3eOV4rfNpSUlJR6Z8up4ca69ZTK3kz9RNMRTkJvPChQv4+PiwfPny/A7ltRS4vOJVaK8EcLuUC9Xdy2PWZQrT7UczsmvNZ55GyDfa8ywb+ydV5/6Au1EGqRlFKGb2DlpGuQ2N2jSiUqNazN7/CV/90JDtbiUoWa8jK5e2wjThOGt3yqioPAgyxeOnamRy+ZOGXAJDq+fUyU0RK0qUrEfHlUtpZZrA8bU7kVVUEiiTI892UDus089yPQ3KyW9w844OZwAZSFLucQjvm4KKHTpS68c97L87lkql5Vg3/R+bL4whOSqcGI0pJUtZYCgH0BNx4CDXa7anfcW8/y4rSrdmxt7WTIwNJTzBAKsyJTF71PrYdsD3sgczHsiwMDd+6pvbIHaEfMbtBCNKW+ygIwqqDPmbja1TSTa2ocTDrirbDr5c9pjBA5kF5k/3Zw/aQchnt0kwKv1UJEZ03ZRE14c/7U4Z+PBVawLTJj58XZX+6y7yxbwQ7qQWo6yDBVmX4TqM3h1C/8hYFCWsMdKmkikzRC2X08H3Mh4zHiCzMM/Wpb49vffDV58z7/hnTI2LQ2tihVn2L9ofECUfj5xH5y4DqFuvPJWLxXJH48JIv8k0MIxkXpAB1btYZv0OGDdhgIeMlTuqU8skkr+ft++pXF2pVOLj40Ny8nOm9lY4MnDVWjQaFaoC/X+g4ejEulSYq0AmacHcjYkrvCD6yJMicmu8Zs1g28Ge3M+/QF/KwcGBRYsWvdEs6PmpUCdJypqT2bss6/W6f7rnbzC5UVamfpVpDG/bmRXGkUQ5jGN1rXf5kRfD3XsBn7oP4Ie6C+ibMJ7PO6zHPDYMs74r6PiS+4ZFWo6g74av8lbHqCUj+q7jq887sN48ljCzvqzolEthw+aMGrSFQa1asdTSgPBUGdXlSuwdJcaNmEITP5s3ftfCm1NU7MfYdr5M87vC4P9VfbhViYlNWbJNkqy7it+6UDzH9MfpNfJ9Q4sylM/tARelCVbmuWwuVpryxYBkGXKFAkkmw8iq5MOHip+ubsWz1ZUUK12e3HpSXk6OkXU5KuV2TFvrrJcKo6ce41diktsbyEaBsbnVS8r898nNGzNh7zUmPLOnDCMOnHvqZyXVJ50mZVLWT9VesC8nE5MXTe1d0BMkNS2WhJG4JJddVRdy9akBRHLr9qwOLdjr1L34/6Lgk0mSJL67v2P6jATi042weO9fH/WkxseTaWzxCt9cX62OPjWe+Ezj57833TU2/RRApWHdqKbZzVcttuJ5YDEe6qyBrnJ54e7xLQjKlSvH3r17KVeu3JsdSKchXavIZexBtkJo0nUoDFWFYJ4f4ZFvvvkGlUrFtGnTUBXsDEEQsvnjjz9Qq9X8+++/TJ+eW9f5u1Wo7yQVFnJ1MSzULy/3Ds6MUfFXnZfi1erIjYrzwtKKUjgUmcn/vDYhR4bD0Jk0e/hZiASpgFGoMHxp5qNA9fJCHyx9yj3uaYtjbSaaVkH4LxBXKeEdM6LW4BXs+PMPtv25hZ+6VhSZuZBvdCHrGDpx56tN5KgLYd3Qiex8aSU99/1603rOBV5nIRVBEAoecb0SBKHwS7nDudPXSbWuQZ1K5ih0iUREaLG0t0CNjoSICHSWJUgP3Mu+U9W4EeWOgzwRjRFEXAjDoHwtKlkp0SVGEKG1xN5CDboEIiJ0mCgC2bvvFNVuROHubJNtPJQ+OZQzgSHo7GtS2zH7wkjauKsEnL1NulllarvYZ43t0icScvosN9OzljuxUT9nG5B8O4CAm1pK13KlfDEFoCcx5DRnb6Zj6eyKs82r357++eefxR1coVC5evUqbdu2zbfziyRJEIQCQ3f3HxbPXsvdxt/QPPI3Vp+2o/+8EdR/wcKM2uBldO26AnnjBhifG8745kvZOuAcX7aPYOpxb2rKo/Dr04F7M9bz8ZHLxN+KY/NxDxz8WjLjXnXq1y5J2IGxNFy0g6/Of0n7iKkc966JPMqPPh2iGT72AZfjbxG3+Thelb1wVDw671I6dVmF4Sf1UZ74htR+m1nwMKbUY5P4/OsAqrX4CM2JYXxTYyknZjuxs08bFhk0p4H6NCPG1sX3wFeEDsyx7eB4iq3uQtcVKpo0KMK5sdP5fPnvdLk6iDaLDGjeQM3pEWOp63uQyXXyNr7owYMH9OrVK9uyHcLbcfDgQUJCQmjatClly5bN73D+c5ycnPJ18LdIkgRBKCD0JCVa0ayehnbz1+LqO4YGezzZEzKM+h89r6nScGzxfJL672LXkNLIE7fSu85ctnb85NmiijK06lAf68jafOvlwBY/PdUG/8byruYk7ehHvfk7+OqZ6YvllGzZgfrWkdT+9kmCBBpOLP2V9EG72DSoJFLoHladSUcbnfU+0pRV+MZ3NF7VIGxHLC3nnyVGW5zLwRnY92tGd69R9AwJx1QfzV85t2UcZtrcBLr/4U9fGxkPqvTj059309Q+mAz7fjTr7sWoniGEv8KK3m5ubty4cSPvFYQ8O3jwIIGBgVhZWaFW58vg0/+8yMhIPDw88uXcIkkSBKGAkFOsshNhG29h324qbUqF8H28E64vXLpdS2SUhlKflng4f045HIrHEpVtMUA9uT7DqyiFk1PWJDtFHMpgGh2Rfb9eesG0XVoi7qZh1+zhenRlWtCvjJ6YxVnvQ0U0W4a7M1tmSakSSaTqKyEpqjNq2Thmz5tNd59LpFX5isV+3z27bZEd4VG3OPttLw4+nFqmSg011UctY9zseczu7sOltCp8tdiP6rZ5u5PUp0+fPJUTXt2+ffsIDAykWbNmtG9fsB/HF15dge6czoi6zJW7GWjvBXP6aiy6l1cpAPQkXNzJ8oW+7LyU+JyZUDVc2/0Xl/KwGPPLaQne/jNzZs9hztx5/LJqJxdi38P8q5oEEt54gc4ETm/ZxZWXfg65lUvh3ObNnIs4zZZdV9CknGPz5nOkAJqEBAr42qHC8+jvc/y0AW7N7JDC/yWwaB3qFQHQE3d+B6sXLWTLpadnrVdRvoIplwOvoQH09wI5F29PBVs1iuR4EnSA5hY3wx/+TchkyB4lP7pwrlxJBPTEX7hMioMTBgYKkuMTyKp2k6xqMmSynImWinLlTLkaFIIWyAjw5rPea4nRA6Sxa85s0gbs5tg/u1jevSIyvR699izrVt3DY+FfBF4/zmDNCtYcO/nstnP2lLe3o/WsLWzfvpWVg+viUMaGs+tWcc9jIX8FXuf4YA0r1jy18LQgCO9EgU2S9HHH2Lx6FoN6jebnjftY/mVvFoc+ufjr4wPZ8tfVfIwwd9rLP9P96z/RWmawdXBPFl7Nmdolc3HVANp2XUTAW2nhNJxd8wsHkophYW4Kt9fTt1kfNt55l4lSBruHd2b2xTfM8nTR7Fvgy8mXfQ65ltOTnpJCStQ+FvieJEOfTkpKOvqM3QzvPJs3DU3IJ+knOBFZhdoOCqSkZNIi/2H+4gOEn/Jh9CZTPmtRhWLKp6cgUFJzqDeN/ulOI4/WuLfxxfz7/9Hatile1Q4wpHlrWrWexwVDA2SAwr4qjqe/p8XEo2jREDC7PZ93as1nP8oYPqYlRZt6Ue3AEJq3bkXreRcwNJCBwp6qjqf5vsVEjmienPfjYZNx3d2Vxp+1pvmQo7j0bImlHEDFR27OXJzXnwEDetB3wx1MEqKIkjlQ0egPBjZpRTvPLqxS9KJno0rPbmvsxsipruzp0oCWnp/SYvwZHGpVwaGiEX8MbEKrdp50WaWgV8/6iM4dQXjHpAIs49BwqWFPf+m+lCZt7uUuTTyX+Xif9pK31KiDXz5GlxutdMm7qdRuZawkSZJ0f7mX1HTaFUn7VIm0P7+T2o+YKX1Zz0ta+eBtnDNFWtephjT8cMbjGK7MaCS5Tjgt3d3uI02aNFzq1PMn6WTcFcl/fB+pg1cHqd+0XVJopiTF75stec/1kQZ90VUaMPVP6XamJEkpOcplREo7pv0kHU6TJEkXIW31ni9pr66Wula0kap3XigFPDqtFC/tm+0tzfUZJH3RdYA09c/bUqburrTdZ5I0aXgnqedPJ6WUK/7S+D4dJK8O/aRpu0KlTG2wNMOtodT7f/2kzp37SZO33ZIyJZ10/+iv0jfdO0pebbtII1ZfkNJyLRcn7Z45U/rz5AypiddK6UHcbmnmzD+lU6u7ShVtqkudF+yVtk3zlrZH6SRJ0krBayZKi58E+5/i6Ogo3bhxI7/DeAd0Unp6hiRJGdLx/3WRJp3NfEHZTCnp/n3pQbYiOik5OkK6nyZJOq1W0j3erJW0uhTJr11laej+WCkyJFyKf7qeLlmKjrgvpUk6SavVPT7Wk9c5zxsnpTyzSyclRVyXbkQmS1pJJ2WkpT9uBzKTYqSo2JRs7UJu26TMJOl+zAMpI8e2mKhYKSVbQSE/derUSQKkTZs25XcowjtQYO8kgY47J69i37IFFpqznIpxxq1SOkGrR/FFu44MXHSGJAn0kTuYNXkyIzr34udT8QRvmkDfju3o2H86f4VpIeFv5kz9kVlfdeOLL6exK/RdzmCiJTIqExu7rNXjTGytSI0IzzZnimGr6Wya3RaHd/YVUIFDzSo8uBrMg0vbWXOtBqO/+xz9/MEsVg9jpf9COkdNYciyEJKD97D0iCXfrvyVHgk+jFgVyvm5OctdJmjfIW5mAlICF/YeRlGxIx3rVaDVyP64PBoSoU8leM9Sjlh+y8pfe5DgM4JV4Qlc2r6GazVG893neuYOXox62Er8F3YmasoQloXqQReNrsZk1izsyv2Zo/CLCGHn2mvU81nPJl8vYn5ewMEMni13N4Xrhw5xIz3rjpk+7TqHDt3Crn1H6lVoxcgv3amkOMji3++g155j7YqbFC8nZhouXOSo1SpAQQl7NRfXLWT+kt3cyrXfXYmJhQWmyuz1jUuUxMIQ5ArFk9vmcgWKRz/IjLEpa0exbGuHGlOipAWGyFE8Lvj065znLY7RM7vkmJQsTzkbYxTIURmqH89QrjSxwtrcKNuM5bltQ2mChZVp9tXhlSZYWZtjJObzFIT3ogAP3E7iRMB17j/YxJLLZygy5H80vDYPj3W2LNgyDbmvJ80iQHpwie1rrjFg3XjcdPPpt1jN3K3+2B0fgeeQZTgt0rFn6RG6HtzIrxHf4zliFdU39cfunaSHMpRK0D7MiiStDrlKxfte1k+XkoLM2AgwwNndk9pOSvwuaHCbVA0ThRI3z1r8sOEimuoGuLRph6OqGHYe1YjcGMjpuBzl1l3P+1gwAxfatHNEVcwOj2qRbDyvxdHAGXfP2jgp/TivcWNSNRMUSjc8a/3AhouZlDeuQ+vP7FAaWdCi6gT2XbdlVDt75o7ryu8p8VyNs6W9BLKc5YIzKf/CYBRU6OKFfOAmgmvG8K9jJ8YUf9NPVsgfChwHrmKtRvMWl9QwovvmyxTAFR8F4Z24c+cOAQEBnDlzhvj4eGQyGRYWFnz88cfUrl2bkiVL5neIBVLBvZOUcZaA1E5MHteV3hMXMPEzWzKDgsis70ElI0Mqfd4C54cpnoGzO561nbC6fgGNmyfVTBSUcPOkVswZLmrAwKUN7RxVFHP1oFrkKc5nvquglVSsbE7o5XB06Ai9dBvrKu95hmldKL+vO0+N5vUBkCkUgAFWllrCQ9MBPQmhESitrVGiJzYmGh2QHBGNyqYU1rmUM5BpSE/Tg/Ye9+IeDYDl2SeG9LHEROuAZCKiVdjYygEZWSFYYakNJ+vQCYRGKLG2ViBlRHH3nh70sYRFGWJjspUxo4OoP3U1/qu/oW4RCT08W872+Z+qjKyBufLSHWhfdB9TfznLR12bU7iXWRTEmmOC8Go0Gg3r1q2jfv361KpVi5UrV6JQKKhcuTJOTk7o9XoWL15MtWrVaNq0KZs3b0arFfPFP63A3knSR10jQq+CTEPUWb1XKG1KoD8SRjrOqCKjuKcvkbVDpiDrOmyJ9kgo6ThjlBBKhNIaayXoY2OI1kGx5AiiVTbYvrNb1XJsOw/HpVMvPE+ZkZDYkBkbbdAGTMZjfjk2+XXPZaXyt0Afjv/QJpwyAV2GDksPH5Z1suDBrEcF1Lh/3Q+/QW1ov86M+5F2jFhVE+V2HTfXDKfHDXPu3TJnyCoXmif3Y122ci0oXeQnBnb5gsPFNITp5YASe0eJcSOm4P73VJo/moJYd5M1w3tww/wet8yHsOojJeseh+DO1/38GNSmPevM7hNpN4JVNQ3YoAxn45COnFTEEl9lLL85m7PY7Cc2/DiRo8kR3JAeEJ8kQ56zXAUF/rl9Fkp7HKVxjJjizt9Tm9O2kyU/TLJjU0PDd/HJC4IgFEinT5+mV69eWFpaMmbMGNq0aYNCkfvFLzMzky1btjBnzhxmzJjBb7/9RtWqVd9zxAVUfg+KeiWZVyTfLxpKTTt0lbw8akoVOqyRtMEPB+5KkiRlXJKWftFIatLuC8nTrak02D9U0kQskJrZVZU8uvaTvmjWVPpyY6j07sc8pkvx9xOkAjdMWJcixcU8iksnRSxoKbVcEColREdLD7TPK5cl7f5dKSZFJ+m0TwrqdE+NVtVFSAtatpQWhCZI0dEPnvsZ61LipJiEHJ9MZpIUn/TUyNnMeCk8JFJK0kqSNiNdytQ9p9zz36iUFZpOitnQU/rMO0jKS63C6r87cFsQCr6COHB74cKFkpWVleTn5yfp9fo819Pr9dLixYslCwsLyc+voD0YlT8K7J2kXCmd6Lf2IF3iHiAr9mSw5D9bHu5XVWHA2oN0i48l09gKMxXo74LBR4NYvKgThnILSpi+jxGPaopZFMCHc+VGFLd68qOBpQMOFMG0hFX2ftcc5QAMLWzJeS8m+xpQBlg6OEARU0pYPb8XV25UHKucG5UmFHu6L0xZDLtHs/sr1M8v9/yzIJfruLqsN/222DNpTdWCe8tUEAThLfrll1+YM2cOJ0+exMHB4ZXqymQyBg4ciJubG82aNUOSJHr06PGOIi0cCuG1Q4Gx+YtG4Moxyp4JkHXtLsELrt0fIDlWXRay8K0dzoouC9/a0d4CBZUG+PHvgPyOQxAE4f3466+/8PHx4ciRI2+0jlzlypXZv38/TZo0wdHRkQYNGrzFKAuX/3zaILfqwsKFXUSCJAiC8Ab69OmDt7c3p0+fzu9QAFi2bBkeHh6cOXMmv0MpEOLj4xkwYACrV69+KwvtOjk5sWjRInr37v1BL4wsUgdBEAThpaKiomjTpg21atXK71AAGDBgAMbGxmRkiMVZAKZPn07r1q1p2rTpWzuml5cXtWrV4qeffnprxyxsRJIkCIIgvBeJp5cysu8YVl1Izu9Q/lNSU1NZuXIlY8eOfevHHjduHIsXL/5gpwYQSZIgCILwXpgUf8C5S0WoWVnMWvY60tLSct2+ZcsW6tat+0oDtZNPrmH5kZjnLML+RI0aNShTpgy7d+9+hUj/O0SSJAiCIOSd7i7/LBxB/++3cuHgIkb37c+8Y0l5qpp+9jyxNevh9J4fGUo/uZJf/o5+aUJQ0FWqVInZs2eTmpqabfuxY8do1qxZHo6g5cLvM/D29mbe3mtc3raNoDzcIGrWrBnHjh17vaALuUL4dJsgCIKQX/RJiVg1q4em3XzWuvoypsEePPeEMKz+Ry+5oGgIOnkFh9q1eK0JUnQhrPtmFUXnTKb1C+aG1cTcIDgyhWwLAqhLk+zTnRFpv/Fjm5K8i4lgAgICsLe3fwdHfuLOnTuMGTOG2bNn8+233zJkyBCMjIwIDAykW7dueTiCkmqdvqPaK563du3aH+y4pEKTJCWHnuZcdDGqu5bDNL+DeZo+nlMbVnIo4qkVzmRqnNoMpk2l9/HxagnevpCdVzUgN8DEzoVWXg2xf04rpElIQFusGEY5d6RcYPt++OTzahi/65CfI+H0Fo4ZtaZxxg5240H7GnmMRJNAgrYYBtc2v1o9QRBembxYZZzCNnLLvh1T25Qi5Pt4nFzLvfxioo8i4JwBNXrknMIlhTvnTnM91ZoadSphrgBdYgQRWkvsLdSgSyAiQoeJIpC9+05R7UYU7s42z7ZhWSch6ep+Nu+5k2PNST2R0dc4cyiY5DYlMXvtd/98Pj4++Pj4vIMjP+vevXuPk6UpU6YQFhaWh662FII2zGDBoVIM/XkQVcN/Z/z0cD6bP5KGuX+Yjzk4OBAeHv7W4i9MCk2SpErdy+SZNqzbUsCSJLkSIzMLLDMyCFwyndAW42nvYEhR9fta1lbD2TW/cMBpLB3L6ok7/D1t/x7NAd82FMtZNGM3wzsfofeOabjmWAZLn3iIxYvBpXU1jPOlE1ZH9L4F+No0p5FTCil5vjGewe7hnTnSewfj9K9STxCE16Pn/vHTGLjNxk4Kxz+wKHW+KZK1J+48f+48RpiJG/3b2hN3Jx3zMlZZd47SAgiIdKZrBSVog/hxUgAdJzRgd7eurJA3poHxOYaPb87S7d/huP5L2kdM5bh3TeRRfvTpEM3wsQ+4HH+LuM3H8arshWOut4PkWDQayORG2bemn/2FMYpf+XOC+1tPkCpWrEjdunXf8lFzd+LEicevTU1N6d+/Px07duSHH37AwMDgxZV1CSg/6km9379gTWBffMolcDPOlDJ5WLHJwMCAzMx3tuhpgVZokiT97XCK1/ck88pFwuydsS8wNwtMcf6sF86kotq1iCKf9qBPAxXoI9kxaylnIi9zy743HdOuYTbyGxqp7rJt+mY8f+jHlY1TmfH7FdItGzJ4ygiaWL9mdiIzoVLznvRppELfIoN9Pa5wP7M+l329WfRPGMmKsrT73ps6Z1ezP+gycb5erO5txB9TffC/mol148FMaAf6xLMsGdqJc1EWtBw3k0GuT5qT+H2z+SVIR9jRYyTYtmT0tK+oFrWJmbM2cjHJDJe+ExjTwoC/5jx8z2UH0F1zlPOZd/j3eCo1en5BiYMr2BXjwBfeM/iiYhLHlmSPz/XhubSJkUSSyMFZQ1hyVotMJgNlBbrN+YG6N5bhvegfwpIVlG33Pd51zrJ6fxCX43z5pHsSkQZaSA1m08xZbLyYhJlLXyaMaYnpP8/GX+eZLFIQhJdL58SJSKp0c0AhBZOcFsmp+YupONiFkxN2UG92H6qcz0SRvp1hrQP4+vRPuKm03PRfx+F0DbazJnPwwlYOV1jClycWMz+pP7t2DaG0PJGtveswd+ugcIoW0wAADgRJREFUXCa5lVOyZQfqW0dS+9vnJUjPZ1i5L7NqGj2zasDb4O3tjbe39zs48rMMDQ1RqVQMHTqUUaNGYWFhAYCxsTFJSUlYWT2znsETCjuqVNEhdzFh68U47ly+hNM30ymdh8tOUlISxsYF5qL7XhWSgdtargWEkBC6i20bx9N57N+kBG/n5zmzmT1nLj8t3sC/YVlzZehu7mHljsukP6qaeJY/tl8g9bnHfkekB1zavoZrNUbznVcJLu47xM1MQErgwt7DaM/NYpCvCSN/28QvbW8yfpQ/sa97Ln0kO8Z50qpVcxq6L8S0dxfKhO9k7bV6+KzfhK9XDD8vOEKZjh2pV6EVI/vX4NpP37DSbBS/bfShQfBKtoTo0d19QNlRq1j9tRrfmX8Q/+QEpF3dy4oTNny3zo9h6uVM23CIuYMXox62Ev+FnYmaMoRltxOevOe29lzdu5JTdj/gN8uZvV//ivS1H8u97vLzr0fRheaM7yCaR+e6fohD1+W4jVnN+vWrmNAkkyjjj/m4+B12rr1GPZ/1bPL1IubnBRwp05GO9SrQamRfnG4f4tD1JM7PHcxi9TBW+i+kc9QUhiwLISVn/P733vi/WBA+TEa0XRnIwmZqUFZn/P6j+E0fTMPwbQSVaU9DC0eauldCE3QV6RMPPlIBKCnXezOh13cwa8JEZvmf5cR0V+SRUWhKOVBCDmBMOYfixEYlZD+dXso+vuh1GL6bBOl9++677wgJCWH69OmPEyQAZ2dngoKC8nAEBQ4fVybp7G+suuPG4EZ5S3yCgoJwdnZ+zagLt8KRJOnjOHkmlUYDhzOg48cY3o8h9ewafjmQRHGL4hilHOb7tkPYkQAZZ1YysksPpp7ISot09/9mweLDJOZHL4yBM+6etXEq/ezjrokBAVyLOsTUPl0YsjyYpNuXuaF5zfPIS9Bs7BKW+y5n5bL+pM+bwA5Td9rZH2dc1458uewccWlpTzU0GZw7m0H9z6pgrCzNF78uZ2A5OcpKjfjUwQjTKk7YpiSQlO0zU1KpUXPKGpriVNGWjMQznNO44VnNBEUJNzxrxXDmYmb296ysREN3e0zLlKFU+Tq4lTOmuL0d6rRUFPYviu8RPVE7R/LVrob8PO9zbA3scW9nz/FxXen45TLOxaWR9kylDM6d1+DmWQ0TRQncPGsRc+Yimpzxp6SJjjlBeAvkajUqQFHCHvXFdSycv4Tdt3ToSvTklzkez3b7P0VVvgKmlwO5pgH09wg8F499ORsMDBQkxyegAzS3bhKuB5Ahk+mR3jhjKrwmTpyYLTl6xMXFhVOnTuXpGAbVq1HkwHkq9PfCJo8ZQEBAAC4uLq8S6n9G4ehu0wQQkN6YwU5y7q++RLEG3TBkGyaVmtOzTyNU+hZk7OvBlfs6PkFBWbdS7B89g9Z7vamdr4HLUCgAlChkGtLT9KC6x704PUWsSlCyXkdWLm2FacJx1u6UUfG1/zcUFClug21JFbbmDaki20Xw+m/xD2rHttUdsPj7S1w36gE5MiQklFiYZ3I0PAOcIWjp9xy2t0UmV/J4JNUzDZEMufzJOCtJYYGl9gqh6eBslEBohBJra8VT7xmQKZA/fi3PlpEn+o9hdJBXjviySw7wofccA8b5D6WaIZDoz5jRQXhtW00Hi7/50nUjekAu46mGU4mVpZajWYGREBqB0toaZc74X+djFoQP3NGjRzExMaF8+fLP7FM4DmTVWg0alYqsIY/lXzp+VFlzKN6NutC90TFKF7lPfLnvWdnamKJhXlSbO4TmrcuiUqZjaNAIFPZUdTzN9y0mUubAZFL/2UNUVNQ7eJeFj4eHBz169GDq1Kk5Fh7PSUvI37f4eNZcuuSlnw3QaDRs2bKFAwcOvJ1gC5lCkSRpg09z80Ekh9b8TNgFNyZMdUSxXU/kjnF4XjVDm3CTO3bfs9dRAWdlqJyHM8doAqNnnOCvXu8vToWBAcrcfu8UpXFrmszALl9wuJiGML0co5Yj6LvuKz7vsB7z2DDM+q6gy+ve19OH4z+0CQGmcvTpGRRt6c0yl5Ps9dvAjxOPkhxxA+lBPEnKmjhK4xgxxZ1dX/di9VeedPIrwv2U+kyZrWTXq5xTWZ9h/fYwuE171pndJ9JuBKtqKlmXx+pFHJwwu/R0fK5kWx1Id5E5g2Zy1bAxi/p7sVBmicf3fXEyu8SGHydyNDmCG9ID4pOU1HSUGDfCm+odAJUa96/7sXZQG9qvM+N+pB0jVtVEuf1V3pwgCDmtXr2a9PR0ihYt+vxCjxOkPJJb09LnH5onx5IomWFh+vCS5NiH3y92IiYqnaIli2OgA4VCTo2tt+ijA4UC7lSpwoYNGyhRosSbvK3/hLp162JmZsbu3btp1apVrmU0J6fQZcp1SnsOZ1oP2zx3I/n7+1O1alUqV6789gIuRGSSVAhuXup16ACtRofaMOtPMHV9Z+oe6sbOiS7w4Aa/j/6Gi30O8KtuGE0DBnNkkpJpLUaSObQFJ34zZ+3OIdjmc+diemwkSUWssVBLyBUKQE9qfDyZxhaYvVLLkjfahAjC080obVMEXYaEUq1Ejh69Xo5cDuhSiEsEM3Pj1543RJ8aT2ymMVav8QZyj++llYgIT8estA1FdBlISjVKOej1+uzfoPSpxMdmYmxl9mqNdiFUrlw59u7dS7ly5fI7FEEQ8snGjRuZNm0aAQEBqNW5zAGTHs+9DFOszPJ+byQ5OZnq1auzcOFCPDw83mK0hUfhGJMkV6CQKx4nSI8oihTHxrYk9pVcaVhFRlho4pNxJkaujJ37CYcnLOeG7pkj5gtDC1usjOQPEyQAOUbF302CBKAsZkdZGxMUKFA9TkAeJkgACmPM3yBBApAbFX+tBOn58b20EnZlbTBRgEKlfnzn7plbzHIjin8ACZIgCAJAp06dcHBwYPLkybkXMCz+SgkSwJgxY2jYsOEHmyBBIeluy52ecP+hNAkwRa5PJ6NoS7xH2iM//KSEketYfuzyJy0+zNnUBUEQhA+ETCZjyZIluLq6Ym9vz6BBg97oeLNmzeLvv//O84Dw/6rC0d0mCMJzie42QRAeuXHjBu7u7gwdOpTRo0e/ZCD3s7RaLVOmTGH9+vUcPHgQOzu7dxRp4VA4utsEQRAEQXip8uXLc+jQIbZt20bTpk25evVqnusGBQVRv359jh8/zqFDhz74BAlEkiQIgiAI/ykODg4cPnwYT09PGjVqRKtWrdi0aRN3797NVk6SJMLCwli/fj2ffPIJLVq0oE+fPuzdu5eSJUvmU/QFi+huE4RCTnS3CYLwPOnp6WzYsIGNGzcSEBCAgYEBVlZWSJJEdHQ0MpmM2rVr061bN9q3b49KJR53eZpIkgShkIuOjsbCwgKlshA/hyEIwjsnSRLh4eHExcUhk8mwsLCgZMmSWetjCrkSSdJ/gF6nRZIpUeTaeapHp5WQKRWib1UQhIJBr0MryVDm3mgh2i2hoBC/f4WcLnQ5XmUaMyv4OZNB6W4w75MytFkSQgGZLkoQhA+ZLpTlXmVoPCuYjP3f0bBGC6adzLFwpWi3hAJCJEmFWiqH58zkUNXu9HR6zpSQivJ061GdYz4+HEh+v9EJgiDklHp4DjMPVaV7TyekB3e4cukad1NydGiIdksoIESSVJgl7mLZxru4tPkcWzno7+5jRq8WNHBxoa57Z/636Roa5Fh/1hrXmM2s+CsxvyMWBOGDlsiuZRu569KGzx+vE6Xj/mEferdoRKPPBvHriTj0ot0SCgiRJBViGQEHOJpYmhofWyEnid3eg5l2QE7jPr1owD/M+nIi21JAblGT6qUf8O/+D3vmVEEQ8llGAAeOJlK6xsdYPbr66MPZtTkYR49PKBmyhq/bf8MfsaLdEgoGkSQVYim3bxONNSVLKgFTWi26wJU/huEsxZKYIYeU+9xL0oPSFltruHc7JL9DFgThQ5Zym9vRYF2y5JM1sWRWeHkvY8KISfhOak3R6P3sOpEu2i2hQBBJUiGm0+kBBUoDQB/DtkE1qNx8NGvPa7AtVRwZkPVkpzzryTfxIKMgCPlJp0MPKJQGT7bJlKjUWSmTytQUI1kGGRkg2i2hIBBJUiFmWqoU5sRyL0YPmpNs3XoTw08nsmbx19QwTEVCympf9HHExoNF6dL5HbIgCB8y01KUMofYezHoH23TR7N3xWqC7l5l68aDRCudqVndQLRbQoEgkqRCTOXakNpFbnPxQhKo6tC2bTnSNvWkjJUrPuFWFJPucOuWFpIvciHEkFr1XfI7ZEEQPmQqVxrWLsLtixdIerRNUZbyKfNxs69M1/UZNJ/iw5flFKLdEgoEMZlkYaa/x/ouzowyXMLV1W0xRUPsrZskmjriaCEjJSkNipjB7j5U7JfIzEub6FFC5MWCIOQXPffWd8F5lCFLrq6mrZGG1AwJlZEa3b07xKpKUtIsazqTlO2i3RLyn/jNK8zkVrQbPQCb/X5si9EDKiwcK+NopQa5CmMzM4xVsexctw/Lft/SUTQ0giDkKzlW7UYzwGY/fttiQKHCyEiNElBblX6cIIFot4SCQdxJKvRSCDl1lrRy9alikUtjoo/jyvHrqKvXwdHk/UcnCIKQU0rIKc6mlaNhFYvcC4h2SyggRJIkCIIgCIKQC3EfUxAEQRAEIRciSRIEQRAEQciFSJIEQRAEQRByIZIkQRAEQRCEXIgkSRAEQRAEIRciSRIEQRAEQciFSJIEQRAEQRByIZIkQRAEQRCEXIgkSRAEQRAEIRciSRIEQRAEQciFSJIEQRAEQRBy8X/sCWDSAxPV5wAAAABJRU5ErkJggg=="
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FilterNet basics\n",
    "- FilterNet models are composed primarily of a stack of parameterized modules, which we will refer to here as FilterNet layer modules (FLMs)\n",
    "- They are also coverage-preserving; that is, even though the input and output of an FLM may differ in sequence length due to a stride ratio, the time period that the input and output cover will be identical\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "## Layers\n",
    "1. (A) Full-Resolution CNN (s=1, t=cnn). High-resolution processing. Convolves CNN filters against the input signal without striding or pooling, in order to extract information at the finest available temporal resolution. This layer is computationally expensive because it is applied to the full resolution input signal (s is a stride ratio)\n",
    "2. (B) Pooling Stack 1 (s>1, t=cnn). Downsamples from the input to the output frequency. \n",
    "3. (C) Pooling Stack 2 (s>1, t=cnn). Downsamples beyond the overall output frequency.\n",
    "4. (D) Resampling Step. Matches output lengths.\n",
    "5. (E) Bottleneck Layer. Reduces channel number\n",
    "6. (F) Recurrent Stack (s=1, t=lstm). Temporal modeling\n",
    "7. (G) Output Module (s=1, k=1, t=cnn). Provides predictions for each output time step\n",
    "\n",
    "## Dataset benchmark\n",
    "-  data processing steps employed by Ordóñez and Roggen [28]\n",
    "- re-scale all data to have zero mean and unit standard deviation according to the statistics of the training set\n",
    "\n",
    "## Performace Metrics\n",
    "- Sample-based metrics are aggregated across all class predictions, and are not affected by the order of the predictions.\n",
    "- Event-based metrics are calculated after the output is segmented into discrete events, and they are strongly affected by the order of the predictions\n",
    "- F1 score for each output class\n",
    "- mean F1\n",
    "- weighted F1\n",
    "\n",
    "## Ensembling\n",
    "- m n-fold ensembling by (a) combining the training and validation sets  into a single contiguous set, (b) dividing that set into n disjoint folds of contiguous samples, (c) training n independent models where the ith model uses the ith fold for validation and the remaining\n",
    "- n-1 folds for training, and (d) ensembling the n models together during inference by simply averaging their logit outputs before the softmax function is applied.\n",
    "\n",
    "## Best Performance:\n",
    "1. 4-fold ms-C/L n=10, stride ratio = 8, params k = 1,371\n",
    "2. ms-C/L\n",
    "\n",
    "## Result\n",
    "- simple FilterNet architerture: p-CNN with the largset output:input stride ratio that can fully resolve the shortest events of interest\n",
    "\n",
    "## References:\n",
    "- sussexwearlab sussexwearlab/DeepConvLSTM Available online: https://github.com/sussexwearlab/DeepConvLSTM (accessed on Dec 9, 2019).\n",
    "- Ordóñez, F.J.; Roggen, D. Deep Convolutional and LSTM Recurrent Neural Networks for Multimodal Wearable Activity Recognition. Sensors 2016, 16, 115\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MikhailPetrovBrainer\\Documents\\Private\\SoSe22\\Projektseminar\\notebook\n",
      "C:\\Users\\MikhailPetrovBrainer\\Documents\\Private\\SoSe22\\Projektseminar\\notebook\\..\\data\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "print(os.path.join(os.getcwd()+\"\\..\\data\"))\n",
    "print(os.path.exists(os.path.join(os.getcwd()+\"\\..\\data1\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKALIEREN VS NORMALISIEREN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DataLoader object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MikhailPetrovBrainer\\Documents\\Private\\SoSe22\\Projektseminar\\notebook\\..\\data\\q4_2017.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "DIR = os.path.join(os.getcwd()+\"\\..\\data\")\n",
    "if not os.path.exists(DIR):\n",
    "    print(\"PROVIDE PATH TO FILE MANUALLY\")\n",
    "FILE_NAME = \"q4_2017.xlsx\" #rewrite: read file name from settings.py\n",
    "PATH = os.path.join(DIR, FILE_NAME)\n",
    "print(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "'''\n",
    "global variables for reading the dataset\n",
    "'''\n",
    "DIR = os.path.join(os.getcwd()+\"\\..\\data\")\n",
    "if not os.path.exists(DIR):\n",
    "    print(\"PROVIDE PATH TO FILE MANUALLY\")\n",
    "\n",
    "FILE_NAME = \"q4_2017.xlsx\" #rewrite: read file name from settings.py\n",
    "PATH = os.path.join(DIR, FILE_NAME)\n",
    "\n",
    "class BillingDataset(Dataset):\n",
    "    '''\n",
    "    create dataset from excel table based on the product number\n",
    "    PS: products' names are converted into numbers from 0 to 9\n",
    "    '''    \n",
    "    def __init__(self, product=None, forecast=False):\n",
    "        if product is None:\n",
    "            self.PROD = 0 # Rewrite to cover all products\n",
    "        else:\n",
    "            self.PROD = product\n",
    "            \n",
    "        self.forecast = forecast\n",
    "        '''\n",
    "        get dataset (excel-table) from folder\n",
    "        '''\n",
    "        df = pd.read_excel(PATH, index_col=None, header=1)\n",
    "        df_ = self._df_perparation(df)\n",
    "        \n",
    "        #CHANGE TO COVER ALL PRODUCTS\n",
    "        x_, y_ = self._df_transformation(df_)\n",
    "        self.x = torch.from_numpy(x_.values.astype(np.double)).double()\n",
    "        self.y = torch.from_numpy(y_.values.astype(np.double)).double()\n",
    "        self.n_samples = x_.values.shape[0]\n",
    "        self.n_features = x_.values.shape[1]\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "        \n",
    "    def _df_perparation(self, df_):\n",
    "        '''\n",
    "        rename columns, create a column with date in iso format, create unique indexes from products' names\n",
    "        '''\n",
    "        df = df_.copy()\n",
    "        df.drop(labels=['Unnamed: 0'], axis=1, inplace=True)\n",
    "        df['Billing'].loc[df['Billing'].isna()] = 0\n",
    "        df['Fc_and_order'].loc[df['Fc_and_order'].isna()] = 0\n",
    "        products = df['Sp_number'].unique()\n",
    "        prod2idx = {}\n",
    "        idx2prod = {}\n",
    "        for idx, prod in enumerate(products):\n",
    "            if prod not in prod2idx:\n",
    "                prod2idx[prod] = idx\n",
    "                idx2prod[idx] = prod\n",
    "\n",
    "        #Add column with integer product names\n",
    "\n",
    "        products_int = []\n",
    "        for idx, row in df['Sp_number'].iteritems():\n",
    "            products_int.append(prod2idx[row])\n",
    "\n",
    "        df['products'] = products_int\n",
    "        \n",
    "        mapper = {\n",
    "            'products': 'product',\n",
    "            'Fc_horizon': 'horizon',\n",
    "            'Fc_and_order': 'forecast',\n",
    "            'Billing': 'billing',\n",
    "            \"Due_date\": \"ddate\",\n",
    "            \"Fc_date\": \"fdate\"\n",
    "        }\n",
    "        df.rename(columns=mapper, inplace=True)\n",
    "        df['isodate'] = df[['ddate']].apply(lambda x: dt.datetime.strptime(str(x['ddate'])+'-1',\"%Y%W-%w\"), axis=1)\n",
    "        return df\n",
    "    \n",
    "    def _df_transformation(self, df_):\n",
    "        df = df_.copy()\n",
    "        if self.forecast is False:\n",
    "            idxs = df.loc[df['billing'] == 0].index\n",
    "        else:\n",
    "            idxs = df.loc[df['billing'] != 0].index\n",
    "        \n",
    "        df.drop(idxs, inplace=True)\n",
    "        \n",
    "        \n",
    "        # CHANGE TO COVER ALL PRODUCTS\n",
    "        # collect billings\n",
    "        df_b = df[['isodate', 'billing']].loc[df['product'] == self.PROD].drop_duplicates(['isodate'])\n",
    "        df_b.set_index(['isodate'], inplace=True)\n",
    "        \n",
    "        # collect forecast\n",
    "        df1 = df.loc[df['product'] == self.PROD].copy()\n",
    "        \n",
    "        hors = df1.horizon.unique()\n",
    "        dates = df1.isodate.unique()\n",
    "        data = {}\n",
    "\n",
    "        for date in dates:\n",
    "            for h in hors:\n",
    "                val = df1.forecast.loc[(df1.horizon == h) & (df1.isodate == date)].values.tolist()\n",
    "                if not val:\n",
    "                    val = [0]\n",
    "                if h not in data:\n",
    "                    data[h] = []\n",
    "                data[h].append(val[0])\n",
    "                \n",
    "        df_ = pd.DataFrame(data, columns=data.keys(), index=dates)\n",
    "        \n",
    "        hors = df1.horizon.unique()\n",
    "        dates = df1.isodate.unique()\n",
    "        data = {}\n",
    "\n",
    "        means = df_.T.mean()\n",
    "\n",
    "        for date in dates:\n",
    "            mean = means[date]\n",
    "            for h in hors:\n",
    "                val = df1.forecast.loc[(df1.horizon == h) & (df1.isodate == date)].values.tolist()\n",
    "                if not val:\n",
    "                    val = [mean]\n",
    "                if h not in data:\n",
    "                    data[h] = []\n",
    "                data[h].append(val[0])\n",
    "                \n",
    "        df_ = pd.DataFrame(data, columns=data.keys(), index=dates)\n",
    "        return df_, df_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Net architecture Programming\n",
    "- 208 time series length (T) - 208 weeks\n",
    "- 12 size of a Window (W) - quarter\n",
    "- 6&3 size of intermediate time series (t)\n",
    "- 6 Filters (F)\n",
    "- 2 Depth\n",
    "- 13 features\n",
    "- 1 output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# from fastai.core import ifnone, listify\n",
    "# from fastai.layers import bn_drop_lin, embedding, Flatten\n",
    "\n",
    "\n",
    "def conv_layer(window, ks=3, dilation=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv1d(1,1, kernel_size=ks, bias=False, dilation=dilation),\n",
    "        nn.AdaptiveAvgPool1d(window),\n",
    "        nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "    )\n",
    "\n",
    "class FilterNet(nn.Module):\n",
    "#     def __init__(self, emb_size, n_cont, out_sz, layers, emb_drop=0., window=12, filters=[1,2,3,4,5,6], y_range=None, ise_bn=False, ps=None, bn_final=False):\n",
    "    def __init__(self, out_sz=4, emb_drop=0., window=12, filters=[1,2,3,4,5,6], y_range=None, ise_bn=False, ps=None, bn_final=False):\n",
    "        super().__init__()\n",
    "        self.c1a = conv_layer(window=window // 2, ks=1, dilation=1)\n",
    "        self.c1b = conv_layer(window=window // 4, ks=1, dilation=2)\n",
    "        self.c2a = conv_layer(window=window // 2, ks=2, dilation=1)\n",
    "        self.c2b = conv_layer(window=window // 4, ks=2, dilation=2)\n",
    "        self.c3a = conv_layer(window=window // 2, ks=3, dilation=1)\n",
    "        self.c3b = conv_layer(window=window // 4, ks=3, dilation=2)\n",
    "        self.c4a = conv_layer(window=window // 2, ks=4, dilation=1)\n",
    "        self.c4b = conv_layer(window=window // 4, ks=4, dilation=2)\n",
    "        self.c5a = conv_layer(window=window // 2, ks=5, dilation=1)\n",
    "        self.c5b = conv_layer(window=window // 4, ks=5, dilation=2)\n",
    "        self.c6a = conv_layer(window=window // 2, ks=6, dilation=1)\n",
    "        self.c6b = conv_layer(window=window // 4, ks=6, dilation=2)\n",
    "        \n",
    "        num_wave_outputs = (len(filters) * (window // 2)) + (len(filters) * (window // 4))\n",
    "        \n",
    "#         # Fastai's Mixed Input model\n",
    "#         ps = ifnone(ps, [0]*len(layers))\n",
    "#         ps = listify(ps, layers)\n",
    "#         self.embeds = nn.ModuleList([embedding(ni, nf) for ni,nf in emb_szs])\n",
    "#         self.emb_drop = nn.Dropout(emb_drop)\n",
    "#         self.bn_cont = nn.BatchNorm1d(n_cont)\n",
    "#         n_emb = sum(e.embedding_dim for e in self.embeds)\n",
    "#         self.n_emb,self.n_cont,self.y_range = n_emb,n_cont,y_range\n",
    "#         sizes = self.get_sizes(layers, out_sz)\n",
    "#         actns = [nn.ReLU(inplace=True)] * (len(sizes)-2) + [None]\n",
    "#         layers = []\n",
    "#         for i,(n_in,n_out,dp,act) in enumerate(zip(sizes[:-2],sizes[1:-1],[0.]+ps,actns)):\n",
    "#             layers += bn_drop_lin(n_in, n_out, bn=use_bn and i!=0, p=dp, actn=act)\n",
    "#         if bn_final: layers.append(nn.BatchNorm1d(sizes[-1]))\n",
    "#         self.layers = nn.Sequential(*layers)\n",
    "\n",
    "        # Final layer\n",
    "#         self.f = Flatten()\n",
    "        self.f = nn.Flatten()\n",
    "        self.lin = nn.Linear(num_wave_outputs, out_sz, bias=False)\n",
    "\n",
    "#         self.sizes = sizes\n",
    "        self.num_wave_outputs = num_wave_outputs\n",
    "\n",
    "#     def get_sizes(self, layers, out_sz):\n",
    "#         return [self.n_emb + self.n_cont] + layers + [out_sz]\n",
    "    \n",
    "#     def forward(self, x_window, x_cat, x_cont):\n",
    "    def forward(self, x_window, x_cat, x_cont):\n",
    "        # TODO: Use the filters arg to generate the conv_layers dynamically\n",
    "        # Wavenet model\n",
    "        self.f1a = self.c1a(x_window)\n",
    "        self.f1b = self.c1b(self.f1a)\n",
    "        self.f2a = self.c2a(x_window)\n",
    "        self.f2b = self.c2b(self.f2a)\n",
    "        self.f3a = self.c3a(x_window)\n",
    "        self.f3b = self.c3b(self.f3a)\n",
    "        self.f4a = self.c4a(x_window)\n",
    "        self.f4b = self.c4b(self.f4a)\n",
    "        self.f5a = self.c5a(x_window)\n",
    "        self.f5b = self.c5b(self.f5a)\n",
    "        self.f6a = self.c6a(x_window)\n",
    "        self.f6b = self.c6b(self.f6a)\n",
    "        self.ffc = torch.cat([self.f1a, self.f1b, self.f2a, self.f2b,\n",
    "                              self.f3a, self.f3b, self.f4a, self.f4b,\n",
    "                              self.f5a, self.f5b, self.f6a, self.f6b, ], 2)\n",
    "\n",
    "#         # Fastai's Mixed Input Model\n",
    "#         if self.n_emb != 0:\n",
    "#             x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]\n",
    "#             x = torch.cat(x, 1)\n",
    "#             x = self.emb_drop(x)\n",
    "#         if self.n_cont != 0:\n",
    "#             x_cont = self.bn_cont(x_cont)\n",
    "#             x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont\n",
    "# #         x = self.layers(x)\n",
    "#         if self.y_range is not None:\n",
    "#             x = (self.y_range[1]-self.y_range[0]) * torch.sigmoid(x) + self.y_range[0]\n",
    "\n",
    "        # Combine results from both nets\n",
    "        x = x.unsqueeze(1)\n",
    "        self.fc = torch.cat([self.ffc, x], 2)\n",
    "        self.flin = self.lin(self.f(self.fc))\n",
    "        return self.flin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFilterNet(nn.Module):\n",
    "    def __init__(self, window=24, ks=3, batch_size=4):\n",
    "        super(MyFilterNet, self).__init__()\n",
    "        self.conv1a = nn.Conv1d(4, 1, kernel_size=1, bias=False, dilation=1)\n",
    "        self.pool1a = nn.AdaptiveAvgPool1d(6)\n",
    "        self.relu1a = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "        \n",
    "        self.conv1b = nn.Conv1d(1, 1, kernel_size=1, bias=False, dilation=2)\n",
    "        self.pool1b = nn.AdaptiveAvgPool1d(2)\n",
    "        self.relu1b = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "        \n",
    "        self.f = nn.Flatten()\n",
    "        self.lin = nn.Linear(2, 4)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1a(x)\n",
    "        x = self.relu1a(x)\n",
    "        x = self.pool1a(x)\n",
    "        x = self.conv1b(x)\n",
    "        x = self.relu1b(x)\n",
    "        x = self.pool1b(x)\n",
    "        x = self.f(x)\n",
    "        x = self.lin(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-70-7c60b5430961>:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Billing'].loc[df['Billing'].isna()] = 0\n",
      "<ipython-input-70-7c60b5430961>:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Fc_and_order'].loc[df['Fc_and_order'].isna()] = 0\n",
      "<ipython-input-70-7c60b5430961>:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Billing'].loc[df['Billing'].isna()] = 0\n",
      "<ipython-input-70-7c60b5430961>:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Fc_and_order'].loc[df['Fc_and_order'].isna()] = 0\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "batch_size = 4\n",
    "learning_rate = 0.01\n",
    "\n",
    "train_set = BillingDataset()\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_set = BillingDataset(forecast=True)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of MyFilterNet(\n",
       "  (conv1a): Conv1d(4, 1, kernel_size=(1,), stride=(1,), bias=False)\n",
       "  (pool1a): AdaptiveAvgPool1d(output_size=6)\n",
       "  (relu1a): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  (conv1b): Conv1d(1, 1, kernel_size=(1,), stride=(1,), dilation=(2,), bias=False)\n",
       "  (pool1b): AdaptiveAvgPool1d(output_size=2)\n",
       "  (relu1b): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  (f): Flatten(start_dim=1, end_dim=-1)\n",
       "  (lin): Linear(in_features=2, out_features=4, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 51766847874.93222\n",
      "loss 9.684327893077888e+60\n",
      "loss inf\n",
      "loss inf\n",
      "loss inf\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [1/30], Step [10/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [1/30], Step [20/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [1/30], Step [30/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [1/30], Step [40/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [1/30], Step [50/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [2/30], Step [10/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [2/30], Step [20/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [2/30], Step [30/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [2/30], Step [40/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [2/30], Step [50/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [3/30], Step [10/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [3/30], Step [20/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [3/30], Step [30/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [3/30], Step [40/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [3/30], Step [50/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [4/30], Step [10/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [4/30], Step [20/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [4/30], Step [30/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [4/30], Step [40/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [4/30], Step [50/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [5/30], Step [10/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [5/30], Step [20/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [5/30], Step [30/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [5/30], Step [40/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [5/30], Step [50/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [6/30], Step [10/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [6/30], Step [20/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [6/30], Step [30/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [6/30], Step [40/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [6/30], Step [50/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [7/30], Step [10/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [7/30], Step [20/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [7/30], Step [30/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [7/30], Step [40/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [7/30], Step [50/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [8/30], Step [10/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [8/30], Step [20/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [8/30], Step [30/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [8/30], Step [40/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [8/30], Step [50/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [9/30], Step [10/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [9/30], Step [20/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [9/30], Step [30/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [9/30], Step [40/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [9/30], Step [50/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [10/30], Step [10/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [10/30], Step [20/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [10/30], Step [30/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [10/30], Step [40/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [10/30], Step [50/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [11/30], Step [10/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [11/30], Step [20/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [11/30], Step [30/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [11/30], Step [40/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [11/30], Step [50/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [12/30], Step [10/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [12/30], Step [20/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [12/30], Step [30/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [12/30], Step [40/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [12/30], Step [50/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [13/30], Step [10/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [13/30], Step [20/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [13/30], Step [30/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [13/30], Step [40/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [13/30], Step [50/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [14/30], Step [10/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [14/30], Step [20/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [14/30], Step [30/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [14/30], Step [40/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [14/30], Step [50/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [15/30], Step [10/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [15/30], Step [20/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [15/30], Step [30/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [15/30], Step [40/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [15/30], Step [50/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [16/30], Step [10/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [16/30], Step [20/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [16/30], Step [30/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [16/30], Step [40/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [16/30], Step [50/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [17/30], Step [10/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [17/30], Step [20/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [17/30], Step [30/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [17/30], Step [40/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [17/30], Step [50/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [18/30], Step [10/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [18/30], Step [20/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [18/30], Step [30/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [18/30], Step [40/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [18/30], Step [50/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [19/30], Step [10/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [19/30], Step [20/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [19/30], Step [30/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [19/30], Step [40/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [19/30], Step [50/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [20/30], Step [10/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [20/30], Step [20/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [20/30], Step [30/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [20/30], Step [40/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [20/30], Step [50/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [21/30], Step [10/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [21/30], Step [20/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [21/30], Step [30/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [21/30], Step [40/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [21/30], Step [50/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [22/30], Step [10/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [22/30], Step [20/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [22/30], Step [30/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [22/30], Step [40/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [22/30], Step [50/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [23/30], Step [10/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [23/30], Step [20/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [23/30], Step [30/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [23/30], Step [40/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [23/30], Step [50/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [24/30], Step [10/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [24/30], Step [20/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [24/30], Step [30/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [24/30], Step [40/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [24/30], Step [50/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [25/30], Step [10/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [25/30], Step [20/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [25/30], Step [30/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [25/30], Step [40/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [25/30], Step [50/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [26/30], Step [10/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [26/30], Step [20/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [26/30], Step [30/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [26/30], Step [40/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [26/30], Step [50/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [27/30], Step [10/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss nan\n",
      "Epoch [27/30], Step [20/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [27/30], Step [30/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [27/30], Step [40/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [27/30], Step [50/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [28/30], Step [10/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [28/30], Step [20/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [28/30], Step [30/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [28/30], Step [40/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [28/30], Step [50/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [29/30], Step [10/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [29/30], Step [20/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [29/30], Step [30/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [29/30], Step [40/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [29/30], Step [50/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [30/30], Step [10/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [30/30], Step [20/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [30/30], Step [30/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [30/30], Step [40/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "loss nan\n",
      "Epoch [30/30], Step [50/52], Loss: [nan]\n",
      "loss nan\n",
      "loss nan\n",
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "# model = FilterNet()\n",
    "model = MyFilterNet()\n",
    "model.double()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (forecast, billing) in enumerate(train_loader):\n",
    "        # forward pass\n",
    "        outputs = model(forecast)\n",
    "#         print(outputs.shape, outputs)\n",
    "        loss = criterion(outputs, billing.view(1,4))\n",
    "        print(f\"loss {loss}\")\n",
    "        # backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1)%10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: [{loss}]\")\n",
    "\n",
    "print(\"Finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 shape: torch.Size([1, 13]), val: tensor([[206215.0114, 189018.4989, 189699.1729, 193364.8533, 189788.6079,\n",
      "         187661.3384, 202973.4609, 208134.7445, 195521.5075, 260612.1153,\n",
      "         224875.3472, 211227.8624, 208243.1299]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[206215.0114, 189018.4989, 189699.1729, 193364.8533, 189788.6079,\n",
      "         187661.3384, 202973.4609, 208134.7445, 195521.5075, 260612.1153,\n",
      "         224875.3472, 211227.8624, 208243.1299]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[194977.5611, 190950.8780, 193474.4690, 202209.9043, 227002.9900,\n",
      "         214782.1132]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[187869.2857, 183989.4161, 186420.9966, 194837.9365, 218727.0623,\n",
      "         206951.7606]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[187869.2857, 183989.4161, 186420.9966, 194837.9365, 218727.0623,\n",
      "         206951.7606]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[186093.2328, 206838.9198]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[186093.2328, 206838.9198]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[ 24073.6395, 236293.9461,  32414.5671,  42271.1559]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 53266344229.09764, val: 53266344229.09764\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[170032.1248, 166437.5811, 167581.5044, 165671.0507, 167418.2578,\n",
      "         163692.9114, 171079.2939, 165576.6801, 155015.2526, 161248.8702,\n",
      "         211355.2501, 195352.4390, 181999.5985]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[170032.1248, 166437.5811, 167581.5044, 165671.0507, 167418.2578,\n",
      "         163692.9114, 171079.2939, 165576.6801, 155015.2526, 161248.8702,\n",
      "         211355.2501, 195352.4390, 181999.5985]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[168017.0701, 166890.2710, 167396.8210, 163890.4088, 175873.1243,\n",
      "         196235.7625]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[161891.7785, 160806.0627, 161294.1438, 157915.5760, 169461.3997,\n",
      "         189081.6129]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[161891.7785, 160806.0627, 161294.1438, 157915.5760, 169461.3997,\n",
      "         189081.6129]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[161330.6617, 172152.8629]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[161330.6617, 172152.8629]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[ 16647.8223, 200743.8739,  29428.5187,  36816.9973]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 54469820238.909836, val: 54469820238.909836\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[195350.4269, 177561.4968, 209588.2210, 201164.9163, 189312.6876,\n",
      "         187892.1193, 199119.7564, 214808.6520, 218142.0240, 181238.2408,\n",
      "         155609.7169, 127061.2498, 155211.6222]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[195350.4269, 177561.4968, 209588.2210, 201164.9163, 189312.6876,\n",
      "         187892.1193, 199119.7564, 214808.6520, 218142.0240, 181238.2408,\n",
      "         155609.7169, 127061.2498, 155211.6222]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[194166.7149, 200021.9416, 192108.1878, 210690.1441, 184996.6606,\n",
      "         145960.8630]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[187088.0031, 192729.7473, 185104.5301, 203008.9854, 178252.2904,\n",
      "         140639.7443]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[187088.0031, 192729.7473, 185104.5301, 203008.9854, 178252.2904,\n",
      "         140639.7443]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[188307.4268, 173967.0067]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[188307.4268, 173967.0067]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[  3532.1766, 218844.4702,  39346.7338,  43615.9727]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 44832961341.11349, val: 44832961341.11349\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[155539.9070, 145310.5267, 147094.6482, 153422.4572, 157541.6668,\n",
      "         152630.1149, 160942.4580, 144952.2464, 153208.2036, 162117.8447,\n",
      "         184021.3355, 155185.6795, 162165.8905]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[155539.9070, 145310.5267, 147094.6482, 153422.4572, 157541.6668,\n",
      "         152630.1149, 160942.4580, 144952.2464, 153208.2036, 162117.8447,\n",
      "         184021.3355, 155185.6795, 162165.8905]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[149315.0273, 152686.2574, 157038.0799, 153034.3027, 166449.1279,\n",
      "         167124.3018]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[143871.6152, 147119.9297, 151313.0841, 147455.2852, 160381.0038,\n",
      "         161031.5607]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[143871.6152, 147119.9297, 151313.0841, 147455.2852, 160381.0038,\n",
      "         161031.5607]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[147434.8763, 156289.2832]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[147434.8763, 156289.2832]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[ 14603.3732, 182859.4173,  27085.6942,  33670.5407]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 45095346065.44003, val: 45095346065.44003\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[173522.8619, 180731.8788, 193271.7412, 197956.8114, 196774.8701,\n",
      "         196770.2761, 196801.6309, 186357.2093, 232985.9604, 207184.4026,\n",
      "         282275.3162, 219729.4407, 182537.2923]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[173522.8619, 180731.8788, 193271.7412, 197956.8114, 196774.8701,\n",
      "         196770.2761, 196801.6309, 186357.2093, 232985.9604, 207184.4026,\n",
      "         282275.3162, 219729.4407, 182537.2923]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[182508.8273, 196001.1409, 196782.2591, 205381.6002, 240815.2264,\n",
      "         228180.6831]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[175855.1639, 188855.5456, 189608.1841, 197893.9918, 232035.7023,\n",
      "         219861.8165]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[175855.1639, 188855.5456, 189608.1841, 197893.9918, 232035.7023,\n",
      "         219861.8165]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[184772.9645, 216597.1702]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[184772.9645, 216597.1702]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[ 30520.1178, 241054.6743,  30104.7000,  41703.7868]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 52016016782.28703, val: 52016016782.28703\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[187279.3328, 175560.4359, 178058.0708, 174829.4311, 191778.1455,\n",
      "         189696.5786, 207807.6722, 222257.8653, 221346.2192, 216480.6176,\n",
      "         214565.0868, 169146.7945, 204779.7446]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[187279.3328, 175560.4359, 178058.0708, 174829.4311, 191778.1455,\n",
      "         189696.5786, 207807.6722, 222257.8653, 221346.2192, 216480.6176,\n",
      "         214565.0868, 169146.7945, 204779.7446]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[180299.2799, 181555.2158, 196427.4655, 217137.2522, 217463.9745,\n",
      "         196163.8753]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[173726.1769, 174936.3212, 189266.3263, 209221.0309, 209535.8409,\n",
      "         189012.3467]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[173726.1769, 174936.3212, 189266.3263, 209221.0309, 209535.8409,\n",
      "         189012.3467]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[179309.6082, 202589.7395]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[179309.6082, 202589.7395]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[ 25135.8468, 229567.3218,  30623.2816,  40651.8475]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 50564729865.41883, val: 50564729865.41883\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[257552.0638, 273097.6472, 236493.0698, 259470.1613, 243118.8063,\n",
      "         240667.9683, 249950.8605, 252701.3541, 271949.4823, 264189.1634,\n",
      "         282059.0351, 264641.4857, 264387.8327]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[257552.0638, 273097.6472, 236493.0698, 259470.1613, 243118.8063,\n",
      "         240667.9683, 249950.8605, 252701.3541, 271949.4823, 264189.1634,\n",
      "         282059.0351, 264641.4857, 264387.8327]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[255714.2602, 246360.6791, 244579.2117, 258200.5656, 272732.5603,\n",
      "         270362.7845]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[246391.5150, 237378.9669, 235662.4521, 248787.1693, 262789.3250,\n",
      "         260505.9517]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[246391.5150, 237378.9669, 235662.4521, 248787.1693, 262789.3250,\n",
      "         260505.9517]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[239810.9780, 257360.8153]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[239810.9780, 257360.8153]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[ 25608.9606, 299236.2394,  43472.9168,  54691.9912]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 42374059663.5918, val: 42374059663.5918\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[183764.2276, 181384.1558, 177310.9414, 195320.7023, 248570.5850,\n",
      "         259815.7557, 221483.6717, 255725.7236, 227552.9853, 224462.7620,\n",
      "         224908.6757, 208130.3532, 223710.0411]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[183764.2276, 181384.1558, 177310.9414, 195320.7023, 248570.5850,\n",
      "         259815.7557, 221483.6717, 255725.7236, 227552.9853, 224462.7620,\n",
      "         224908.6757, 208130.3532, 223710.0411]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[180819.7749, 207067.4096, 243290.0041, 234920.7936, 225641.4744,\n",
      "         218916.3567]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[174227.6947, 199518.3363, 234420.2493, 226356.1814, 217415.1877,\n",
      "         210935.2689]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[174227.6947, 199518.3363, 234420.2493, 226356.1814, 217415.1877,\n",
      "         210935.2689]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[202722.0934, 218235.5460]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[202722.0934, 218235.5460]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[ 22047.8118, 253345.3342,  36623.9191,  46217.2429]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 52649687311.7628, val: 52649687311.7628\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[210120.9961, 205429.9229, 206450.0625, 212955.8783, 212593.0774,\n",
      "         214903.3384, 210063.2967, 234238.2177, 273058.4465, 259806.6203,\n",
      "         249647.9759, 186418.8606, 203322.1504]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[210120.9961, 205429.9229, 206450.0625, 212955.8783, 212593.0774,\n",
      "         214903.3384, 210063.2967, 234238.2177, 273058.4465, 259806.6203,\n",
      "         249647.9759, 186418.8606, 203322.1504]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[207333.6605, 210666.3394, 212519.9042, 239119.9870, 260837.6809,\n",
      "         213129.6623]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[199774.8797, 202986.0486, 204772.0321, 230402.2716, 251328.1349,\n",
      "         205359.5583]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[199774.8797, 202986.0486, 204772.0321, 230402.2716, 251328.1349,\n",
      "         205359.5583]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[202510.9868, 229029.9883]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[202510.9868, 229029.9883]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[ 28521.8755, 259401.6883,  34543.6780,  45906.5050]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 39735082387.48416, val: 39735082387.48416\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[124664.8451, 124926.9679, 118441.6130, 121243.9095, 115085.3290,\n",
      "         112932.7478, 110769.2577, 114171.7467, 133501.2424, 125796.6695,\n",
      "         137335.1050, 170984.6877, 138140.3685]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[124664.8451, 124926.9679, 118441.6130, 121243.9095, 115085.3290,\n",
      "         112932.7478, 110769.2577, 114171.7467, 133501.2424, 125796.6695,\n",
      "         137335.1050, 170984.6877, 138140.3685]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[122677.8087, 118256.9505, 112929.1115, 119480.7489, 132211.0056,\n",
      "         148820.0537]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[118205.5939, 113945.9209, 108812.3358, 115125.0994, 127391.2089,\n",
      "         143394.6885]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[118205.5939, 113945.9209, 108812.3358, 115125.0994, 127391.2089,\n",
      "         143394.6885]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[113654.6169, 128636.9989]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[113654.6169, 128636.9989]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[ 16065.5119, 145639.8682,  19368.6465,  25761.6076]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 36840268073.15528, val: 36840268073.15528\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[169912.7912, 178477.7133, 188572.9598, 187293.9906, 173952.2412,\n",
      "         171977.8034, 173196.3720, 186350.5785, 214034.0664, 219233.7638,\n",
      "         229663.2611, 216492.9085, 220972.5579]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[169912.7912, 178477.7133, 188572.9598, 187293.9906, 173952.2412,\n",
      "         171977.8034, 173196.3720, 186350.5785, 214034.0664, 219233.7638,\n",
      "         229663.2611, 216492.9085, 220972.5579]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[178987.8214, 183273.0639, 173042.1389, 191193.6723, 220977.0304,\n",
      "         222376.2425]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[172462.5345, 176591.5363, 166733.6325, 184223.3581, 212920.8101,\n",
      "         214269.0067]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[172462.5345, 176591.5363, 166733.6325, 184223.3581, 212920.8101,\n",
      "         214269.0067]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[171929.2344, 203804.3916]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[171929.2344, 203804.3916]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[ 29732.6343, 225596.4356,  27592.8216,  38750.9996]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 46075027090.21009, val: 46075027090.21009\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[169644.7540, 173394.1943, 166933.1989, 158717.7503, 167962.1744,\n",
      "         163467.5118, 180976.0376, 187301.7302, 203655.0168, 212741.3321,\n",
      "         225378.9203, 219061.9252, 229800.2119]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[169644.7540, 173394.1943, 166933.1989, 158717.7503, 167962.1744,\n",
      "         163467.5118, 180976.0376, 187301.7302, 203655.0168, 212741.3321,\n",
      "         225378.9203, 219061.9252, 229800.2119]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[169990.7157, 164537.7079, 170801.9079, 190644.2615, 213925.0897,\n",
      "         224747.0191]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[163793.4647, 158539.2744, 164575.0807, 183693.9789, 206125.9844,\n",
      "         216553.3445]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[163793.4647, 158539.2744, 164575.0807, 183693.9789, 206125.9844,\n",
      "         216553.3445]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[162302.6066, 202124.4359]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[162302.6066, 202124.4359]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[ 33804.2566, 218545.1456,  24244.8346,  36349.3959]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 42363939254.43886, val: 42363939254.43886\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[190795.4548, 196349.2446, 195651.2892, 208820.2736, 214063.0274,\n",
      "         229470.4660, 220808.0981, 218310.3452, 220804.1370, 227512.3807,\n",
      "         258764.5050, 233052.5565, 243047.4257]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[190795.4548, 196349.2446, 195651.2892, 208820.2736, 214063.0274,\n",
      "         229470.4660, 220808.0981, 218310.3452, 220804.1370, 227512.3807,\n",
      "         258764.5050, 233052.5565, 243047.4257]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[194265.3295, 206178.1967, 221447.1972, 219974.1934, 235693.6742,\n",
      "         244954.8291]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[187183.0222, 198661.5443, 213373.8345, 211954.5368, 227100.8828,\n",
      "         236024.3744]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[187183.0222, 198661.5443, 213373.8345, 211954.5368, 227100.8828,\n",
      "         236024.3744]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[199739.4670, 225026.5980]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[199739.4670, 225026.5980]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[ 27619.3077, 255353.2931,  34231.9236,  45298.9429]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 38860412042.54762, val: 38860412042.54762\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[188506.5685, 164729.7353, 159698.0554, 159317.8589, 164081.2858,\n",
      "         169814.7618, 163386.2376, 179674.5077, 185247.3695, 181781.5997,\n",
      "         156754.0758, 137899.9019, 155678.8367]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[188506.5685, 164729.7353, 159698.0554, 159317.8589, 164081.2858,\n",
      "         169814.7618, 163386.2376, 179674.5077, 185247.3695, 181781.5997,\n",
      "         156754.0758, 137899.9019, 155678.8367]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[170978.1197, 161032.4001, 165760.7617, 176102.7049, 174594.3484,\n",
      "         150110.9382]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[164744.8678, 155161.7707, 159717.7355, 169682.6098, 168229.2482,\n",
      "         144638.5070]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[164744.8678, 155161.7707, 159717.7355, 169682.6098, 168229.2482,\n",
      "         144638.5070]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[159874.7914, 160850.1217]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[159874.7914, 160850.1217]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[ 10750.6658, 193341.8368,  30969.2905,  36717.0457]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 34642064797.0849, val: 34642064797.0849\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[133825.2314, 132131.9241, 154121.6136, 160965.7357, 161064.1304,\n",
      "         161633.3607, 166520.3239, 147905.6721, 152562.6406, 174517.6595,\n",
      "         157346.0922, 161574.4152, 168010.2366]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[133825.2314, 132131.9241, 154121.6136, 160965.7357, 161064.1304,\n",
      "         161633.3607, 166520.3239, 147905.6721, 152562.6406, 174517.6595,\n",
      "         157346.0922, 161574.4152, 168010.2366]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[140026.2564, 158717.1599, 163072.6050, 155662.8788, 161475.4641,\n",
      "         162310.2480]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[134921.5144, 152930.9445, 157127.5894, 149988.0231, 155588.6806,\n",
      "         156393.0281]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[134921.5144, 152930.9445, 157127.5894, 149988.0231, 155588.6806,\n",
      "         156393.0281]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[148326.6828, 153989.9106]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[148326.6828, 153989.9106]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[ 12779.0159, 182104.8726,  27850.7135,  33951.5184]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 22255212578.102226, val: 22255212578.102226\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[175442.1159, 175468.3634, 163899.1693, 154388.0736, 152883.4779,\n",
      "         162188.0744, 158867.6161, 154182.9761, 160899.0940, 159239.0722,\n",
      "         168887.9399, 134689.7855, 172256.3630]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[175442.1159, 175468.3634, 163899.1693, 154388.0736, 152883.4779,\n",
      "         162188.0744, 158867.6161, 154182.9761, 160899.0940, 159239.0722,\n",
      "         168887.9399, 134689.7855, 172256.3630]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[171603.2162, 157056.9069, 157979.7228, 157983.2287, 163008.7020,\n",
      "         158611.3628]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[165347.1732, 151331.2247, 152220.3945, 152223.7726, 157066.0164,\n",
      "         152829.0048]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[165347.1732, 151331.2247, 152220.3945, 152223.7726, 157066.0164,\n",
      "         152829.0048]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[156299.5975, 154039.5979]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[156299.5975, 154039.5979]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[  8615.9531, 187175.4967,  30872.1516,  35972.5294]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 49911171252.790924, val: 49911171252.790924\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[215939.9896, 193800.8528, 208956.5016, 195746.1076, 188703.0321,\n",
      "         179272.5021, 154612.8251, 156194.4841, 149986.7797, 217518.0192,\n",
      "         209958.3479, 282372.3666, 224070.1627]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[215939.9896, 193800.8528, 208956.5016, 195746.1076, 188703.0321,\n",
      "         179272.5021, 154612.8251, 156194.4841, 149986.7797, 217518.0192,\n",
      "         209958.3479, 282372.3666, 224070.1627]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[206232.4480, 197801.8805, 174196.1198, 153598.0297, 192487.7156,\n",
      "         238800.2924]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[198713.8176, 190590.6299, 167845.5391, 147998.4586, 185470.2203,\n",
      "         230094.2331]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[198713.8176, 190590.6299, 167845.5391, 147998.4586, 185470.2203,\n",
      "         230094.2331]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[185716.6622, 187854.3040]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[185716.6622, 187854.3040]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[ 13080.6845, 225169.4472,  35788.8984,  42627.9828]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 44499039504.30736, val: 44499039504.30736\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[271792.7081, 182030.7391, 178530.8828, 168821.7991, 192530.3317,\n",
      "         183403.1818, 195221.7855, 213348.8897, 205072.0076, 211556.0171,\n",
      "         156573.2945, 190341.0218, 168873.5427]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[271792.7081, 182030.7391, 178530.8828, 168821.7991, 192530.3317,\n",
      "         183403.1818, 195221.7855, 213348.8897, 205072.0076, 211556.0171,\n",
      "         156573.2945, 190341.0218, 168873.5427]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[210784.7767, 179961.0045, 190385.0996, 204547.5609, 191067.1064,\n",
      "         171929.2864]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[203100.1676, 173400.2352, 183444.2661, 197090.3618, 184101.4068,\n",
      "         165661.3547]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[203100.1676, 173400.2352, 183444.2661, 197090.3618, 184101.4068,\n",
      "         165661.3547]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[186648.2230, 182284.3744]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[186648.2230, 182284.3744]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[  9307.5105, 222564.6073,  37174.9983,  42996.9696]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 27799849284.778854, val: 27799849284.778854\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[202341.3445, 197701.5271, 200153.7062, 201277.4100, 202036.9490,\n",
      "         212050.3793, 200200.9644, 170676.3625, 158403.8551, 164149.8687,\n",
      "         175600.6420, 153516.5077, 170794.2100]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[202341.3445, 197701.5271, 200153.7062, 201277.4100, 202036.9490,\n",
      "         212050.3793, 200200.9644, 170676.3625, 158403.8551, 164149.8687,\n",
      "         175600.6420, 153516.5077, 170794.2100]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[200065.5259, 201156.0217, 204762.7642, 176427.0607, 166051.4553,\n",
      "         166637.1199]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[192771.7425, 193822.4786, 197297.7187, 169995.1395, 159997.8303,\n",
      "         160562.1415]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[192771.7425, 193822.4786, 197297.7187, 169995.1395, 159997.8303,\n",
      "         160562.1415]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[194630.6466, 163518.3705]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[194630.6466, 163518.3705]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[ -5951.9406, 216851.7585,  43686.2249,  45468.7052]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 41447270764.65433, val: 41447270764.65433\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[193746.3602, 206094.7577, 192948.4960, 204143.7511, 214504.8696,\n",
      "         205611.7436, 209979.8362, 190024.4179, 196906.1260, 189840.2840,\n",
      "         185004.6288, 172685.0153, 177025.7884]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[193746.3602, 206094.7577, 192948.4960, 204143.7511, 214504.8696,\n",
      "         205611.7436, 209979.8362, 190024.4179, 196906.1260, 189840.2840,\n",
      "         185004.6288, 172685.0153, 177025.7884]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[197596.5380, 203865.7056, 210032.1498, 198970.1267, 190583.6796,\n",
      "         178238.4775]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[190392.7742, 196433.3670, 202374.9817, 191716.2817, 183635.6058,\n",
      "         171740.5117]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[190392.7742, 196433.3670, 202374.9817, 191716.2817, 183635.6058,\n",
      "         171740.5117]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[196400.3743, 182364.1331]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[196400.3743, 182364.1331]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[  4226.6080, 228777.6780,  40867.1837,  45468.5374]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 36868375672.75495, val: 36868375672.75495\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[189954.7397, 212248.6204, 187300.0443, 188616.8252, 190339.2355,\n",
      "         194306.2437, 173653.0147, 178206.6864, 178753.1065, 135557.3796,\n",
      "         145718.1641, 130099.1927, 150587.9962]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[189954.7397, 212248.6204, 187300.0443, 188616.8252, 190339.2355,\n",
      "         194306.2437, 173653.0147, 178206.6864, 178753.1065, 135557.3796,\n",
      "         145718.1641, 130099.1927, 150587.9962]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[196501.1348, 188752.0350, 186099.4979, 176870.9359, 153342.8834,\n",
      "         142135.1177]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[189337.3096, 181870.7433, 179314.9181, 170422.8309, 147752.6150,\n",
      "         136953.4862]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[189337.3096, 181870.7433, 179314.9181, 170422.8309, 147752.6150,\n",
      "         136953.4862]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[183507.6570, 151709.6441]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[183507.6570, 151709.6441]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[ -7064.1566, 203046.0145,  41646.1047,  42928.9082]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 31986849926.20294, val: 31986849926.20294\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[180823.8248, 171439.5245, 187417.0867, 168876.6664, 183932.0542,\n",
      "         175085.1600, 162950.3570, 185703.1440, 156729.1726, 185178.2744,\n",
      "         189956.2204, 178869.5571, 176673.5526]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[180823.8248, 171439.5245, 187417.0867, 168876.6664, 183932.0542,\n",
      "         175085.1600, 162950.3570, 185703.1440, 156729.1726, 185178.2744,\n",
      "         189956.2204, 178869.5571, 176673.5526]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[179893.4787, 180075.2691, 173989.1904, 168460.8912, 177287.8891,\n",
      "         181833.1101]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[173335.1713, 173510.3337, 167646.1544, 162319.4178, 170824.5820,\n",
      "         175204.0834]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[173335.1713, 173510.3337, 167646.1544, 162319.4178, 170824.5820,\n",
      "         175204.0834]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[171497.2198, 169449.3611]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[171497.2198, 169449.3611]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[  9708.3509, 205623.0129,  33793.9103,  39459.9879]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 31665120790.221306, val: 31665120790.221306\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[146207.2903, 161357.0456, 157263.9974, 144042.1795, 146366.9963,\n",
      "         144885.5833, 147113.8894, 132074.4109, 138057.5649, 141617.8439,\n",
      "         136738.8917, 130232.2790, 143300.6398]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[146207.2903, 161357.0456, 157263.9974, 144042.1795, 146366.9963,\n",
      "         144885.5833, 147113.8894, 132074.4109, 138057.5649, 141617.8439,\n",
      "         136738.8917, 130232.2790, 143300.6398]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[154942.7777, 149224.3911, 146122.1564, 139081.9551, 138804.7669,\n",
      "         136757.2702]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[149294.1770, 143784.2836, 140795.1569, 134011.6425, 133744.5606,\n",
      "         131771.7160]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[149294.1770, 143784.2836, 140795.1569, 134011.6425, 133744.5606,\n",
      "         131771.7160]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[144624.5392, 133175.9730]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[144624.5392, 133175.9730]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[  2456.3878, 167828.2676,  30299.8734,  33508.4564]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 35423754719.83015, val: 35423754719.83015\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[168232.5909, 153269.0635, 150582.6737, 161164.0353, 148836.1531,\n",
      "         148478.5797, 161237.0888, 156479.2028, 154103.8491, 165034.1709,\n",
      "         148354.5239, 134782.9058, 166446.7854]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[168232.5909, 153269.0635, 150582.6737, 161164.0353, 148836.1531,\n",
      "         148478.5797, 161237.0888, 156479.2028, 154103.8491, 165034.1709,\n",
      "         148354.5239, 134782.9058, 166446.7854]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[157361.4427, 153527.6207, 152850.6072, 157273.3802, 155830.8480,\n",
      "         149861.4050]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[151624.6570, 147930.6168, 147278.2873, 151539.8053, 150149.8680,\n",
      "         144398.0719]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[151624.6570, 147930.6168, 147278.2873, 151539.8053, 150149.8680,\n",
      "         144398.0719]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[148944.5204, 148695.9151]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[148944.5204, 148695.9151]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[  9333.4615, 179459.8607,  29066.4366,  34234.3600]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 35739115477.588974, val: 35739115477.588974\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[216241.0413, 218489.7113, 200648.2418, 198589.2145, 199577.3291,\n",
      "         197167.1660, 191691.4890, 186484.5240, 194537.5175, 204664.3719,\n",
      "         172424.7713, 187967.4216, 193360.1497]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[216241.0413, 218489.7113, 200648.2418, 198589.2145, 199577.3291,\n",
      "         197167.1660, 191691.4890, 186484.5240, 194537.5175, 204664.3719,\n",
      "         172424.7713, 187967.4216, 193360.1497]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[211792.9981, 199604.9284, 196145.3280, 190904.5102, 190542.2202,\n",
      "         184584.1142]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[204071.6292, 192327.9385, 188994.4756, 183944.7388, 183595.6581,\n",
      "         177854.7855]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[204071.6292, 192327.9385, 188994.4756, 183944.7388, 183595.6581,\n",
      "         177854.7855]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[195131.3478, 181798.3941]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[195131.3478, 181798.3941]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[  4560.4038, 227650.7241,  40489.6248,  45160.1499]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 33763387505.433746, val: 33763387505.433746\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[184692.8156, 187566.1786, 198504.2456, 200124.2211, 190345.2030,\n",
      "         189712.7247, 189822.0442, 166123.3748, 161037.9222, 121071.3386,\n",
      "         134591.0683, 122965.8970, 113833.3304]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[184692.8156, 187566.1786, 198504.2456, 200124.2211, 190345.2030,\n",
      "         189712.7247, 189822.0442, 166123.3748, 161037.9222, 121071.3386,\n",
      "         134591.0683, 122965.8970, 113833.3304]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[190254.4133, 196324.5566, 189959.9906, 172327.7804, 138900.1097,\n",
      "         123796.7653]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[183318.3446, 189167.1695, 183034.6567, 166045.3196, 133836.4272,\n",
      "         119283.7531]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[183318.3446, 189167.1695, 183034.6567, 166045.3196, 133836.4272,\n",
      "         119283.7531]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[185173.3902, 139721.8333]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[185173.3902, 139721.8333]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[-15006.5598, 197225.2637,  44500.3824,  43637.0213]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 36107811306.22142, val: 36107811306.22142\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[183001.9231, 177638.4801, 175629.6389, 182779.2844, 180513.8179,\n",
      "         182810.9228, 196079.8897, 179043.7398, 172284.5986, 185279.6809,\n",
      "         208382.9347, 186893.3810, 213371.8518]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[183001.9231, 177638.4801, 175629.6389, 182779.2844, 180513.8179,\n",
      "         182810.9228, 196079.8897, 179043.7398, 172284.5986, 185279.6809,\n",
      "         208382.9347, 186893.3810, 213371.8518]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[178756.6807, 179640.9138, 186468.2101, 182469.4094, 188649.0714,\n",
      "         202882.7225]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[172239.8212, 173091.8150, 179670.1870, 175817.1832, 181771.5337,\n",
      "         195486.2236]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[172239.8212, 173091.8150, 179670.1870, 175817.1832, 181771.5337,\n",
      "         195486.2236]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[175000.6077, 184358.3135]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[175000.6077, 184358.3135]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[ 16654.6459, 216387.8262,  32363.2765,  39993.3342]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 45420139782.04784, val: 45420139782.04784\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[150389.4157, 146477.5887, 145378.8338, 140159.2585, 168606.7809,\n",
      "         159352.0807, 160414.4147, 140116.6133, 137519.9051, 143365.3537,\n",
      "         170166.4232, 146161.9904, 157721.9749]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[150389.4157, 146477.5887, 145378.8338, 140159.2585, 168606.7809,\n",
      "         159352.0807, 160414.4147, 140116.6133, 137519.9051, 143365.3537,\n",
      "         170166.4232, 146161.9904, 157721.9749]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[147415.2794, 151381.6244, 162791.0921, 146016.9777, 150350.5607,\n",
      "         158016.7962]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[142041.1325, 145862.8639, 156856.3406, 140693.8131, 144869.3929,\n",
      "         152256.1162]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[142041.1325, 145862.8639, 156856.3406, 140693.8131, 144869.3929,\n",
      "         152256.1162]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[148253.4456, 145939.7740]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[148253.4456, 145939.7740]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[  8072.1679, 177442.3652,  29314.4052,  34124.7499]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 30884246376.33792, val: 30884246376.33792\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[257859.3019, 252019.3694, 266384.7626, 238270.6173, 261228.2843,\n",
      "         262119.9307, 255595.6145, 253171.0781, 235254.5455, 262582.8162,\n",
      "         202690.0373, 230287.9822, 234927.7711]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[257859.3019, 252019.3694, 266384.7626, 238270.6173, 261228.2843,\n",
      "         262119.9307, 255595.6145, 253171.0781, 235254.5455, 262582.8162,\n",
      "         202690.0373, 230287.9822, 234927.7711]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[258754.4780, 255294.5547, 259647.9432, 248007.0793, 233509.1330,\n",
      "         222635.2635]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[249320.8859, 245987.1120, 250181.7752, 238965.3391, 224995.9903,\n",
      "         214518.5838]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[249320.8859, 245987.1120, 250181.7752, 238965.3391, 224995.9903,\n",
      "         214518.5838]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[248496.5910, 226159.9711]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[248496.5910, 226159.9711]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[  2649.7223, 286837.5796,  52555.3670,  57638.3581]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 51271647848.62655, val: 51271647848.62655\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[177474.8841, 194161.0316, 199672.8583, 212252.6274, 202784.9220,\n",
      "         197839.5146, 246030.1154, 196083.4678, 231085.8519, 232668.7944,\n",
      "         234119.7651, 183981.6443, 194862.7789]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[177474.8841, 194161.0316, 199672.8583, 212252.6274, 202784.9220,\n",
      "         197839.5146, 246030.1154, 196083.4678, 231085.8519, 232668.7944,\n",
      "         234119.7651, 183981.6443, 194862.7789]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[190436.2580, 204903.4693, 215551.5174, 224399.8117, 232624.8038,\n",
      "         204321.3961]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[183493.5593, 197433.2936, 207693.1122, 216218.7963, 224143.9038,\n",
      "         196872.4429]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[183493.5593, 197433.2936, 207693.1122, 216218.7963, 224143.9038,\n",
      "         196872.4429]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[196206.6550, 212411.7143]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[196206.6550, 212411.7143]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[ 22040.7819, 245885.3602,  35226.3275,  44703.4739]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 18586734424.62039, val: 18586734424.62039\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[222077.5978, 170152.7881, 153864.2229, 177670.1608, 175068.8410,\n",
      "         178195.9566, 186486.4080, 153039.6475, 178531.4746, 184092.7541,\n",
      "         164022.4859, 196230.9505, 196884.0434]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[222077.5978, 170152.7881, 153864.2229, 177670.1608, 175068.8410,\n",
      "         178195.9566, 186486.4080, 153039.6475, 178531.4746, 184092.7541,\n",
      "         164022.4859, 196230.9505, 196884.0434]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[182031.5363, 168867.7416, 179917.0686, 172685.8434, 175548.9049,\n",
      "         185712.4933]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[175395.2750, 162711.4344, 173357.9011, 166390.3276, 169149.0014,\n",
      "         178942.0237]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[175395.2750, 162711.4344, 173357.9011, 166390.3276, 169149.0014,\n",
      "         178942.0237]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[170488.2035, 171493.7842]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[170488.2035, 171493.7842]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[ 11444.0535, 206157.2373,  33031.5746,  39155.3565]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 25473092717.42283, val: 25473092717.42283\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[111105.0815, 142216.5874, 141597.0363, 142192.3148, 159922.5690,\n",
      "         170131.2274, 157670.1263, 176604.6388, 172991.8225, 162999.6514,\n",
      "         183493.8986, 141406.4005, 171675.3057]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[111105.0815, 142216.5874, 141597.0363, 142192.3148, 159922.5690,\n",
      "         170131.2274, 157670.1263, 176604.6388, 172991.8225, 162999.6514,\n",
      "         183493.8986, 141406.4005, 171675.3057]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[131639.5684, 147903.9734, 162574.6409, 169088.8625, 173161.7908,\n",
      "         165525.2016]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[126840.6064, 142512.0086, 156647.7812, 162924.4932, 166848.9219,\n",
      "         159490.7640]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[126840.6064, 142512.0086, 156647.7812, 162924.4932, 166848.9219,\n",
      "         159490.7640]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[142000.1321, 163088.0597]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[142000.1321, 163088.0597]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[ 21468.8057, 183321.0676,  23760.1746,  32130.1184]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 27753275876.91037, val: 27753275876.91037\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[128488.5753, 148000.3596, 151004.1881, 158449.7484, 172861.6686,\n",
      "         164875.5500, 183464.9010, 186080.6776, 220313.7643, 210702.2109,\n",
      "         221425.5919, 177852.2565, 253037.8546]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[128488.5753, 148000.3596, 151004.1881, 158449.7484, 172861.6686,\n",
      "         164875.5500, 183464.9010, 186080.6776, 220313.7643, 210702.2109,\n",
      "         221425.5919, 177852.2565, 253037.8546]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[142497.7077, 160771.8684, 173734.0398, 196619.7809, 217480.5224,\n",
      "         217438.5676]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[137302.8562, 154910.7380, 167400.3066, 189451.6299, 209551.7853,\n",
      "         209511.3603]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[137302.8562, 154910.7380, 167400.3066, 189451.6299, 209551.7853,\n",
      "         209511.3603]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[153204.6336, 202838.2585]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[153204.6336, 202838.2585]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[ 39008.9728, 213200.8457,  20654.2789,  34024.8410]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 41732212681.65314, val: 41732212681.65314\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[ 84630.6364,  78732.8410,  91179.7131, 100912.6748,  97620.1743,\n",
      "         110817.5623, 103917.5325, 113208.7158, 123142.3800, 143040.1936,\n",
      "         149389.6831, 147204.2998, 149434.0710]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[ 84630.6364,  78732.8410,  91179.7131, 100912.6748,  97620.1743,\n",
      "         110817.5623, 103917.5325, 113208.7158, 123142.3800, 143040.1936,\n",
      "         149389.6831, 147204.2998, 149434.0710]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[ 84847.7302,  96570.8541, 104118.4230, 113422.8761, 138524.0856,\n",
      "         148676.0180]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[ 81754.8061,  93050.5030, 100322.8864, 109288.0976, 133474.1130,\n",
      "         143255.9043]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[ 81754.8061,  93050.5030, 100322.8864, 109288.0976, 133474.1130,\n",
      "         143255.9043]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[ 91709.3985, 128672.7050]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[ 91709.3985, 128672.7050]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[ 27625.8890, 131782.0202,  11020.2698,  20194.7235]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 24392185818.555862, val: 24392185818.555862\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[120453.2147, 123547.3130, 134450.7583, 141788.5739, 135931.1018,\n",
      "         132652.6502, 125230.0683, 169178.7104, 177234.9281, 197105.7314,\n",
      "         207129.5151, 160785.5163, 196314.5305]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[120453.2147, 123547.3130, 134450.7583, 141788.5739, 135931.1018,\n",
      "         132652.6502, 125230.0683, 169178.7104, 177234.9281, 197105.7314,\n",
      "         207129.5151, 160785.5163, 196314.5305]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[126150.4287, 137390.1447, 131271.2734, 157214.5689, 193823.3915,\n",
      "         188076.5207]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[121551.6016, 132381.5157, 126485.7395, 151483.1383, 186757.1973,\n",
      "         181219.8583]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[121551.6016, 132381.5157, 126485.7395, 151483.1383, 186757.1973,\n",
      "         181219.8583]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[126806.2856, 173153.3980]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[126806.2856, 173153.3980]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[ 35391.3693, 179484.1329,  16119.8207,  28036.6341]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 54118335191.962715, val: 54118335191.962715\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[ 87501.1220,  78991.4262,  88402.3753,  85865.7521,  77791.7833,\n",
      "          77961.9755,  90535.5360, 117215.3379, 143362.7700, 155921.6043,\n",
      "         145941.9709, 137222.2858, 161798.4242]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[ 87501.1220,  78991.4262,  88402.3753,  85865.7521,  77791.7833,\n",
      "          77961.9755,  90535.5360, 117215.3379, 143362.7700, 155921.6043,\n",
      "         145941.9709, 137222.2858, 161798.4242]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[ 84964.9745,  84019.9702,  82096.4316, 117037.8813, 148408.7817,\n",
      "         148320.8936]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[ 81867.7757,  80957.2264,  79103.8204, 112771.2992, 142998.4115,\n",
      "         142913.7278]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[ 81867.7757,  80957.2264,  79103.8204, 112771.2992, 142998.4115,\n",
      "         142913.7278]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[ 80642.9408, 132894.4795]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[ 80642.9408, 132894.4795]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[ 35933.5339, 127204.4013,   6031.5049,  17287.3135]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 31174308411.924984, val: 31174308411.924984\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[216894.5553, 217529.5554, 220225.1022, 210812.4394, 212203.8229,\n",
      "         221870.1443, 211746.1778, 200694.0219, 200783.8247, 229537.5769,\n",
      "         142456.1212, 197775.2271, 127180.1870]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[216894.5553, 217529.5554, 220225.1022, 210812.4394, 212203.8229,\n",
      "         221870.1443, 211746.1778, 200694.0219, 200783.8247, 229537.5769,\n",
      "         142456.1212, 197775.2271, 127180.1870]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[218216.4043, 214413.7882, 215273.3817, 204408.0082, 190925.8410,\n",
      "         155803.8451]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[210260.8369, 206596.8648, 207425.1174, 196955.8971, 183965.2919,\n",
      "         150123.8497]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[210260.8369, 206596.8648, 207425.1174, 196955.8971, 183965.2919,\n",
      "         150123.8497]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[208094.2730, 177015.0129]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[208094.2730, 177015.0129]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[ -5075.5033, 233105.6412,  46303.3248,  48561.9480]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 46140485529.14172, val: 46140485529.14172\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[259082.5677, 254869.2914, 260315.5510, 256431.3199, 256072.3539,\n",
      "         258887.1444, 258587.2914, 279101.2862, 275435.0956, 212817.8667,\n",
      "         236065.5839, 196892.8925, 206129.9032]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[259082.5677, 254869.2914, 260315.5510, 256431.3199, 256072.3539,\n",
      "         258887.1444, 258587.2914, 279101.2862, 275435.0956, 212817.8667,\n",
      "         236065.5839, 196892.8925, 206129.9032]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[258089.1367, 257606.4083, 257848.9299, 271041.2244, 241439.5154,\n",
      "         213029.4599]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[248679.8031, 248214.6750, 248448.3543, 261159.6556, 232637.2297,\n",
      "         205263.0093]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[248679.8031, 248214.6750, 248448.3543, 261159.6556, 232637.2297,\n",
      "         205263.0093]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[248447.6108, 233019.9648]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[248447.6108, 233019.9648]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[  6719.2752, 290740.3320,  51265.7281,  57462.4852]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 44951170657.70768, val: 44951170657.70768\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[264151.2987, 254813.4955, 265050.4419, 266012.7322, 271965.0527,\n",
      "         272406.5669, 278915.2843, 303021.4836, 294174.5704, 215941.9674,\n",
      "         234964.0589, 227892.8937, 226046.8154]], dtype=torch.float64,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[264151.2987, 254813.4955, 265050.4419, 266012.7322, 271965.0527,\n",
      "         272406.5669, 278915.2843, 303021.4836, 294174.5704, 215941.9674,\n",
      "         234964.0589, 227892.8937, 226046.8154]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[261338.4120, 267676.0756, 274428.9680, 292037.1128, 248360.1989,\n",
      "         229634.5893]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[251810.6093, 257917.2007, 264423.8815, 281390.0305, 239305.5839,\n",
      "         221262.7131]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[251810.6093, 257917.2007, 264423.8815, 281390.0305, 239305.5839,\n",
      "         221262.7131]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[258050.5638, 247319.4425]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[258050.5638, 247319.4425]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[ 10099.0002, 305013.0654,  52266.5691,  59557.4047]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 34910553794.06803, val: 34910553794.06803\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[284590.0141, 287105.1107, 282333.1972, 284748.4533, 286064.3544,\n",
      "         284713.8134, 292866.5357, 292455.2134, 271244.4092, 214578.9622,\n",
      "         230066.2594, 250069.1311, 204207.5495]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[284590.0141, 287105.1107, 282333.1972, 284748.4533, 286064.3544,\n",
      "         284713.8134, 292866.5357, 292455.2134, 271244.4092, 214578.9622,\n",
      "         230066.2594, 250069.1311, 204207.5495]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[284676.1073, 284382.0016, 287881.5678, 285522.0527, 238629.8769,\n",
      "         228114.3133]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[274297.4084, 274014.0259, 277385.9974, 275112.5106, 229930.0310,\n",
      "         219797.8666]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[274297.4084, 274014.0259, 277385.9974, 275112.5106, 229930.0310,\n",
      "         219797.8666]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[275232.4772, 241613.4694]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[275232.4772, 241613.4694]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[ -2299.1986, 312607.0606,  59854.9182,  64051.2600]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 46155203495.34411, val: 46155203495.34411\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[256629.2765, 251397.7110, 255732.3517, 260979.4699, 266105.3059,\n",
      "         263479.1425, 262416.6450, 279864.1619, 261341.1950, 189768.0162,\n",
      "         193005.0214, 194797.1961, 193963.1740]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[256629.2765, 251397.7110, 255732.3517, 260979.4699, 266105.3059,\n",
      "         263479.1425, 262416.6450, 279864.1619, 261341.1950, 189768.0162,\n",
      "         193005.0214, 194797.1961, 193963.1740]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[254586.4464, 260939.0425, 264000.3645, 267874.0006, 214704.7442,\n",
      "         193921.7971]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[245304.8214, 251425.8008, 254375.5065, 258107.9093, 206877.2125,\n",
      "         186852.0151]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[245304.8214, 251425.8008, 254375.5065, 258107.9093, 206877.2125,\n",
      "         186852.0151]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[250368.7096, 217279.0456]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[250368.7096, 217279.0456]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[ -3569.7707, 282928.9303,  54912.4511,  58324.7911]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 24962429157.76227, val: 24962429157.76227\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[131460.3031, 119367.4928, 126262.2303, 128174.2528, 131199.7664,\n",
      "         124723.6965, 135771.2458, 165122.3036, 176878.4972, 180082.6117,\n",
      "         183623.9148, 175351.9826, 210727.6409]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[131460.3031, 119367.4928, 126262.2303, 128174.2528, 131199.7664,\n",
      "         124723.6965, 135771.2458, 165122.3036, 176878.4972, 180082.6117,\n",
      "         183623.9148, 175351.9826, 210727.6409]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[125696.6754, 128545.4165, 130564.9029, 159257.3489, 180195.0079,\n",
      "         189901.1794]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[121114.3923, 123859.2678, 125805.1234, 153451.4382, 173625.7068,\n",
      "         182977.9897]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[121114.3923, 123859.2678, 125805.1234, 153451.4382, 173625.7068,\n",
      "         182977.9897]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[123592.9278, 170018.3782]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[123592.9278, 170018.3782]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[ 35233.0137, 175654.2659,  15479.2242,  27296.3170]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 65030304981.65454, val: 65030304981.65454\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[167275.4034, 181386.8624, 181735.3355, 167558.7314, 185863.2900,\n",
      "         190031.5789, 186688.4589, 197767.7809, 206139.5262, 211038.8589,\n",
      "         172550.5068, 201757.8913, 179963.2928]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[167275.4034, 181386.8624, 181735.3355, 167558.7314, 185863.2900,\n",
      "         190031.5789, 186688.4589, 197767.7809, 206139.5262, 211038.8589,\n",
      "         172550.5068, 201757.8913, 179963.2928]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[176799.2004, 178385.7856, 187527.7759, 196865.2553, 196576.2973,\n",
      "         184757.2303]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[170353.7110, 171882.4490, 180691.1209, 189688.1542, 189409.7317,\n",
      "         178021.5898]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[170353.7110, 171882.4490, 180691.1209, 189688.1542, 189409.7317,\n",
      "         178021.5898]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[174309.0936, 185706.4919]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[174309.0936, 185706.4919]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[ 17812.9789, 216723.5937,  31850.6292,  39785.8210]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 36986567551.048874, val: 36986567551.048874\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[157224.5647, 154481.8737, 160869.2307, 158251.5364, 160655.8104,\n",
      "         177210.9071, 176572.8061, 127811.6396, 153774.6887, 167390.3270,\n",
      "         164513.6907, 172610.1536, 131515.3302]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[157224.5647, 154481.8737, 160869.2307, 158251.5364, 160655.8104,\n",
      "         177210.9071, 176572.8061, 127811.6396, 153774.6887, 167390.3270,\n",
      "         164513.6907, 172610.1536, 131515.3302]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[157525.2230, 159925.5258, 171479.8412, 152719.7115, 161892.9021,\n",
      "         156213.0581]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[151782.4659, 154095.2533, 165228.2964, 147152.1641, 155990.8987,\n",
      "         150518.1427]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[151782.4659, 154095.2533, 165228.2964, 147152.1641, 155990.8987,\n",
      "         150518.1427]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[157035.3385, 151220.4019]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[157035.3385, 151220.4019]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[  6567.2356, 186024.1727,  31674.1607,  36226.3094]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 47288828325.66461, val: 47288828325.66461\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[211152.3422, 259684.8745, 198530.1078, 203658.0145, 197946.6769,\n",
      "         205822.2949, 193794.9251, 188027.4319, 233887.7842, 199940.7187,\n",
      "         220419.3422, 238222.8800, 193666.9225]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[211152.3422, 259684.8745, 198530.1078, 203658.0145, 197946.6769,\n",
      "         205822.2949, 193794.9251, 188027.4319, 233887.7842, 199940.7187,\n",
      "         220419.3422, 238222.8800, 193666.9225]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[223122.4415, 200044.9331, 199187.9657, 205236.7137, 218082.6150,\n",
      "         217436.3816]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[214987.9992, 192751.9004, 191926.1782, 197754.3879, 210131.9256,\n",
      "         209509.2539]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[214987.9992, 192751.9004, 191926.1782, 197754.3879, 210131.9256,\n",
      "         209509.2539]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[199888.6926, 205798.5225]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[199888.6926, 205798.5225]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[ 16206.3627, 244421.7046,  37851.2275,  45794.9299]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 46522364280.84061, val: 46522364280.84061\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[205554.9996, 207239.6609, 223557.3878, 228840.3088, 240741.9309,\n",
      "         243457.7523, 241228.2888, 220263.7824, 251663.1837, 269570.4994,\n",
      "         221949.6088, 208159.0071, 217854.2612]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[205554.9996, 207239.6609, 223557.3878, 228840.3088, 240741.9309,\n",
      "         243457.7523, 241228.2888, 220263.7824, 251663.1837, 269570.4994,\n",
      "         221949.6088, 208159.0071, 217854.2612]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[212117.3495, 231046.5425, 241809.3240, 237718.4183, 247727.7640,\n",
      "         215987.6257]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[204384.1546, 222623.1862, 232993.5549, 229051.8044, 238696.2077,\n",
      "         208113.3199]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[204384.1546, 222623.1862, 232993.5549, 229051.8044, 238696.2077,\n",
      "         208113.3199]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[220000.2986, 225287.1106]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[220000.2986, 225287.1106]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[ 17119.2358, 268315.7829,  41885.1519,  50431.5529]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 47913010958.702484, val: 47913010958.702484\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[251738.1109, 255916.0406, 256258.9171, 263655.2240, 262348.7817,\n",
      "         277345.5046, 256560.9447, 248544.8465, 254389.5054, 276629.8558,\n",
      "         215360.2497, 201976.8250, 196461.6649]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[251738.1109, 255916.0406, 256258.9171, 263655.2240, 262348.7817,\n",
      "         277345.5046, 256560.9447, 248544.8465, 254389.5054, 276629.8558,\n",
      "         215360.2497, 201976.8250, 196461.6649]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[254637.6896, 260754.3076, 265418.4103, 253165.0989, 248793.2036,\n",
      "         204599.5799]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[245354.1962, 251247.8014, 255741.8502, 243935.2965, 239722.8012,\n",
      "         197140.4841]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[245354.1962, 251247.8014, 255741.8502, 243935.2965, 239722.8012,\n",
      "         197140.4841]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[250781.2826, 226932.8606]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[250781.2826, 226932.8606]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[  1903.9753, 288725.6341,  53280.6171,  58199.4162]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 40215028563.54945, val: 40215028563.54945\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[265985.1968, 273114.9422, 258884.7622, 260197.3932, 282406.0378,\n",
      "         287254.4885, 281543.6704, 282765.4841, 279149.3163, 302874.5406,\n",
      "         246850.4627, 277223.0209, 250669.2322]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[265985.1968, 273114.9422, 258884.7622, 260197.3932, 282406.0378,\n",
      "         287254.4885, 281543.6704, 282765.4841, 279149.3163, 302874.5406,\n",
      "         246850.4627, 277223.0209, 250669.2322]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[265994.9671, 267162.7311, 283734.7322, 281152.8236, 276291.4399,\n",
      "         258247.5719]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[256297.3856, 257422.5727, 273390.3560, 270902.5842, 266218.4473,\n",
      "         248832.4617]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[256297.3856, 257422.5727, 273390.3560, 270902.5842, 266218.4473,\n",
      "         248832.4617]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[262370.1048, 261984.4978]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[262370.1048, 261984.4978]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[ 16472.3706, 316154.1621,  51191.3585,  60303.5657]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 37591591807.08934, val: 37591591807.08934\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[199566.8260, 185804.4712, 198123.7824, 200775.8099, 214093.1298,\n",
      "         214505.4361, 197493.3662, 189102.1653, 219299.6635, 173139.9085,\n",
      "         181453.6271, 171172.5486, 180259.9563]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[199566.8260, 185804.4712, 198123.7824, 200775.8099, 214093.1298,\n",
      "         214505.4361, 197493.3662, 189102.1653, 219299.6635, 173139.9085,\n",
      "         181453.6271, 171172.5486, 180259.9563]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[194498.3599, 204330.9074, 208697.3107, 201965.0650, 191297.7330,\n",
      "         177628.7106]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[187407.5562, 196881.6074, 201088.8110, 194602.0241, 184323.6247,\n",
      "         171152.9771]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[187407.5562, 196881.6074, 201088.8110, 194602.0241, 184323.6247,\n",
      "         171152.9771]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[195125.9915, 183359.5420]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[195125.9915, 183359.5420]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[  5483.4785, 228542.5462,  40198.3391,  45121.5946]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 22077613249.225224, val: 22077613249.225224\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[131843.9450, 136545.0118, 137570.9800, 132655.2086, 126414.7379,\n",
      "         123914.2792, 125615.1853, 118154.8616,  87398.2292, 127723.9896,\n",
      "         187937.3219, 181241.9045, 144373.0955]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[131843.9450, 136545.0118, 137570.9800, 132655.2086, 126414.7379,\n",
      "         123914.2792, 125615.1853, 118154.8616,  87398.2292, 127723.9896,\n",
      "         187937.3219, 181241.9045, 144373.0955]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[135319.9789, 132213.6422, 125314.7341, 110389.4254, 134353.1802,\n",
      "         171184.1073]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[130386.8286, 127393.7493, 120746.3766, 106365.2470, 129455.2795,\n",
      "         164943.3450]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[130386.8286, 127393.7493, 120746.3766, 106365.2470, 129455.2795,\n",
      "         164943.3450]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[126175.6515, 133587.9572]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[126175.6515, 133587.9572]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[ 12400.1159, 156397.2949,  23210.7963,  28819.3920]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 37080463400.98492, val: 37080463400.98492\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[128628.1341, 137698.8154, 130377.6014, 126834.6376, 137181.4332,\n",
      "         146571.5762, 131363.0537, 133679.3509, 170246.7519, 179271.8588,\n",
      "         185385.4249, 176596.2706, 188849.9549]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[128628.1341, 137698.8154, 130377.6014, 126834.6376, 137181.4332,\n",
      "         146571.5762, 131363.0537, 133679.3509, 170246.7519, 179271.8588,\n",
      "         185385.4249, 176596.2706, 188849.9549]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[132234.8503, 131464.5574, 138372.0210, 145096.3855, 178301.3452,\n",
      "         183610.5502]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[127414.1842, 126671.9763, 133327.5927, 139806.7858, 171801.0873,\n",
      "         176916.7178]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[127414.1842, 126671.9763, 133327.5927, 139806.7858, 171801.0873,\n",
      "         176916.7178]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[129137.9177, 162841.5303]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[129137.9177, 162841.5303]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[ 28086.7476, 175045.5381,  18916.6992,  28873.7093]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 49089196549.84863, val: 49089196549.84863\n",
      "conv1 shape: torch.Size([1, 13]), val: tensor([[282869.3596, 290182.1489, 311395.2536, 321267.8002, 315106.6323,\n",
      "         318505.2367, 317151.5713, 300552.0815, 339531.9890, 339769.3529,\n",
      "         276816.6193, 247329.3189, 267816.8163]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "relu shape: torch.Size([1, 13]), val: tensor([[282869.3596, 290182.1489, 311395.2536, 321267.8002, 315106.6323,\n",
      "         318505.2367, 317151.5713, 300552.0815, 339531.9890, 339769.3529,\n",
      "         276816.6193, 247329.3189, 267816.8163]], dtype=torch.float64,\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "pool shape: torch.Size([1, 6]), val: tensor([[294815.5874, 315923.2287, 316921.1468, 319078.5472, 318705.9871,\n",
      "         263987.5848]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([1, 6]), val: tensor([[284067.2014, 304405.2547, 305366.7885, 307445.5299, 307086.5533,\n",
      "         254363.1928]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([1, 6]), val: tensor([[284067.2014, 304405.2547, 305366.7885, 307445.5299, 307086.5533,\n",
      "         254363.1928]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([1, 2]), val: tensor([[297946.4149, 289631.7587]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([1, 2]), val: tensor([[297946.4149, 289631.7587]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([1, 4]), val: tensor([[ 14062.8636, 354506.6667,  59592.0473,  68668.1563]],\n",
      "       dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 29103741213.703518, val: 29103741213.703518\n"
     ]
    }
   ],
   "source": [
    "conv1 = nn.Conv1d(4, 1, kernel_size=1, dilation=1)\n",
    "relu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "pool1 = nn.AdaptiveAvgPool1d(6)\n",
    "conv2 = nn.Conv1d(1, 1, kernel_size=1, dilation=2)\n",
    "pool2 = nn.AdaptiveAvgPool1d(2)\n",
    "flat = nn.Flatten()\n",
    "lin = nn.Linear(2,4)\n",
    "conv1.double()\n",
    "conv2.double()\n",
    "lin.double()\n",
    "criterion = nn.MSELoss()\n",
    "for i, (forecast, billing) in enumerate(train_loader):\n",
    "#     forecast = forecast.view(-1, 4, 13)\n",
    "#     print(i, forecast, forecast.shape)\n",
    "    x = conv1(forecast)\n",
    "    print(f'conv1 shape: {x.shape}, val: {x}')\n",
    "    x = relu(x)\n",
    "    print(f'relu shape: {x.shape}, val: {x}')\n",
    "    x = pool1(x)\n",
    "    print(f'pool shape: {x.shape}, val: {x}')\n",
    "    x = conv2(x)\n",
    "    print(f'conv2 shape: {x.shape}, val: {x}')\n",
    "    x = relu(x)\n",
    "    print(f'relu2 shape: {x.shape}, val: {x}')\n",
    "    x = pool2(x)\n",
    "    print(f'pool2 shape: {x.shape}, val: {x}')\n",
    "    x = flat(x)\n",
    "    print(f'flat shape: {x.shape}, val: {x}')\n",
    "    x = lin(x)\n",
    "    print(f'lin shape: {x.shape}, val: {x}')\n",
    "    loss = criterion(x, billing.view(1,4))\n",
    "    print(f'loss shape: {loss}, val: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([[301000.],\n",
      "        [224000.],\n",
      "        [280000.],\n",
      "        [316000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "1 tensor([[234000.],\n",
      "        [234000.],\n",
      "        [246000.],\n",
      "        [387000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "2 tensor([[223000.],\n",
      "        [223000.],\n",
      "        [341000.],\n",
      "        [244000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "3 tensor([[249000.],\n",
      "        [133000.],\n",
      "        [329000.],\n",
      "        [212000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "4 tensor([[245000.],\n",
      "        [293000.],\n",
      "        [302000.],\n",
      "        [334000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "5 tensor([[341000.],\n",
      "        [165000.],\n",
      "        [232000.],\n",
      "        [281000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "6 tensor([[332155.],\n",
      "        [332155.],\n",
      "        [ 93000.],\n",
      "        [323000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "7 tensor([[269000.],\n",
      "        [282193.],\n",
      "        [283000.],\n",
      "        [343000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "8 tensor([[230000.],\n",
      "        [266000.],\n",
      "        [330000.],\n",
      "        [222000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "9 tensor([[338000.],\n",
      "        [301000.],\n",
      "        [156000.],\n",
      "        [ 56000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "10 tensor([[226000.],\n",
      "        [284000.],\n",
      "        [278000.],\n",
      "        [321000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "11 tensor([[298000.],\n",
      "        [277000.],\n",
      "        [275000.],\n",
      "        [219000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "12 tensor([[205000.],\n",
      "        [272000.],\n",
      "        [215000.],\n",
      "        [347000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "13 tensor([[134000.],\n",
      "        [409000.],\n",
      "        [239000.],\n",
      "        [220000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "14 tensor([[212000.],\n",
      "        [156000.],\n",
      "        [114000.],\n",
      "        [237000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "15 tensor([[365000.],\n",
      "        [115000.],\n",
      "        [194000.],\n",
      "        [238000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "16 tensor([[223000.],\n",
      "        [174000.],\n",
      "        [207000.],\n",
      "        [362000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "17 tensor([[181000.],\n",
      "        [173000.],\n",
      "        [228000.],\n",
      "        [250000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "18 tensor([[335000.],\n",
      "        [104000.],\n",
      "        [144000.],\n",
      "        [209000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "19 tensor([[277000.],\n",
      "        [251000.],\n",
      "        [239000.],\n",
      "        [228000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "20 tensor([[228000.],\n",
      "        [181000.],\n",
      "        [280000.],\n",
      "        [167000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "21 tensor([[178000.],\n",
      "        [294000.],\n",
      "        [265000.],\n",
      "        [232000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "22 tensor([[284000.],\n",
      "        [268000.],\n",
      "        [259000.],\n",
      "        [ 43000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "23 tensor([[ 43000.],\n",
      "        [262000.],\n",
      "        [304000.],\n",
      "        [278000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "24 tensor([[181000.],\n",
      "        [250000.],\n",
      "        [249000.],\n",
      "        [290000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "25 tensor([[250000.],\n",
      "        [225000.],\n",
      "        [297000.],\n",
      "        [142000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "26 tensor([[289000.],\n",
      "        [152000.],\n",
      "        [296000.],\n",
      "        [224000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "27 tensor([[240000.],\n",
      "        [182000.],\n",
      "        [281000.],\n",
      "        [114000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "28 tensor([[282000.],\n",
      "        [153000.],\n",
      "        [252000.],\n",
      "        [321000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "29 tensor([[204000.],\n",
      "        [274000.],\n",
      "        [154000.],\n",
      "        [207000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "30 tensor([[193000.],\n",
      "        [211000.],\n",
      "        [158000.],\n",
      "        [270000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "31 tensor([[280000.],\n",
      "        [196000.],\n",
      "        [156000.],\n",
      "        [195000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "32 tensor([[324000.],\n",
      "        [316000.],\n",
      "        [262000.],\n",
      "        [164000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "33 tensor([[200000.],\n",
      "        [206000.],\n",
      "        [213000.],\n",
      "        [167000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "34 tensor([[368000.],\n",
      "        [237000.],\n",
      "        [320000.],\n",
      "        [129000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "35 tensor([[234000.],\n",
      "        [297000.],\n",
      "        [244000.],\n",
      "        [ 20000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "36 tensor([[237000.],\n",
      "        [240000.],\n",
      "        [265000.],\n",
      "        [328000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "37 tensor([[237000.],\n",
      "        [320758.],\n",
      "        [373000.],\n",
      "        [207000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "38 tensor([[217000.],\n",
      "        [260000.],\n",
      "        [295000.],\n",
      "        [249000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "39 tensor([[209000.],\n",
      "        [318000.],\n",
      "        [295000.],\n",
      "        [355000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "40 tensor([[167000.],\n",
      "        [223000.],\n",
      "        [194000.],\n",
      "        [277000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "41 tensor([[334000.],\n",
      "        [198000.],\n",
      "        [355000.],\n",
      "        [262000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "42 tensor([[219000.],\n",
      "        [246000.],\n",
      "        [289000.],\n",
      "        [241000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "43 tensor([[275000.],\n",
      "        [114000.],\n",
      "        [268000.],\n",
      "        [273000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "44 tensor([[295000.],\n",
      "        [183000.],\n",
      "        [265000.],\n",
      "        [276000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "45 tensor([[276000.],\n",
      "        [161000.],\n",
      "        [206000.],\n",
      "        [344000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "46 tensor([[221000.],\n",
      "        [253000.],\n",
      "        [262000.],\n",
      "        [319000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "47 tensor([[222000.],\n",
      "        [295000.],\n",
      "        [173000.],\n",
      "        [365000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "48 tensor([[282000.],\n",
      "        [315000.],\n",
      "        [ 97000.],\n",
      "        [ 79000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "49 tensor([[198000.],\n",
      "        [170000.],\n",
      "        [296000.],\n",
      "        [227000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "50 tensor([[282000.],\n",
      "        [330000.],\n",
      "        [243000.],\n",
      "        [269000.]], dtype=torch.float64) torch.Size([4, 1])\n",
      "51 tensor([[207000.],\n",
      "        [459000.],\n",
      "        [279997.],\n",
      "        [209000.]], dtype=torch.float64) torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "for i, (forecast, billing) in enumerate(train_loader):\n",
    "#     forecast = forecast.view(-1, 4, 13)\n",
    "    print(i, billing, billing.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "conv1d() received an invalid combination of arguments - got (DataLoader, Parameter, NoneType, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mDataLoader\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mNoneType\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[32;1mint\u001b[0m)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mDataLoader\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mNoneType\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[32;1mint\u001b[0m)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-276-5ebe68409ce4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-271-bfba4d135257>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1a\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu1a\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool1a\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    296\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[1;32m--> 298\u001b[1;33m         return F.conv1d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    299\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: conv1d() received an invalid combination of arguments - got (DataLoader, Parameter, NoneType, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mDataLoader\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mNoneType\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[32;1mint\u001b[0m)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mDataLoader\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mNoneType\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[32;1mint\u001b[0m)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for forecast, labels in test_loader:\n",
    "        outputs = model(forecast)\n",
    "        \n",
    "        print(f\"outputs [{outputs}]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
