{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "PATH = r'C:\\Users\\MikhailPetrovBrainer\\Documents\\Private\\SoSe22\\Projektseminar\\data\\q4_2017.xlsx'\n",
    "columns=['Sp_nummer', 'Due_date', 'Fc_horizon', 'Fc_date', 'Fc_and_order', 'Billing']\n",
    "df = pd.read_excel(PATH, index_col=None, header=1)\n",
    "df.drop(labels=['Unnamed: 0'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sp_number</th>\n",
       "      <th>Due_date</th>\n",
       "      <th>Fc_horizon</th>\n",
       "      <th>Fc_date</th>\n",
       "      <th>Fc_and_order</th>\n",
       "      <th>Billing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Product_19</td>\n",
       "      <td>201813</td>\n",
       "      <td>13</td>\n",
       "      <td>201752</td>\n",
       "      <td>176316</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Product_20</td>\n",
       "      <td>201813</td>\n",
       "      <td>13</td>\n",
       "      <td>201752</td>\n",
       "      <td>516510</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Product_22</td>\n",
       "      <td>201813</td>\n",
       "      <td>13</td>\n",
       "      <td>201752</td>\n",
       "      <td>237587</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Product_30</td>\n",
       "      <td>201813</td>\n",
       "      <td>13</td>\n",
       "      <td>201752</td>\n",
       "      <td>393741</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Product_39</td>\n",
       "      <td>201813</td>\n",
       "      <td>13</td>\n",
       "      <td>201752</td>\n",
       "      <td>92112</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sp_number  Due_date  Fc_horizon  Fc_date  Fc_and_order  Billing\n",
       "0  Product_19    201813          13   201752        176316      NaN\n",
       "1  Product_20    201813          13   201752        516510      NaN\n",
       "2  Product_22    201813          13   201752        237587      NaN\n",
       "3  Product_30    201813          13   201752        393741      NaN\n",
       "4  Product_39    201813          13   201752         92112      NaN"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = df['Sp_number'].unique()\n",
    "prod2idx = {}\n",
    "idx2prod = {}\n",
    "for idx, prod in enumerate(products):\n",
    "    if prod not in prod2idx:\n",
    "        prod2idx[prod] = idx\n",
    "        idx2prod[idx] = prod\n",
    "        \n",
    "#Add column with integer product names\n",
    "\n",
    "products_int = []\n",
    "for idx, row in df['Sp_number'].iteritems():\n",
    "    products_int.append(prod2idx[row])\n",
    "    \n",
    "df['products'] = products_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = {\n",
    "    'products': 'product',\n",
    "    'Fc_horizon': 'horizon',\n",
    "    'Fc_and_order': 'forecast',\n",
    "    'Billing': 'billing',\n",
    "    \"Due_date\": \"ddate\",\n",
    "    \"Fc_date\": \"fdate\"\n",
    "}\n",
    "df.rename(columns=mapper, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['isodate'] = df[['ddate']].apply(lambda x: dt.datetime.strptime(str(x['ddate'])+'-1',\"%Y%W-%w\"), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sp_number</th>\n",
       "      <th>ddate</th>\n",
       "      <th>horizon</th>\n",
       "      <th>fdate</th>\n",
       "      <th>forecast</th>\n",
       "      <th>billing</th>\n",
       "      <th>product</th>\n",
       "      <th>isodate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Product_19</td>\n",
       "      <td>201813</td>\n",
       "      <td>13</td>\n",
       "      <td>201752</td>\n",
       "      <td>176316</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-03-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Product_20</td>\n",
       "      <td>201813</td>\n",
       "      <td>13</td>\n",
       "      <td>201752</td>\n",
       "      <td>516510</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-03-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Product_22</td>\n",
       "      <td>201813</td>\n",
       "      <td>13</td>\n",
       "      <td>201752</td>\n",
       "      <td>237587</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-03-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Product_30</td>\n",
       "      <td>201813</td>\n",
       "      <td>13</td>\n",
       "      <td>201752</td>\n",
       "      <td>393741</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>2018-03-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Product_39</td>\n",
       "      <td>201813</td>\n",
       "      <td>13</td>\n",
       "      <td>201752</td>\n",
       "      <td>92112</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-03-26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sp_number   ddate  horizon   fdate  forecast  billing  product    isodate\n",
       "0  Product_19  201813       13  201752    176316      NaN        0 2018-03-26\n",
       "1  Product_20  201813       13  201752    516510      NaN        1 2018-03-26\n",
       "2  Product_22  201813       13  201752    237587      NaN        2 2018-03-26\n",
       "3  Product_30  201813       13  201752    393741      NaN        3 2018-03-26\n",
       "4  Product_39  201813       13  201752     92112      NaN        4 2018-03-26"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27064 entries, 0 to 27063\n",
      "Data columns (total 8 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   Sp_number  27064 non-null  object        \n",
      " 1   ddate      27064 non-null  int64         \n",
      " 2   horizon    27064 non-null  int64         \n",
      " 3   fdate      27064 non-null  int64         \n",
      " 4   forecast   27064 non-null  int64         \n",
      " 5   billing    26254 non-null  float64       \n",
      " 6   product    27064 non-null  int64         \n",
      " 7   isodate    27064 non-null  datetime64[ns]\n",
      "dtypes: datetime64[ns](1), float64(1), int64(5), object(1)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dh = df.groupby(['isodate', 'horizon', 'product'], as_index=False).sum()\n",
    "idxs = df_dh.loc[df_dh['billing'] == 0].index\n",
    "df_dh.drop(idxs, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 26124 entries, 0 to 26123\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count  Dtype         \n",
      "---  ------    --------------  -----         \n",
      " 0   isodate   26124 non-null  datetime64[ns]\n",
      " 1   horizon   26124 non-null  int64         \n",
      " 2   product   26124 non-null  int64         \n",
      " 3   ddate     26124 non-null  int64         \n",
      " 4   fdate     26124 non-null  int64         \n",
      " 5   forecast  26124 non-null  int64         \n",
      " 6   billing   26124 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(1), int64(5)\n",
      "memory usage: 1.4 MB\n"
     ]
    }
   ],
   "source": [
    "df_dh.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_dh.loc[df_dh['product'] == 0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isodate</th>\n",
       "      <th>horizon</th>\n",
       "      <th>product</th>\n",
       "      <th>ddate</th>\n",
       "      <th>fdate</th>\n",
       "      <th>forecast</th>\n",
       "      <th>billing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-01-06</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>201401</td>\n",
       "      <td>201352</td>\n",
       "      <td>220834</td>\n",
       "      <td>209000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2014-01-06</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>201401</td>\n",
       "      <td>201351</td>\n",
       "      <td>244710</td>\n",
       "      <td>209000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2014-01-06</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>201401</td>\n",
       "      <td>201350</td>\n",
       "      <td>250756</td>\n",
       "      <td>209000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2014-01-06</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>201401</td>\n",
       "      <td>201349</td>\n",
       "      <td>425917</td>\n",
       "      <td>209000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2014-01-06</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>201401</td>\n",
       "      <td>201348</td>\n",
       "      <td>421559</td>\n",
       "      <td>209000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26074</th>\n",
       "      <td>2017-12-25</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>201752</td>\n",
       "      <td>201743</td>\n",
       "      <td>178765</td>\n",
       "      <td>301000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26084</th>\n",
       "      <td>2017-12-25</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>201752</td>\n",
       "      <td>201742</td>\n",
       "      <td>178101</td>\n",
       "      <td>301000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26094</th>\n",
       "      <td>2017-12-25</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>201752</td>\n",
       "      <td>201741</td>\n",
       "      <td>167090</td>\n",
       "      <td>301000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26104</th>\n",
       "      <td>2017-12-25</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>201752</td>\n",
       "      <td>201740</td>\n",
       "      <td>171074</td>\n",
       "      <td>301000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26114</th>\n",
       "      <td>2017-12-25</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>201752</td>\n",
       "      <td>201739</td>\n",
       "      <td>196910</td>\n",
       "      <td>301000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2623 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         isodate  horizon  product   ddate   fdate  forecast   billing\n",
       "0     2014-01-06        1        0  201401  201352    220834  209000.0\n",
       "9     2014-01-06        2        0  201401  201351    244710  209000.0\n",
       "18    2014-01-06        3        0  201401  201350    250756  209000.0\n",
       "27    2014-01-06        4        0  201401  201349    425917  209000.0\n",
       "36    2014-01-06        5        0  201401  201348    421559  209000.0\n",
       "...          ...      ...      ...     ...     ...       ...       ...\n",
       "26074 2017-12-25        9        0  201752  201743    178765  301000.0\n",
       "26084 2017-12-25       10        0  201752  201742    178101  301000.0\n",
       "26094 2017-12-25       11        0  201752  201741    167090  301000.0\n",
       "26104 2017-12-25       12        0  201752  201740    171074  301000.0\n",
       "26114 2017-12-25       13        0  201752  201739    196910  301000.0\n",
       "\n",
       "[2623 rows x 7 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "hors = df1.horizon.unique()\n",
    "dates = df1.isodate.unique()\n",
    "data = {}\n",
    "\n",
    "for date in dates:\n",
    "    for h in hors:\n",
    "        val = df1.forecast.loc[(df1.horizon == h) & (df1.isodate == date)].values.tolist()\n",
    "        if not val:\n",
    "            val = [0]\n",
    "        if h not in data:\n",
    "            data[h] = []\n",
    "        data[h].append(val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-01-06</th>\n",
       "      <td>220834</td>\n",
       "      <td>244710</td>\n",
       "      <td>250756</td>\n",
       "      <td>425917</td>\n",
       "      <td>421559</td>\n",
       "      <td>421549</td>\n",
       "      <td>431115</td>\n",
       "      <td>455151</td>\n",
       "      <td>455123</td>\n",
       "      <td>463780</td>\n",
       "      <td>463904</td>\n",
       "      <td>459784</td>\n",
       "      <td>458313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-13</th>\n",
       "      <td>262289</td>\n",
       "      <td>250018</td>\n",
       "      <td>249931</td>\n",
       "      <td>250071</td>\n",
       "      <td>224503</td>\n",
       "      <td>220719</td>\n",
       "      <td>196538</td>\n",
       "      <td>192658</td>\n",
       "      <td>192600</td>\n",
       "      <td>179523</td>\n",
       "      <td>173930</td>\n",
       "      <td>159483</td>\n",
       "      <td>216909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-20</th>\n",
       "      <td>449633</td>\n",
       "      <td>525423</td>\n",
       "      <td>539445</td>\n",
       "      <td>536118</td>\n",
       "      <td>539133</td>\n",
       "      <td>409341</td>\n",
       "      <td>440793</td>\n",
       "      <td>408595</td>\n",
       "      <td>398796</td>\n",
       "      <td>401730</td>\n",
       "      <td>365757</td>\n",
       "      <td>290489</td>\n",
       "      <td>287652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-27</th>\n",
       "      <td>206085</td>\n",
       "      <td>186124</td>\n",
       "      <td>114911</td>\n",
       "      <td>117072</td>\n",
       "      <td>108968</td>\n",
       "      <td>107969</td>\n",
       "      <td>111797</td>\n",
       "      <td>115140</td>\n",
       "      <td>113152</td>\n",
       "      <td>115152</td>\n",
       "      <td>116882</td>\n",
       "      <td>124997</td>\n",
       "      <td>123413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-02-03</th>\n",
       "      <td>126028</td>\n",
       "      <td>171459</td>\n",
       "      <td>186274</td>\n",
       "      <td>167025</td>\n",
       "      <td>168022</td>\n",
       "      <td>170353</td>\n",
       "      <td>169847</td>\n",
       "      <td>195428</td>\n",
       "      <td>178293</td>\n",
       "      <td>178154</td>\n",
       "      <td>183590</td>\n",
       "      <td>202598</td>\n",
       "      <td>195719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-27</th>\n",
       "      <td>183804</td>\n",
       "      <td>222646</td>\n",
       "      <td>297679</td>\n",
       "      <td>241484</td>\n",
       "      <td>224469</td>\n",
       "      <td>224019</td>\n",
       "      <td>235650</td>\n",
       "      <td>212731</td>\n",
       "      <td>230485</td>\n",
       "      <td>216830</td>\n",
       "      <td>213490</td>\n",
       "      <td>216072</td>\n",
       "      <td>224634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-04</th>\n",
       "      <td>328866</td>\n",
       "      <td>344773</td>\n",
       "      <td>319621</td>\n",
       "      <td>422807</td>\n",
       "      <td>277310</td>\n",
       "      <td>265061</td>\n",
       "      <td>256083</td>\n",
       "      <td>220126</td>\n",
       "      <td>239771</td>\n",
       "      <td>253228</td>\n",
       "      <td>240649</td>\n",
       "      <td>242886</td>\n",
       "      <td>268371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-11</th>\n",
       "      <td>0</td>\n",
       "      <td>329160</td>\n",
       "      <td>284936</td>\n",
       "      <td>280945</td>\n",
       "      <td>303023</td>\n",
       "      <td>295784</td>\n",
       "      <td>272773</td>\n",
       "      <td>237929</td>\n",
       "      <td>251200</td>\n",
       "      <td>241125</td>\n",
       "      <td>248384</td>\n",
       "      <td>245842</td>\n",
       "      <td>253764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-18</th>\n",
       "      <td>194600</td>\n",
       "      <td>0</td>\n",
       "      <td>215511</td>\n",
       "      <td>225748</td>\n",
       "      <td>222492</td>\n",
       "      <td>279558</td>\n",
       "      <td>269261</td>\n",
       "      <td>255357</td>\n",
       "      <td>249216</td>\n",
       "      <td>239889</td>\n",
       "      <td>255379</td>\n",
       "      <td>246320</td>\n",
       "      <td>251957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-25</th>\n",
       "      <td>182723</td>\n",
       "      <td>209772</td>\n",
       "      <td>0</td>\n",
       "      <td>201367</td>\n",
       "      <td>193309</td>\n",
       "      <td>192361</td>\n",
       "      <td>189891</td>\n",
       "      <td>191804</td>\n",
       "      <td>178765</td>\n",
       "      <td>178101</td>\n",
       "      <td>167090</td>\n",
       "      <td>171074</td>\n",
       "      <td>196910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                1       2       3       4       5       6       7       8   \\\n",
       "2014-01-06  220834  244710  250756  425917  421559  421549  431115  455151   \n",
       "2014-01-13  262289  250018  249931  250071  224503  220719  196538  192658   \n",
       "2014-01-20  449633  525423  539445  536118  539133  409341  440793  408595   \n",
       "2014-01-27  206085  186124  114911  117072  108968  107969  111797  115140   \n",
       "2014-02-03  126028  171459  186274  167025  168022  170353  169847  195428   \n",
       "...            ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "2017-11-27  183804  222646  297679  241484  224469  224019  235650  212731   \n",
       "2017-12-04  328866  344773  319621  422807  277310  265061  256083  220126   \n",
       "2017-12-11       0  329160  284936  280945  303023  295784  272773  237929   \n",
       "2017-12-18  194600       0  215511  225748  222492  279558  269261  255357   \n",
       "2017-12-25  182723  209772       0  201367  193309  192361  189891  191804   \n",
       "\n",
       "                9       10      11      12      13  \n",
       "2014-01-06  455123  463780  463904  459784  458313  \n",
       "2014-01-13  192600  179523  173930  159483  216909  \n",
       "2014-01-20  398796  401730  365757  290489  287652  \n",
       "2014-01-27  113152  115152  116882  124997  123413  \n",
       "2014-02-03  178293  178154  183590  202598  195719  \n",
       "...            ...     ...     ...     ...     ...  \n",
       "2017-11-27  230485  216830  213490  216072  224634  \n",
       "2017-12-04  239771  253228  240649  242886  268371  \n",
       "2017-12-11  251200  241125  248384  245842  253764  \n",
       "2017-12-18  249216  239889  255379  246320  251957  \n",
       "2017-12-25  178765  178101  167090  171074  196910  \n",
       "\n",
       "[208 rows x 13 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ = pd.DataFrame(data, columns=data.keys(), index=dates)\n",
    "df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2014-01-06    397884.230769\n",
       "2014-01-13    213013.230769\n",
       "2014-01-20    430223.461538\n",
       "2014-01-27    127820.153846\n",
       "2014-02-03    176368.461538\n",
       "                  ...      \n",
       "2017-11-27    226461.000000\n",
       "2017-12-04    283042.461538\n",
       "2017-12-11    249605.000000\n",
       "2017-12-18    223483.692308\n",
       "2017-12-25    173320.538462\n",
       "Length: 208, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_.T.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "hors = df1.horizon.unique()\n",
    "dates = df1.isodate.unique()\n",
    "data = {}\n",
    "\n",
    "means = df_.T.mean()\n",
    "\n",
    "for date in dates:\n",
    "    mean = means[date]\n",
    "    for h in hors:\n",
    "        val = df1.forecast.loc[(df1.horizon == h) & (df1.isodate == date)].values.tolist()\n",
    "        if not val:\n",
    "            val = [mean]\n",
    "        if h not in data:\n",
    "            data[h] = []\n",
    "        data[h].append(val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = pd.DataFrame(data, columns=data.keys(), index=dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-01-06</th>\n",
       "      <td>220834.0</td>\n",
       "      <td>244710.000000</td>\n",
       "      <td>250756.000000</td>\n",
       "      <td>425917.0</td>\n",
       "      <td>421559.0</td>\n",
       "      <td>421549.0</td>\n",
       "      <td>431115.0</td>\n",
       "      <td>455151.0</td>\n",
       "      <td>455123.0</td>\n",
       "      <td>463780.0</td>\n",
       "      <td>463904.0</td>\n",
       "      <td>459784.0</td>\n",
       "      <td>458313.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-13</th>\n",
       "      <td>262289.0</td>\n",
       "      <td>250018.000000</td>\n",
       "      <td>249931.000000</td>\n",
       "      <td>250071.0</td>\n",
       "      <td>224503.0</td>\n",
       "      <td>220719.0</td>\n",
       "      <td>196538.0</td>\n",
       "      <td>192658.0</td>\n",
       "      <td>192600.0</td>\n",
       "      <td>179523.0</td>\n",
       "      <td>173930.0</td>\n",
       "      <td>159483.0</td>\n",
       "      <td>216909.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-20</th>\n",
       "      <td>449633.0</td>\n",
       "      <td>525423.000000</td>\n",
       "      <td>539445.000000</td>\n",
       "      <td>536118.0</td>\n",
       "      <td>539133.0</td>\n",
       "      <td>409341.0</td>\n",
       "      <td>440793.0</td>\n",
       "      <td>408595.0</td>\n",
       "      <td>398796.0</td>\n",
       "      <td>401730.0</td>\n",
       "      <td>365757.0</td>\n",
       "      <td>290489.0</td>\n",
       "      <td>287652.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-27</th>\n",
       "      <td>206085.0</td>\n",
       "      <td>186124.000000</td>\n",
       "      <td>114911.000000</td>\n",
       "      <td>117072.0</td>\n",
       "      <td>108968.0</td>\n",
       "      <td>107969.0</td>\n",
       "      <td>111797.0</td>\n",
       "      <td>115140.0</td>\n",
       "      <td>113152.0</td>\n",
       "      <td>115152.0</td>\n",
       "      <td>116882.0</td>\n",
       "      <td>124997.0</td>\n",
       "      <td>123413.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-02-03</th>\n",
       "      <td>126028.0</td>\n",
       "      <td>171459.000000</td>\n",
       "      <td>186274.000000</td>\n",
       "      <td>167025.0</td>\n",
       "      <td>168022.0</td>\n",
       "      <td>170353.0</td>\n",
       "      <td>169847.0</td>\n",
       "      <td>195428.0</td>\n",
       "      <td>178293.0</td>\n",
       "      <td>178154.0</td>\n",
       "      <td>183590.0</td>\n",
       "      <td>202598.0</td>\n",
       "      <td>195719.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-27</th>\n",
       "      <td>183804.0</td>\n",
       "      <td>222646.000000</td>\n",
       "      <td>297679.000000</td>\n",
       "      <td>241484.0</td>\n",
       "      <td>224469.0</td>\n",
       "      <td>224019.0</td>\n",
       "      <td>235650.0</td>\n",
       "      <td>212731.0</td>\n",
       "      <td>230485.0</td>\n",
       "      <td>216830.0</td>\n",
       "      <td>213490.0</td>\n",
       "      <td>216072.0</td>\n",
       "      <td>224634.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-04</th>\n",
       "      <td>328866.0</td>\n",
       "      <td>344773.000000</td>\n",
       "      <td>319621.000000</td>\n",
       "      <td>422807.0</td>\n",
       "      <td>277310.0</td>\n",
       "      <td>265061.0</td>\n",
       "      <td>256083.0</td>\n",
       "      <td>220126.0</td>\n",
       "      <td>239771.0</td>\n",
       "      <td>253228.0</td>\n",
       "      <td>240649.0</td>\n",
       "      <td>242886.0</td>\n",
       "      <td>268371.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-11</th>\n",
       "      <td>249605.0</td>\n",
       "      <td>329160.000000</td>\n",
       "      <td>284936.000000</td>\n",
       "      <td>280945.0</td>\n",
       "      <td>303023.0</td>\n",
       "      <td>295784.0</td>\n",
       "      <td>272773.0</td>\n",
       "      <td>237929.0</td>\n",
       "      <td>251200.0</td>\n",
       "      <td>241125.0</td>\n",
       "      <td>248384.0</td>\n",
       "      <td>245842.0</td>\n",
       "      <td>253764.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-18</th>\n",
       "      <td>194600.0</td>\n",
       "      <td>223483.692308</td>\n",
       "      <td>215511.000000</td>\n",
       "      <td>225748.0</td>\n",
       "      <td>222492.0</td>\n",
       "      <td>279558.0</td>\n",
       "      <td>269261.0</td>\n",
       "      <td>255357.0</td>\n",
       "      <td>249216.0</td>\n",
       "      <td>239889.0</td>\n",
       "      <td>255379.0</td>\n",
       "      <td>246320.0</td>\n",
       "      <td>251957.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-25</th>\n",
       "      <td>182723.0</td>\n",
       "      <td>209772.000000</td>\n",
       "      <td>173320.538462</td>\n",
       "      <td>201367.0</td>\n",
       "      <td>193309.0</td>\n",
       "      <td>192361.0</td>\n",
       "      <td>189891.0</td>\n",
       "      <td>191804.0</td>\n",
       "      <td>178765.0</td>\n",
       "      <td>178101.0</td>\n",
       "      <td>167090.0</td>\n",
       "      <td>171074.0</td>\n",
       "      <td>196910.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  1              2              3         4         5   \\\n",
       "2014-01-06  220834.0  244710.000000  250756.000000  425917.0  421559.0   \n",
       "2014-01-13  262289.0  250018.000000  249931.000000  250071.0  224503.0   \n",
       "2014-01-20  449633.0  525423.000000  539445.000000  536118.0  539133.0   \n",
       "2014-01-27  206085.0  186124.000000  114911.000000  117072.0  108968.0   \n",
       "2014-02-03  126028.0  171459.000000  186274.000000  167025.0  168022.0   \n",
       "...              ...            ...            ...       ...       ...   \n",
       "2017-11-27  183804.0  222646.000000  297679.000000  241484.0  224469.0   \n",
       "2017-12-04  328866.0  344773.000000  319621.000000  422807.0  277310.0   \n",
       "2017-12-11  249605.0  329160.000000  284936.000000  280945.0  303023.0   \n",
       "2017-12-18  194600.0  223483.692308  215511.000000  225748.0  222492.0   \n",
       "2017-12-25  182723.0  209772.000000  173320.538462  201367.0  193309.0   \n",
       "\n",
       "                  6         7         8         9         10        11  \\\n",
       "2014-01-06  421549.0  431115.0  455151.0  455123.0  463780.0  463904.0   \n",
       "2014-01-13  220719.0  196538.0  192658.0  192600.0  179523.0  173930.0   \n",
       "2014-01-20  409341.0  440793.0  408595.0  398796.0  401730.0  365757.0   \n",
       "2014-01-27  107969.0  111797.0  115140.0  113152.0  115152.0  116882.0   \n",
       "2014-02-03  170353.0  169847.0  195428.0  178293.0  178154.0  183590.0   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2017-11-27  224019.0  235650.0  212731.0  230485.0  216830.0  213490.0   \n",
       "2017-12-04  265061.0  256083.0  220126.0  239771.0  253228.0  240649.0   \n",
       "2017-12-11  295784.0  272773.0  237929.0  251200.0  241125.0  248384.0   \n",
       "2017-12-18  279558.0  269261.0  255357.0  249216.0  239889.0  255379.0   \n",
       "2017-12-25  192361.0  189891.0  191804.0  178765.0  178101.0  167090.0   \n",
       "\n",
       "                  12        13  \n",
       "2014-01-06  459784.0  458313.0  \n",
       "2014-01-13  159483.0  216909.0  \n",
       "2014-01-20  290489.0  287652.0  \n",
       "2014-01-27  124997.0  123413.0  \n",
       "2014-02-03  202598.0  195719.0  \n",
       "...              ...       ...  \n",
       "2017-11-27  216072.0  224634.0  \n",
       "2017-12-04  242886.0  268371.0  \n",
       "2017-12-11  245842.0  253764.0  \n",
       "2017-12-18  246320.0  251957.0  \n",
       "2017-12-25  171074.0  196910.0  \n",
       "\n",
       "[208 rows x 13 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = df_.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = df_dh[['isodate','billing']].loc[df['product'] == 0].drop_duplicates(['isodate']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.set_index(['isodate'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>billing</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>isodate</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-01-06</th>\n",
       "      <td>209000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-13</th>\n",
       "      <td>760000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-20</th>\n",
       "      <td>50000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-27</th>\n",
       "      <td>207000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-02-03</th>\n",
       "      <td>1225000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-27</th>\n",
       "      <td>960000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-04</th>\n",
       "      <td>200000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-11</th>\n",
       "      <td>135000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-18</th>\n",
       "      <td>1472500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-25</th>\n",
       "      <td>357500.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              billing\n",
       "isodate              \n",
       "2014-01-06   209000.0\n",
       "2014-01-13   760000.0\n",
       "2014-01-20    50000.0\n",
       "2014-01-27   207000.0\n",
       "2014-02-03  1225000.0\n",
       "...               ...\n",
       "2017-11-27   960000.0\n",
       "2017-12-04   200000.0\n",
       "2017-12-11   135000.0\n",
       "2017-12-18  1472500.0\n",
       "2017-12-25   357500.0\n",
       "\n",
       "[208 rows x 1 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-01-06</th>\n",
       "      <td>220834.0</td>\n",
       "      <td>244710.000000</td>\n",
       "      <td>250756.000000</td>\n",
       "      <td>425917.0</td>\n",
       "      <td>421559.0</td>\n",
       "      <td>421549.0</td>\n",
       "      <td>431115.0</td>\n",
       "      <td>455151.0</td>\n",
       "      <td>455123.0</td>\n",
       "      <td>463780.0</td>\n",
       "      <td>463904.0</td>\n",
       "      <td>459784.0</td>\n",
       "      <td>458313.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-13</th>\n",
       "      <td>262289.0</td>\n",
       "      <td>250018.000000</td>\n",
       "      <td>249931.000000</td>\n",
       "      <td>250071.0</td>\n",
       "      <td>224503.0</td>\n",
       "      <td>220719.0</td>\n",
       "      <td>196538.0</td>\n",
       "      <td>192658.0</td>\n",
       "      <td>192600.0</td>\n",
       "      <td>179523.0</td>\n",
       "      <td>173930.0</td>\n",
       "      <td>159483.0</td>\n",
       "      <td>216909.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-20</th>\n",
       "      <td>449633.0</td>\n",
       "      <td>525423.000000</td>\n",
       "      <td>539445.000000</td>\n",
       "      <td>536118.0</td>\n",
       "      <td>539133.0</td>\n",
       "      <td>409341.0</td>\n",
       "      <td>440793.0</td>\n",
       "      <td>408595.0</td>\n",
       "      <td>398796.0</td>\n",
       "      <td>401730.0</td>\n",
       "      <td>365757.0</td>\n",
       "      <td>290489.0</td>\n",
       "      <td>287652.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-27</th>\n",
       "      <td>206085.0</td>\n",
       "      <td>186124.000000</td>\n",
       "      <td>114911.000000</td>\n",
       "      <td>117072.0</td>\n",
       "      <td>108968.0</td>\n",
       "      <td>107969.0</td>\n",
       "      <td>111797.0</td>\n",
       "      <td>115140.0</td>\n",
       "      <td>113152.0</td>\n",
       "      <td>115152.0</td>\n",
       "      <td>116882.0</td>\n",
       "      <td>124997.0</td>\n",
       "      <td>123413.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-02-03</th>\n",
       "      <td>126028.0</td>\n",
       "      <td>171459.000000</td>\n",
       "      <td>186274.000000</td>\n",
       "      <td>167025.0</td>\n",
       "      <td>168022.0</td>\n",
       "      <td>170353.0</td>\n",
       "      <td>169847.0</td>\n",
       "      <td>195428.0</td>\n",
       "      <td>178293.0</td>\n",
       "      <td>178154.0</td>\n",
       "      <td>183590.0</td>\n",
       "      <td>202598.0</td>\n",
       "      <td>195719.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-27</th>\n",
       "      <td>183804.0</td>\n",
       "      <td>222646.000000</td>\n",
       "      <td>297679.000000</td>\n",
       "      <td>241484.0</td>\n",
       "      <td>224469.0</td>\n",
       "      <td>224019.0</td>\n",
       "      <td>235650.0</td>\n",
       "      <td>212731.0</td>\n",
       "      <td>230485.0</td>\n",
       "      <td>216830.0</td>\n",
       "      <td>213490.0</td>\n",
       "      <td>216072.0</td>\n",
       "      <td>224634.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-04</th>\n",
       "      <td>328866.0</td>\n",
       "      <td>344773.000000</td>\n",
       "      <td>319621.000000</td>\n",
       "      <td>422807.0</td>\n",
       "      <td>277310.0</td>\n",
       "      <td>265061.0</td>\n",
       "      <td>256083.0</td>\n",
       "      <td>220126.0</td>\n",
       "      <td>239771.0</td>\n",
       "      <td>253228.0</td>\n",
       "      <td>240649.0</td>\n",
       "      <td>242886.0</td>\n",
       "      <td>268371.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-11</th>\n",
       "      <td>249605.0</td>\n",
       "      <td>329160.000000</td>\n",
       "      <td>284936.000000</td>\n",
       "      <td>280945.0</td>\n",
       "      <td>303023.0</td>\n",
       "      <td>295784.0</td>\n",
       "      <td>272773.0</td>\n",
       "      <td>237929.0</td>\n",
       "      <td>251200.0</td>\n",
       "      <td>241125.0</td>\n",
       "      <td>248384.0</td>\n",
       "      <td>245842.0</td>\n",
       "      <td>253764.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-18</th>\n",
       "      <td>194600.0</td>\n",
       "      <td>223483.692308</td>\n",
       "      <td>215511.000000</td>\n",
       "      <td>225748.0</td>\n",
       "      <td>222492.0</td>\n",
       "      <td>279558.0</td>\n",
       "      <td>269261.0</td>\n",
       "      <td>255357.0</td>\n",
       "      <td>249216.0</td>\n",
       "      <td>239889.0</td>\n",
       "      <td>255379.0</td>\n",
       "      <td>246320.0</td>\n",
       "      <td>251957.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-25</th>\n",
       "      <td>182723.0</td>\n",
       "      <td>209772.000000</td>\n",
       "      <td>173320.538462</td>\n",
       "      <td>201367.0</td>\n",
       "      <td>193309.0</td>\n",
       "      <td>192361.0</td>\n",
       "      <td>189891.0</td>\n",
       "      <td>191804.0</td>\n",
       "      <td>178765.0</td>\n",
       "      <td>178101.0</td>\n",
       "      <td>167090.0</td>\n",
       "      <td>171074.0</td>\n",
       "      <td>196910.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  1              2              3         4         5   \\\n",
       "2014-01-06  220834.0  244710.000000  250756.000000  425917.0  421559.0   \n",
       "2014-01-13  262289.0  250018.000000  249931.000000  250071.0  224503.0   \n",
       "2014-01-20  449633.0  525423.000000  539445.000000  536118.0  539133.0   \n",
       "2014-01-27  206085.0  186124.000000  114911.000000  117072.0  108968.0   \n",
       "2014-02-03  126028.0  171459.000000  186274.000000  167025.0  168022.0   \n",
       "...              ...            ...            ...       ...       ...   \n",
       "2017-11-27  183804.0  222646.000000  297679.000000  241484.0  224469.0   \n",
       "2017-12-04  328866.0  344773.000000  319621.000000  422807.0  277310.0   \n",
       "2017-12-11  249605.0  329160.000000  284936.000000  280945.0  303023.0   \n",
       "2017-12-18  194600.0  223483.692308  215511.000000  225748.0  222492.0   \n",
       "2017-12-25  182723.0  209772.000000  173320.538462  201367.0  193309.0   \n",
       "\n",
       "                  6         7         8         9         10        11  \\\n",
       "2014-01-06  421549.0  431115.0  455151.0  455123.0  463780.0  463904.0   \n",
       "2014-01-13  220719.0  196538.0  192658.0  192600.0  179523.0  173930.0   \n",
       "2014-01-20  409341.0  440793.0  408595.0  398796.0  401730.0  365757.0   \n",
       "2014-01-27  107969.0  111797.0  115140.0  113152.0  115152.0  116882.0   \n",
       "2014-02-03  170353.0  169847.0  195428.0  178293.0  178154.0  183590.0   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2017-11-27  224019.0  235650.0  212731.0  230485.0  216830.0  213490.0   \n",
       "2017-12-04  265061.0  256083.0  220126.0  239771.0  253228.0  240649.0   \n",
       "2017-12-11  295784.0  272773.0  237929.0  251200.0  241125.0  248384.0   \n",
       "2017-12-18  279558.0  269261.0  255357.0  249216.0  239889.0  255379.0   \n",
       "2017-12-25  192361.0  189891.0  191804.0  178765.0  178101.0  167090.0   \n",
       "\n",
       "                  12        13  \n",
       "2014-01-06  459784.0  458313.0  \n",
       "2014-01-13  159483.0  216909.0  \n",
       "2014-01-20  290489.0  287652.0  \n",
       "2014-01-27  124997.0  123413.0  \n",
       "2014-02-03  202598.0  195719.0  \n",
       "...              ...       ...  \n",
       "2017-11-27  216072.0  224634.0  \n",
       "2017-12-04  242886.0  268371.0  \n",
       "2017-12-11  245842.0  253764.0  \n",
       "2017-12-18  246320.0  251957.0  \n",
       "2017-12-25  171074.0  196910.0  \n",
       "\n",
       "[208 rows x 13 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FilterNet basics\n",
    "- FilterNet models are composed primarily of a stack of parameterized modules, which we will refer to here as FilterNet layer modules (FLMs)\n",
    "- They are also coverage-preserving; that is, even though the input and output of an FLM may differ in sequence length due to a stride ratio, the time period that the input and output cover will be identical\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "## Layers\n",
    "1. (A) Full-Resolution CNN (s=1, t=cnn). High-resolution processing. Convolves CNN filters against the input signal without striding or pooling, in order to extract information at the finest available temporal resolution. This layer is computationally expensive because it is applied to the full resolution input signal (s is a stride ratio)\n",
    "2. (B) Pooling Stack 1 (s>1, t=cnn). Downsamples from the input to the output frequency. \n",
    "3. (C) Pooling Stack 2 (s>1, t=cnn). Downsamples beyond the overall output frequency.\n",
    "4. (D) Resampling Step. Matches output lengths.\n",
    "5. (E) Bottleneck Layer. Reduces channel number\n",
    "6. (F) Recurrent Stack (s=1, t=lstm). Temporal modeling\n",
    "7. (G) Output Module (s=1, k=1, t=cnn). Provides predictions for each output time step\n",
    "\n",
    "## Dataset benchmark\n",
    "-  data processing steps employed by Ordóñez and Roggen [28]\n",
    "- re-scale all data to have zero mean and unit standard deviation according to the statistics of the training set\n",
    "\n",
    "## Performace Metrics\n",
    "- Sample-based metrics are aggregated across all class predictions, and are not affected by the order of the predictions.\n",
    "- Event-based metrics are calculated after the output is segmented into discrete events, and they are strongly affected by the order of the predictions\n",
    "- F1 score for each output class\n",
    "- mean F1\n",
    "- weighted F1\n",
    "\n",
    "## Ensembling\n",
    "- m n-fold ensembling by (a) combining the training and validation sets  into a single contiguous set, (b) dividing that set into n disjoint folds of contiguous samples, (c) training n independent models where the ith model uses the ith fold for validation and the remaining\n",
    "- n-1 folds for training, and (d) ensembling the n models together during inference by simply averaging their logit outputs before the softmax function is applied.\n",
    "\n",
    "## Best Performance:\n",
    "1. 4-fold ms-C/L n=10, stride ratio = 8, params k = 1,371\n",
    "2. ms-C/L\n",
    "\n",
    "## Result\n",
    "- simple FilterNet architerture: p-CNN with the largset output:input stride ratio that can fully resolve the shortest events of interest\n",
    "\n",
    "## References:\n",
    "- sussexwearlab sussexwearlab/DeepConvLSTM Available online: https://github.com/sussexwearlab/DeepConvLSTM (accessed on Dec 9, 2019).\n",
    "- Ordóñez, F.J.; Roggen, D. Deep Convolutional and LSTM Recurrent Neural Networks for Multimodal Wearable Activity Recognition. Sensors 2016, 16, 115\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MikhailPetrovBrainer\\Documents\\Private\\SoSe22\\Projektseminar\\notebook\n",
      "C:\\Users\\MikhailPetrovBrainer\\Documents\\Private\\SoSe22\\Projektseminar\\notebook\\..\\data\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "print(os.path.join(os.getcwd()+\"\\..\\data\"))\n",
    "print(os.path.exists(os.path.join(os.getcwd()+\"\\..\\data1\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKALIEREN VS NORMALISIEREN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DataLoader object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "'''\n",
    "global variables for reading the dataset\n",
    "'''\n",
    "DIR = os.path.join(os.getcwd()+\"\\..\\data\")\n",
    "if not os.path.exists(DIR):\n",
    "    print(\"PROVIDE PATH TO FILE MANUALLY\")\n",
    "\n",
    "FILE_NAME = \"q4_2017.xlsx\" #rewrite: read file name from settings.py\n",
    "PATH = os.path.join(DIR, FILE_NAME)\n",
    "TRAIN_TEST_DATE='2017-09-30'\n",
    "\n",
    "class BillingDataset(Dataset):\n",
    "    '''\n",
    "    create dataset from excel table based on the product number\n",
    "    PS: products' names are converted into numbers from 0 to 9\n",
    "    '''    \n",
    "    def __init__(self, product=None, ttp='train'):\n",
    "        if product is None:\n",
    "            self.PROD = 0 # Rewrite to cover all products\n",
    "        else:\n",
    "            self.PROD = product\n",
    "            \n",
    "        self.ttp = ttp\n",
    "        '''\n",
    "        get dataset (excel-table) from folder\n",
    "        '''\n",
    "        df = pd.read_excel(PATH, index_col=None, header=1)\n",
    "        df_ = self._df_perparation(df)\n",
    "        \n",
    "        #CHANGE TO COVER ALL PRODUCTS\n",
    "#         self.x_, self.y_ = self._df_transformation(df_)\n",
    "#         self.x = torch.from_numpy(self.x_.values.astype(np.double)).double()\n",
    "#         self.y = torch.from_numpy(self.y_.values.astype(np.double)).double()\n",
    "#         self.n_samples = self.x_.values.shape[0]\n",
    "#         self.n_features = self.x_.values.shape[1]\n",
    "        \n",
    "        x_, y_ = self._df_transformation(df_)\n",
    "        self.x = torch.from_numpy(x_.values.astype(np.double)).double()\n",
    "        self.y = torch.from_numpy(y_.values.astype(np.double)).double()\n",
    "        self.n_samples = x_.values.shape[0]\n",
    "        self.n_features = x_.values.shape[1]\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "        \n",
    "    def _df_perparation(self, df_):\n",
    "        '''\n",
    "        rename columns, create a column with date in iso format, create unique indexes from products' names\n",
    "        '''\n",
    "        df = df_.copy()\n",
    "        df.drop(labels=['Unnamed: 0'], axis=1, inplace=True)\n",
    "        df['Billing'].loc[df['Billing'].isna()] = 0\n",
    "        df['Fc_and_order'].loc[df['Fc_and_order'].isna()] = 0\n",
    "        products = df['Sp_number'].unique()\n",
    "        prod2idx = {}\n",
    "        idx2prod = {}\n",
    "        for idx, prod in enumerate(products):\n",
    "            if prod not in prod2idx:\n",
    "                prod2idx[prod] = idx\n",
    "                idx2prod[idx] = prod\n",
    "\n",
    "        #Add column with integer product names\n",
    "\n",
    "        products_int = []\n",
    "        for idx, row in df['Sp_number'].iteritems():\n",
    "            products_int.append(prod2idx[row])\n",
    "\n",
    "        df['products'] = products_int\n",
    "        \n",
    "        mapper = {\n",
    "            'products': 'product',\n",
    "            'Fc_horizon': 'horizon',\n",
    "            'Fc_and_order': 'forecast',\n",
    "            'Billing': 'billing',\n",
    "            \"Due_date\": \"ddate\",\n",
    "            \"Fc_date\": \"fdate\"\n",
    "        }\n",
    "        df.rename(columns=mapper, inplace=True)\n",
    "        df['isodate'] = df[['ddate']].apply(lambda x: dt.datetime.strptime(str(x['ddate'])+'-1',\"%Y%W-%w\"), axis=1)\n",
    "        return df\n",
    "    \n",
    "    def _df_transformation(self, df_):\n",
    "        df = df_.copy()\n",
    "        if self.ttp == 'train' or self.ttp == 'test':\n",
    "            idxs = df.loc[df['billing'] == 0].index\n",
    "            train_test_date = TRAIN_TEST_DATE\n",
    "        elif self.ttp == 'predict':\n",
    "            idxs = df.loc[df['billing'] != 0].index\n",
    "        \n",
    "        df.drop(idxs, inplace=True)\n",
    "        \n",
    "        \n",
    "        # CHANGE TO COVER ALL PRODUCTS\n",
    "        # collect billings\n",
    "        df_b = df[['isodate', 'billing']].loc[df['product'] == self.PROD].drop_duplicates(['isodate'])\n",
    "        df_b.set_index(['isodate'], inplace=True)\n",
    "        df_b.index = pd.DatetimeIndex(df_b.index).to_period('W')\n",
    "        \n",
    "        # collect forecast\n",
    "        df1 = df.loc[df['product'] == self.PROD].copy()\n",
    "        \n",
    "        hors = df1.horizon.unique()\n",
    "        dates = df1.isodate.unique()\n",
    "        data = {}\n",
    "\n",
    "        for date in dates:\n",
    "            for h in hors:\n",
    "                val = df1.forecast.loc[(df1.horizon == h) & (df1.isodate == date)].values.tolist()\n",
    "                if not val:\n",
    "                    val = [0]\n",
    "                if h not in data:\n",
    "                    data[h] = []\n",
    "                data[h].append(val[0])\n",
    "                \n",
    "        df_ = pd.DataFrame(data, columns=data.keys(), index=dates)\n",
    "        \n",
    "        hors = df1.horizon.unique()\n",
    "        dates = df1.isodate.unique()\n",
    "        data = {}\n",
    "\n",
    "        means = df_.T.mean()\n",
    "\n",
    "        for date in dates:\n",
    "            mean = means[date]\n",
    "            for h in hors:\n",
    "                val = df1.forecast.loc[(df1.horizon == h) & (df1.isodate == date)].values.tolist()\n",
    "                if not val:\n",
    "                    val = [mean]\n",
    "                if h not in data:\n",
    "                    data[h] = []\n",
    "                data[h].append(val[0])\n",
    "                \n",
    "        df_ = pd.DataFrame(data, columns=data.keys(), index=dates)\n",
    "        df_.index = pd.DatetimeIndex(df_.index).to_period('W')\n",
    "        \n",
    "        if self.ttp == 'train':\n",
    "            return df_[train_test_date:], df_b[train_test_date:]\n",
    "        elif self.ttp == 'test':\n",
    "            return df_[:train_test_date], df_b[:train_test_date]\n",
    "        else:\n",
    "            return df_, df_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Net architecture Programming\n",
    "- 208 time series length (T) - 208 weeks\n",
    "- 12 size of a Window (W) - quarter\n",
    "- 6&3 size of intermediate time series (t)\n",
    "- 6 Filters (F)\n",
    "- 2 Depth\n",
    "- 13 features\n",
    "- 1 output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# from fastai.core import ifnone, listify\n",
    "# from fastai.layers import bn_drop_lin, embedding, Flatten\n",
    "\n",
    "\n",
    "def conv_layer(window, ks=3, dilation=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv1d(1,1, kernel_size=ks, bias=False, dilation=dilation),\n",
    "        nn.AdaptiveAvgPool1d(window),\n",
    "        nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "    )\n",
    "\n",
    "class FilterNet(nn.Module):\n",
    "#     def __init__(self, emb_size, n_cont, out_sz, layers, emb_drop=0., window=12, filters=[1,2,3,4,5,6], y_range=None, ise_bn=False, ps=None, bn_final=False):\n",
    "    def __init__(self, out_sz=4, emb_drop=0., window=12, filters=[1,2,3,4,5,6], y_range=None, ise_bn=False, ps=None, bn_final=False):\n",
    "        super().__init__()\n",
    "        self.c1a = conv_layer(window=window // 2, ks=1, dilation=1)\n",
    "        self.c1b = conv_layer(window=window // 4, ks=1, dilation=2)\n",
    "        self.c2a = conv_layer(window=window // 2, ks=2, dilation=1)\n",
    "        self.c2b = conv_layer(window=window // 4, ks=2, dilation=2)\n",
    "        self.c3a = conv_layer(window=window // 2, ks=3, dilation=1)\n",
    "        self.c3b = conv_layer(window=window // 4, ks=3, dilation=2)\n",
    "        self.c4a = conv_layer(window=window // 2, ks=4, dilation=1)\n",
    "        self.c4b = conv_layer(window=window // 4, ks=4, dilation=2)\n",
    "        self.c5a = conv_layer(window=window // 2, ks=5, dilation=1)\n",
    "        self.c5b = conv_layer(window=window // 4, ks=5, dilation=2)\n",
    "        self.c6a = conv_layer(window=window // 2, ks=6, dilation=1)\n",
    "        self.c6b = conv_layer(window=window // 4, ks=6, dilation=2)\n",
    "        \n",
    "        num_wave_outputs = (len(filters) * (window // 2)) + (len(filters) * (window // 4))\n",
    "        \n",
    "#         # Fastai's Mixed Input model\n",
    "#         ps = ifnone(ps, [0]*len(layers))\n",
    "#         ps = listify(ps, layers)\n",
    "#         self.embeds = nn.ModuleList([embedding(ni, nf) for ni,nf in emb_szs])\n",
    "#         self.emb_drop = nn.Dropout(emb_drop)\n",
    "#         self.bn_cont = nn.BatchNorm1d(n_cont)\n",
    "#         n_emb = sum(e.embedding_dim for e in self.embeds)\n",
    "#         self.n_emb,self.n_cont,self.y_range = n_emb,n_cont,y_range\n",
    "#         sizes = self.get_sizes(layers, out_sz)\n",
    "#         actns = [nn.ReLU(inplace=True)] * (len(sizes)-2) + [None]\n",
    "#         layers = []\n",
    "#         for i,(n_in,n_out,dp,act) in enumerate(zip(sizes[:-2],sizes[1:-1],[0.]+ps,actns)):\n",
    "#             layers += bn_drop_lin(n_in, n_out, bn=use_bn and i!=0, p=dp, actn=act)\n",
    "#         if bn_final: layers.append(nn.BatchNorm1d(sizes[-1]))\n",
    "#         self.layers = nn.Sequential(*layers)\n",
    "\n",
    "        # Final layer\n",
    "#         self.f = Flatten()\n",
    "        self.f = nn.Flatten()\n",
    "        self.lin = nn.Linear(num_wave_outputs, out_sz, bias=False)\n",
    "\n",
    "#         self.sizes = sizes\n",
    "        self.num_wave_outputs = num_wave_outputs\n",
    "\n",
    "#     def get_sizes(self, layers, out_sz):\n",
    "#         return [self.n_emb + self.n_cont] + layers + [out_sz]\n",
    "    \n",
    "#     def forward(self, x_window, x_cat, x_cont):\n",
    "    def forward(self, x_window, x_cat, x_cont):\n",
    "        # TODO: Use the filters arg to generate the conv_layers dynamically\n",
    "        # Wavenet model\n",
    "        self.f1a = self.c1a(x_window)\n",
    "        self.f1b = self.c1b(self.f1a)\n",
    "        self.f2a = self.c2a(x_window)\n",
    "        self.f2b = self.c2b(self.f2a)\n",
    "        self.f3a = self.c3a(x_window)\n",
    "        self.f3b = self.c3b(self.f3a)\n",
    "        self.f4a = self.c4a(x_window)\n",
    "        self.f4b = self.c4b(self.f4a)\n",
    "        self.f5a = self.c5a(x_window)\n",
    "        self.f5b = self.c5b(self.f5a)\n",
    "        self.f6a = self.c6a(x_window)\n",
    "        self.f6b = self.c6b(self.f6a)\n",
    "        self.ffc = torch.cat([self.f1a, self.f1b, self.f2a, self.f2b,\n",
    "                              self.f3a, self.f3b, self.f4a, self.f4b,\n",
    "                              self.f5a, self.f5b, self.f6a, self.f6b, ], 2)\n",
    "\n",
    "#         # Fastai's Mixed Input Model\n",
    "#         if self.n_emb != 0:\n",
    "#             x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]\n",
    "#             x = torch.cat(x, 1)\n",
    "#             x = self.emb_drop(x)\n",
    "#         if self.n_cont != 0:\n",
    "#             x_cont = self.bn_cont(x_cont)\n",
    "#             x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont\n",
    "# #         x = self.layers(x)\n",
    "#         if self.y_range is not None:\n",
    "#             x = (self.y_range[1]-self.y_range[0]) * torch.sigmoid(x) + self.y_range[0]\n",
    "\n",
    "        # Combine results from both nets\n",
    "        x = x.unsqueeze(1)\n",
    "        self.fc = torch.cat([self.ffc, x], 2)\n",
    "        self.flin = self.lin(self.f(self.fc))\n",
    "        return self.flin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFilterNet(nn.Module):\n",
    "    def __init__(self, window=13, ks=3):\n",
    "        super(MyFilterNet, self).__init__()\n",
    "        self.conv1a = nn.Conv1d(13, 13, kernel_size=2, bias=False, dilation=1)\n",
    "        self.bn1a = nn.BatchNorm1d(12)\n",
    "        self.pool1a = nn.AdaptiveAvgPool1d(6)\n",
    "        self.relu1a = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "        \n",
    "        self.conv1b = nn.Conv1d(13, 13, kernel_size=1, bias=False, dilation=2)\n",
    "        self.bn1b = nn.BatchNorm1d(6)\n",
    "        self.pool1b = nn.MaxPool1d(1)\n",
    "        self.relu1b = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "        \n",
    "        self.f = nn.Flatten()\n",
    "        self.lin = nn.Linear(6, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1a(x)\n",
    "#         x = self.bn1a(x)\n",
    "        x = self.relu1a(x)\n",
    "        x = self.pool1a(x)\n",
    "        x = self.conv1b(x)\n",
    "#         x = self.bn1b(x)\n",
    "        x = self.relu1b(x)\n",
    "        x = self.pool1b(x)\n",
    "        x = self.f(x)\n",
    "        x = self.lin(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-35-689b1332d0fd>:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Billing'].loc[df['Billing'].isna()] = 0\n",
      "<ipython-input-35-689b1332d0fd>:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Fc_and_order'].loc[df['Fc_and_order'].isna()] = 0\n",
      "<ipython-input-35-689b1332d0fd>:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Billing'].loc[df['Billing'].isna()] = 0\n",
      "<ipython-input-35-689b1332d0fd>:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Fc_and_order'].loc[df['Fc_and_order'].isna()] = 0\n",
      "<ipython-input-35-689b1332d0fd>:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Billing'].loc[df['Billing'].isna()] = 0\n",
      "<ipython-input-35-689b1332d0fd>:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Fc_and_order'].loc[df['Fc_and_order'].isna()] = 0\n"
     ]
    }
   ],
   "source": [
    "batch_size = 13\n",
    "\n",
    "\n",
    "train_set = BillingDataset(ttp='train')\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_set = BillingDataset(ttp='test')\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "predict_set = BillingDataset(ttp='predict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- train 2017-09-25/2017-10-01 len 195\n",
    "- test 2017-12-25/2017-12-31 2017-12-25/2017-12-31 len 14\n",
    "- predict len 13: predict.x - input for predictions; predict.y equals 0 weil keine Billings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of MyFilterNet(\n",
       "  (conv1a): Conv1d(13, 13, kernel_size=(2,), stride=(1,), bias=False)\n",
       "  (pool1a): AdaptiveAvgPool1d(output_size=6)\n",
       "  (relu1a): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  (conv1b): Conv1d(13, 13, kernel_size=(1,), stride=(1,), dilation=(2,), bias=False)\n",
       "  (pool1b): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  (relu1b): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  (f): Flatten(start_dim=1, end_dim=-1)\n",
       "  (lin): Linear(in_features=6, out_features=1, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 80442075174.62456\n",
      "loss 76683451949.55392\n",
      "loss 69374478982.98535\n",
      "loss 56480624424.094246\n",
      "loss 55696419656.90788\n",
      "loss 59369761959.17741\n",
      "loss 55078595710.34779\n",
      "loss 57635141145.41988\n",
      "loss 54956418261.85853\n",
      "loss 60134487863.30786\n",
      "Epoch [1/50], Step [10/15], Loss: [60134487863.30786]\n",
      "loss 69992593271.2203\n",
      "loss 73281550721.05898\n",
      "loss 62933789801.05668\n",
      "loss 72477336265.3434\n",
      "loss 70445067820.83899\n",
      "loss 78786714789.01357\n",
      "loss 75369613330.54697\n",
      "loss 68573135399.27426\n",
      "loss 55579144884.18434\n",
      "loss 54865031606.40329\n",
      "loss 58802484674.75574\n",
      "loss 54445392116.05482\n",
      "loss 57213668132.6977\n",
      "loss 54468606271.526764\n",
      "loss 59334423716.66729\n",
      "Epoch [2/50], Step [10/15], Loss: [59334423716.66729]\n",
      "loss 69293498903.6314\n",
      "loss 72712138088.72403\n",
      "loss 62181251250.09712\n",
      "loss 71982571288.13814\n",
      "loss 70100182812.37611\n",
      "loss 78433760447.83168\n",
      "loss 75003943641.2067\n",
      "loss 68386984728.11787\n",
      "loss 55437962459.683105\n",
      "loss 54600878296.11471\n",
      "loss 58600943988.09653\n",
      "loss 54295407031.391815\n",
      "loss 56803560629.40491\n",
      "loss 54219563679.03223\n",
      "loss 59028262503.67088\n",
      "Epoch [3/50], Step [10/15], Loss: [59028262503.67088]\n",
      "loss 69066836122.71811\n",
      "loss 72571491153.37321\n",
      "loss 62018431970.355995\n",
      "loss 71622013636.57433\n",
      "loss 69949225489.3748\n",
      "loss 78284697026.67928\n",
      "loss 74887665315.28777\n",
      "loss 68271147329.623695\n",
      "loss 55328900128.700165\n",
      "loss 54501204864.9895\n",
      "loss 58524263061.1823\n",
      "loss 54172728999.24792\n",
      "loss 56708960078.76794\n",
      "loss 54163252791.248604\n",
      "loss 58898935895.79081\n",
      "Epoch [4/50], Step [10/15], Loss: [58898935895.79081]\n",
      "loss 68896653681.80386\n",
      "loss 72431956133.12437\n",
      "loss 61896525862.3174\n",
      "loss 71518099327.3993\n",
      "loss 69808154048.33005\n",
      "loss 78116432256.39987\n",
      "loss 74816517845.90442\n",
      "loss 68138027233.79955\n",
      "loss 55224690002.76867\n",
      "loss 54415897249.22827\n",
      "loss 58462842985.03597\n",
      "loss 54080794969.21973\n",
      "loss 56637025669.19458\n",
      "loss 54103776204.426346\n",
      "loss 58772222262.45075\n",
      "Epoch [5/50], Step [10/15], Loss: [58772222262.45075]\n",
      "loss 68805569273.28784\n",
      "loss 72308850665.22215\n",
      "loss 61825732297.69621\n",
      "loss 71457706357.52036\n",
      "loss 69690494036.53249\n",
      "loss 78004486229.793\n",
      "loss 74725373035.92416\n",
      "loss 68057044609.151405\n",
      "loss 55171691748.53559\n",
      "loss 54327995634.65984\n",
      "loss 58391846694.085014\n",
      "loss 54018462981.33805\n",
      "loss 56526859578.051254\n",
      "loss 54022300362.54049\n",
      "loss 58618566522.92728\n",
      "Epoch [6/50], Step [10/15], Loss: [58618566522.92728]\n",
      "loss 68734888363.49982\n",
      "loss 72211098010.23676\n",
      "loss 61718990734.74737\n",
      "loss 71365764088.91942\n",
      "loss 69595167533.73587\n",
      "loss 77924411930.98209\n",
      "loss 74621009515.51236\n",
      "loss 67984014164.88023\n",
      "loss 55097316879.0348\n",
      "loss 54236530945.7289\n",
      "loss 58305867229.575836\n",
      "loss 53935424540.09544\n",
      "loss 56413552820.88527\n",
      "loss 53928190764.85611\n",
      "loss 58432096395.44786\n",
      "Epoch [7/50], Step [10/15], Loss: [58432096395.44786]\n",
      "loss 68636075668.48154\n",
      "loss 72068253945.0005\n",
      "loss 61599924195.74136\n",
      "loss 71244805565.7899\n",
      "loss 69461671761.077\n",
      "loss 77803183377.098\n",
      "loss 74487432422.38387\n",
      "loss 67870060610.9753\n",
      "loss 54983339830.186485\n",
      "loss 54107287659.594185\n",
      "loss 58170493483.41999\n",
      "loss 53809990032.42496\n",
      "loss 56242182892.43703\n",
      "loss 53778635699.00883\n",
      "loss 58172151738.34546\n",
      "Epoch [8/50], Step [10/15], Loss: [58172151738.34546]\n",
      "loss 68475585966.84\n",
      "loss 71840599036.8476\n",
      "loss 61411892284.92608\n",
      "loss 71044160775.04298\n",
      "loss 69247211164.0517\n",
      "loss 77603520861.03595\n",
      "loss 74272007830.77583\n",
      "loss 67681375918.058395\n",
      "loss 54798553741.14489\n",
      "loss 53908949563.8271\n",
      "loss 57946970348.0911\n",
      "loss 53602463272.120125\n",
      "loss 55962883318.41383\n",
      "loss 53530673929.52032\n",
      "loss 57826046652.25884\n",
      "Epoch [9/50], Step [10/15], Loss: [57826046652.25884]\n",
      "loss 68206135650.28161\n",
      "loss 71464213507.98643\n",
      "loss 61100167776.751274\n",
      "loss 70706229970.90115\n",
      "loss 68892592883.59554\n",
      "loss 77265161464.28732\n",
      "loss 73907937962.57745\n",
      "loss 67361173616.43367\n",
      "loss 54490904221.626434\n",
      "loss 53575959140.14061\n",
      "loss 57572335321.618286\n",
      "loss 53254911824.37293\n",
      "loss 55510814277.94295\n",
      "loss 53114251006.72074\n",
      "loss 57279956967.09531\n",
      "Epoch [10/50], Step [10/15], Loss: [57279956967.09531]\n",
      "loss 67747056556.1392\n",
      "loss 70832973331.70222\n",
      "loss 60575387378.422356\n",
      "loss 70128056588.20436\n",
      "loss 68295635638.473404\n",
      "loss 76682021986.95209\n",
      "loss 73282373535.14923\n",
      "loss 66808788001.50578\n",
      "loss 53971842588.68043\n",
      "loss 53009248367.87071\n",
      "loss 56934209483.966324\n",
      "loss 52665391510.86054\n",
      "loss 54751412821.13316\n",
      "loss 52408297756.17243\n",
      "loss 56363935938.19429\n",
      "Epoch [11/50], Step [10/15], Loss: [56363935938.19429]\n",
      "loss 66960172883.97606\n",
      "loss 69766661122.05125\n",
      "loss 59687449475.54017\n",
      "loss 69135647521.24219\n",
      "loss 67286986518.67286\n",
      "loss 75678170502.07704\n",
      "loss 72210734468.43033\n",
      "loss 65860209018.723785\n",
      "loss 53100682973.5734\n",
      "loss 52052371016.00492\n",
      "loss 55853288587.28322\n",
      "loss 51674959756.8743\n",
      "loss 53486066580.33316\n",
      "loss 51225587780.47839\n",
      "loss 54850980306.86058\n",
      "Epoch [12/50], Step [10/15], Loss: [54850980306.86058]\n",
      "loss 65636085544.834915\n",
      "loss 67999043691.324104\n",
      "loss 58215530049.18123\n",
      "loss 67458482575.70208\n",
      "loss 65621837464.82313\n",
      "loss 73995721762.57338\n",
      "loss 70411670379.13771\n",
      "loss 64279627547.09091\n",
      "loss 51665609079.28617\n",
      "loss 50481530326.804535\n",
      "loss 54058907829.76709\n",
      "loss 50049632534.26611\n",
      "loss 51424014728.02565\n",
      "loss 49293430658.18481\n",
      "loss 52404396008.44377\n",
      "Epoch [13/50], Step [10/15], Loss: [52404396008.44377]\n",
      "loss 63488127276.23967\n",
      "loss 65159858288.89877\n",
      "loss 55849242228.52042\n",
      "loss 64675608250.90484\n",
      "loss 62973643083.369064\n",
      "loss 71281282247.58238\n",
      "loss 67448213670.76981\n",
      "loss 61748761453.09046\n",
      "loss 49300038291.30033\n",
      "loss 47971655920.17744\n",
      "loss 51100791099.65514\n",
      "loss 47417016923.42249\n",
      "loss 48078442074.07038\n",
      "loss 46141210689.35922\n",
      "loss 48460830663.71236\n",
      "Epoch [14/50], Step [10/15], Loss: [48460830663.71236]\n",
      "loss 60008084113.01668\n",
      "loss 60555932306.71062\n",
      "loss 52023333476.138596\n",
      "loss 60037624508.05729\n",
      "loss 58731804985.84422\n",
      "loss 66749422474.188774\n",
      "loss 62510399132.17653\n",
      "loss 57574068511.255264\n",
      "loss 45307170035.77827\n",
      "loss 43921407366.26978\n",
      "loss 46171847936.384346\n",
      "loss 43099638619.088326\n",
      "loss 42622295055.211624\n",
      "loss 40981738895.16751\n",
      "loss 42110697079.22843\n",
      "Epoch [15/50], Step [10/15], Loss: [42110697079.22843]\n",
      "loss 54241375270.24089\n",
      "loss 53065783310.1466\n",
      "loss 45823981798.8055\n",
      "loss 52453330200.6138\n",
      "loss 51947892317.95678\n",
      "loss 59273895492.3051\n",
      "loss 54504200428.29871\n",
      "loss 50823420500.409096\n",
      "loss 38965831735.37039\n",
      "loss 37700689526.50558\n",
      "loss 38515469983.76071\n",
      "loss 36524590440.77414\n",
      "loss 34519258984.09781\n",
      "loss 33313863667.43742\n",
      "loss 32939488218.66767\n",
      "Epoch [16/50], Step [10/15], Loss: [32939488218.66767]\n",
      "loss 45610014545.57422\n",
      "loss 42223622635.07785\n",
      "loss 36922085535.91207\n",
      "loss 41732255274.365395\n",
      "loss 42416142193.37285\n",
      "loss 48579472020.60514\n",
      "loss 43377432808.04276\n",
      "loss 41419344482.56866\n",
      "loss 30458943551.6233\n",
      "loss 29658575743.116367\n",
      "loss 28585925594.65059\n",
      "loss 28177026824.598995\n",
      "loss 24568798709.54975\n",
      "loss 23949221559.91069\n",
      "loss 22160370843.24107\n",
      "Epoch [17/50], Step [10/15], Loss: [22160370843.24107]\n",
      "loss 34989149597.62924\n",
      "loss 29449902135.26162\n",
      "loss 26489822645.786438\n",
      "loss 29540700885.822426\n",
      "loss 31567691825.588097\n",
      "loss 36131746735.25038\n",
      "loss 30876904366.113113\n",
      "loss 30791915002.1992\n",
      "loss 21257507830.160744\n",
      "loss 21458373038.259113\n",
      "loss 18332820967.07325\n",
      "loss 19753240454.178654\n",
      "loss 14993798085.544674\n",
      "loss 15025376835.627066\n",
      "loss 12473472454.851398\n",
      "Epoch [18/50], Step [10/15], Loss: [12473472454.851398]\n",
      "loss 24586350679.405937\n",
      "loss 17777175710.09631\n",
      "loss 16930868330.441576\n",
      "loss 18871203786.609497\n",
      "loss 21943838782.04985\n",
      "loss 24607621956.30725\n",
      "loss 19860141282.797096\n",
      "loss 21261569226.32835\n",
      "loss 13524886283.197884\n",
      "loss 15172926088.338022\n",
      "loss 10272826974.024244\n",
      "loss 13267354129.549358\n",
      "loss 8260299060.610323\n",
      "loss 8790125074.543388\n",
      "loss 6448717302.988845\n",
      "Epoch [19/50], Step [10/15], Loss: [6448717302.988845]\n",
      "loss 16637206852.004108\n",
      "loss 9993850616.051413\n",
      "loss 10370111150.832863\n",
      "loss 12057301824.429321\n",
      "loss 15474852022.871412\n",
      "loss 16214463219.837261\n",
      "loss 12448181902.242596\n",
      "loss 14527129790.00835\n",
      "loss 8736849515.299988\n",
      "loss 11580751977.275381\n",
      "loss 5665177567.608046\n",
      "loss 9568984001.04507\n",
      "loss 5114769609.452912\n",
      "loss 5768411098.880303\n",
      "loss 4288832091.392062\n",
      "Epoch [20/50], Step [10/15], Loss: [4288832091.392062]\n",
      "loss 11778127842.061764\n",
      "loss 6368583598.50142\n",
      "loss 6968640341.506814\n",
      "loss 8887408377.462984\n",
      "loss 11966194915.613838\n",
      "loss 11192840941.595041\n",
      "loss 8491445939.093438\n",
      "loss 10544436815.138027\n",
      "loss 6611407625.923194\n",
      "loss 9651200095.098515\n",
      "loss 3660064829.003865\n",
      "loss 7829398027.247024\n",
      "loss 4161444150.024117\n",
      "loss 4611593316.495558\n",
      "loss 3946736082.133495\n",
      "Epoch [21/50], Step [10/15], Loss: [3946736082.133495]\n",
      "loss 9004789263.638855\n",
      "loss 4959214300.173632\n",
      "loss 5349204485.365938\n",
      "loss 7502857186.396693\n",
      "loss 9938827875.560514\n",
      "loss 8391817832.588062\n",
      "loss 6466151579.306728\n",
      "loss 8262154002.501238\n",
      "loss 5898854305.099087\n",
      "loss 8220649710.804934\n",
      "loss 2804641373.598742\n",
      "loss 6909453189.676803\n",
      "loss 3887359315.5242124\n",
      "loss 4044732319.857007\n",
      "loss 3845814859.4999447\n",
      "Epoch [22/50], Step [10/15], Loss: [3845814859.4999447]\n",
      "loss 7278369967.246377\n",
      "loss 4230623386.973514\n",
      "loss 4439429326.378393\n",
      "loss 6696939191.193419\n",
      "loss 8515108745.724\n",
      "loss 6788517173.16236\n",
      "loss 5313161318.503857\n",
      "loss 6890949548.728141\n",
      "loss 5733072531.925637\n",
      "loss 7038326359.016139\n",
      "loss 2380228441.6831594\n",
      "loss 6339549201.1063175\n",
      "loss 3788137859.9457197\n",
      "loss 3696779454.66933\n",
      "loss 3739424748.176122\n",
      "Epoch [23/50], Step [10/15], Loss: [3739424748.176122]\n",
      "loss 6149417143.349575\n",
      "loss 3770356573.666483\n",
      "loss 3878980985.5387845\n",
      "loss 6179059893.202953\n",
      "loss 7489900267.30742\n",
      "loss 5821963541.91323\n",
      "loss 4604791162.594915\n",
      "loss 6024820768.243373\n",
      "loss 5755170812.907164\n",
      "loss 6162333542.900455\n",
      "loss 2158523047.427398\n",
      "loss 5986649884.639886\n",
      "loss 3793118139.9331284\n",
      "loss 3511258585.782145\n",
      "loss 3718467213.682095\n",
      "Epoch [24/50], Step [10/15], Loss: [3718467213.682095]\n",
      "loss 5395063829.620734\n",
      "loss 3497561999.8503337\n",
      "loss 3532349208.642698\n",
      "loss 5866394237.135192\n",
      "loss 6763555346.487433\n",
      "loss 5188121448.148759\n",
      "loss 4140199719.734986\n",
      "loss 5439731114.067763\n",
      "loss 5831484019.647034\n",
      "loss 5558556091.335455\n",
      "loss 2042276396.1309903\n",
      "loss 5770949120.070851\n",
      "loss 3865351401.659702\n",
      "loss 3437387707.6135993\n",
      "loss 3784477901.6783395\n",
      "Epoch [25/50], Step [10/15], Loss: [3784477901.6783395]\n",
      "loss 4872925545.991927\n",
      "loss 3347383857.909968\n",
      "loss 3315790718.962334\n",
      "loss 5683710195.36579\n",
      "loss 6240666816.036165\n",
      "loss 4744994695.331992\n",
      "loss 3819395159.9376745\n",
      "loss 5025633121.939487\n",
      "loss 5920567455.540351\n",
      "loss 5146847261.706989\n",
      "loss 1983449440.1906533\n",
      "loss 5637466839.513641\n",
      "loss 3967744604.433364\n",
      "loss 3426885867.4750967\n",
      "loss 3892112248.415345\n",
      "Epoch [26/50], Step [10/15], Loss: [3892112248.415345]\n",
      "loss 4502554223.177928\n",
      "loss 3267342768.032212\n",
      "loss 3179746439.5792575\n",
      "loss 5575169489.857398\n",
      "loss 5853490278.08203\n",
      "loss 4431964552.95233\n",
      "loss 3594138945.974765\n",
      "loss 4730129903.063091\n",
      "loss 6011653277.468902\n",
      "loss 4861064605.468119\n",
      "loss 1957617138.0238771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 5552796886.498966\n",
      "loss 4074999953.1447906\n",
      "loss 3447229484.979143\n",
      "loss 4004728733.692218\n",
      "Epoch [27/50], Step [10/15], Loss: [4004728733.692218]\n",
      "loss 4237011312.623096\n",
      "loss 3224368378.93188\n",
      "loss 3094683592.183088\n",
      "loss 5507237812.789038\n",
      "loss 5559329329.651274\n",
      "loss 4213371304.874591\n",
      "loss 3435514920.0025187\n",
      "loss 4520135782.994383\n",
      "loss 6099558429.828269\n",
      "loss 4658693112.421521\n",
      "loss 1950397924.2878454\n",
      "loss 5497354758.29135\n",
      "loss 4174407170.1658573\n",
      "loss 3480276895.7604723\n",
      "loss 4105812415.731188\n",
      "Epoch [28/50], Step [10/15], Loss: [4105812415.731188]\n",
      "loss 4044802430.355697\n",
      "loss 3200743445.772563\n",
      "loss 3041928218.0832553\n",
      "loss 5462113706.870693\n",
      "loss 5331377900.230445\n",
      "loss 4060944163.1655173\n",
      "loss 3322705458.0118284\n",
      "loss 4370192983.833358\n",
      "loss 6180191581.3655205\n",
      "loss 4513966420.859388\n",
      "loss 1952866497.9098318\n",
      "loss 5459819909.102105\n",
      "loss 4261591005.950932\n",
      "loss 3517076253.6303196\n",
      "loss 4191711019.7464604\n",
      "Epoch [29/50], Step [10/15], Loss: [4191711019.7464604]\n",
      "loss 3903693465.036516\n",
      "loss 3187809835.3824167\n",
      "loss 3009524855.420057\n",
      "loss 5430752112.367055\n",
      "loss 5152155603.58758\n",
      "loss 3952894856.822546\n",
      "loss 3240842560.115984\n",
      "loss 4261335857.380066\n",
      "loss 6251179034.397123\n",
      "loss 4410374251.036721\n",
      "loss 1959758742.066523\n",
      "loss 5433578949.151012\n",
      "loss 4336033997.337738\n",
      "loss 3553338583.939138\n",
      "loss 4263295132.267172\n",
      "Epoch [30/50], Step [10/15], Loss: [4263295132.267172]\n",
      "loss 3798336939.05717\n",
      "loss 3181134899.759943\n",
      "loss 2989900913.41181\n",
      "loss 5408192398.045531\n",
      "loss 5009664090.745741\n",
      "loss 3874311015.0227237\n",
      "loss 3180005910.475375\n",
      "loss 4180607619.4136515\n",
      "loss 6311963762.203164\n",
      "loss 4336374902.387619\n",
      "loss 1968201170.2727573\n",
      "loss 5414636633.885685\n",
      "loss 4398530550.726711\n",
      "loss 3586927419.257175\n",
      "loss 4322063162.311443\n",
      "Epoch [31/50], Step [10/15], Loss: [4322063162.311443]\n",
      "loss 3718417688.224216\n",
      "loss 3178112537.239738\n",
      "loss 2978304461.0198536\n",
      "loss 5391369515.883422\n",
      "loss 4895341519.613018\n",
      "loss 3815738338.1555247\n",
      "loss 3133837590.1651435\n",
      "loss 4119558692.7227325\n",
      "loss 6363157183.698792\n",
      "loss 4283564205.509812\n",
      "loss 1976741410.5014753\n",
      "loss 5400510193.9067745\n",
      "loss 4450269432.104153\n",
      "loss 3616821923.0920134\n",
      "loss 4369374093.759295\n",
      "Epoch [32/50], Step [10/15], Loss: [4369374093.759295]\n",
      "loss 3656990237.481618\n",
      "loss 3177101186.1351347\n",
      "loss 2971751824.1765356\n",
      "loss 5378296440.203934\n",
      "loss 4802932628.992809\n",
      "loss 3771190819.1692195\n",
      "loss 3098211824.006427\n",
      "loss 4072610640.6869836\n",
      "loss 6405855073.220414\n",
      "loss 4245851354.1723185\n",
      "loss 1984701782.6786833\n",
      "loss 5389638288.07693\n",
      "loss 4492584442.223138\n",
      "loss 3642687272.1117334\n",
      "loss 4406581626.345638\n",
      "Epoch [33/50], Step [10/15], Loss: [4406581626.345638]\n",
      "loss 3609248472.365921\n",
      "loss 3177104315.645544\n",
      "loss 2968351628.6808796\n",
      "loss 5367709246.045519\n",
      "loss 4727784778.057716\n",
      "loss 3736681088.52091\n",
      "loss 3070326574.6464887\n",
      "loss 4035917434.3961453\n",
      "loss 6441234857.527373\n",
      "loss 4218893987.6876307\n",
      "loss 1991805899.5859168\n",
      "loss 5381033967.465147\n",
      "loss 4526841260.429881\n",
      "loss 3664613556.585606\n",
      "loss 4435095529.664198\n",
      "Epoch [34/50], Step [10/15], Loss: [4435095529.664198]\n",
      "loss 3571761569.198599\n",
      "loss 3177562263.089816\n",
      "loss 2966892867.805413\n",
      "loss 5358825384.373881\n",
      "loss 4666375584.236227\n",
      "loss 3709420370.6567\n",
      "loss 3048199395.04847\n",
      "loss 4006734858.26785\n",
      "loss 6470396393.386439\n",
      "loss 4199619771.6996326\n",
      "loss 1997982913.1492631\n",
      "loss 5374066259.838805\n",
      "loss 4554338774.041544\n",
      "loss 3682918385.364865\n",
      "loss 4456294215.968641\n",
      "Epoch [35/50], Step [10/15], Loss: [4456294215.968641]\n",
      "loss 3542030814.2291346\n",
      "loss 3178180552.2093763\n",
      "loss 2966597738.834287\n",
      "loss 5351158432.38093\n",
      "loss 4615991637.380232\n",
      "loss 3687421339.015391\n",
      "loss 3030399583.704069\n",
      "loss 3983081378.0795317\n",
      "loss 6494320449.473027\n",
      "loss 4185847628.846772\n",
      "loss 2003266435.1527689\n",
      "loss 5368322318.396949\n",
      "loss 4576240680.504419\n",
      "loss 3698011768.66749\n",
      "loss 4471426566.722025\n",
      "Epoch [36/50], Step [10/15], Loss: [4471426566.722025]\n",
      "loss 3518217766.3541107\n",
      "loss 3178809279.4620914\n",
      "loss 2966969138.6935987\n",
      "loss 5344394198.342026\n",
      "loss 4574510521.550256\n",
      "loss 3669268373.0112333\n",
      "loss 3015886784.0090885\n",
      "loss 3963525460.416214\n",
      "loss 6513864100.905416\n",
      "loss 4176017774.9884\n",
      "loss 2007739547.2953115\n",
      "loss 5363523204.524177\n",
      "loss 4593548704.342096\n",
      "loss 3710319016.60912\n",
      "loss 4481568783.350826\n",
      "Epoch [37/50], Step [10/15], Loss: [4481568783.350826]\n",
      "loss 3498959445.767766\n",
      "loss 3179373790.667102\n",
      "loss 2967692748.2527113\n",
      "loss 5338319306.93767\n",
      "loss 4540251622.101999\n",
      "loss 3653953607.453636\n",
      "loss 3003898549.2401233\n",
      "loss 3947031147.465629\n",
      "loss 6529764739.205619\n",
      "loss 4169009115.423856\n",
      "loss 2011503573.5436883\n",
      "loss 5359473739.808744\n",
      "loss 4607105665.538275\n",
      "loss 3720243461.586997\n",
      "loss 4487626285.210763\n",
      "Epoch [38/50], Step [10/15], Loss: [4487626285.210763]\n",
      "loss 3483235980.8525753\n",
      "loss 3179839548.7286215\n",
      "loss 2968572846.315061\n",
      "loss 5332782758.436265\n",
      "loss 4511871572.717195\n",
      "loss 3640754273.399827\n",
      "loss 2993869175.4123664\n",
      "loss 3932843636.6553717\n",
      "loss 6542647938.156345\n",
      "loss 4164014815.3617015\n",
      "loss 2014660833.4025788\n",
      "loss 5356032570.220819\n",
      "loss 4617611859.421065\n",
      "loss 3728150760.17879\n",
      "loss 4490353447.614312\n",
      "Epoch [39/50], Step [10/15], Loss: [4490353447.614312]\n",
      "loss 3470276118.8149543\n",
      "loss 3180194278.6821585\n",
      "loss 2969490259.045769\n",
      "loss 5327674365.413946\n",
      "loss 4488288802.845024\n",
      "loss 3629145714.903869\n",
      "loss 2985372892.7742004\n",
      "loss 3920407844.8038626\n",
      "loss 6553038643.474385\n",
      "loss 4160454609.10721\n",
      "loss 2017306078.6429057\n",
      "loss 5353093854.310741\n",
      "loss 4625644528.17862\n",
      "loss 3734363555.99033\n",
      "loss 4490375348.825744\n",
      "Epoch [40/50], Step [10/15], Loss: [4490375348.825744]\n",
      "loss 3459490726.7717543\n",
      "loss 3180437883.028191\n",
      "loss 2970375010.5369897\n",
      "loss 5322911450.55513\n",
      "loss 4468627639.018816\n",
      "loss 3618742632.2101617\n",
      "loss 2978085123.342068\n",
      "loss 3909312093.137802\n",
      "loss 6561374172.265856\n",
      "loss 4157911553.45309\n",
      "loss 2019523047.8395097\n",
      "loss 5350575775.404546\n",
      "loss 4631676522.992121\n",
      "loss 3739161244.031954\n",
      "loss 4488207199.303773\n",
      "Epoch [41/50], Step [10/15], Loss: [4488207199.303773]\n",
      "loss 3450425930.9575872\n",
      "loss 3180576317.6186237\n",
      "loss 2971188576.5112653\n",
      "loss 5318430185.768101\n",
      "loss 4452176038.002928\n",
      "loss 3609259577.2734833\n",
      "loss 2971755890.1543546\n",
      "loss 3899249021.427584\n",
      "loss 6568017205.428137\n",
      "loss 4156086251.796887\n",
      "loss 2021383750.4879618\n",
      "loss 5348413234.053713\n",
      "loss 4636093209.874\n",
      "loss 3742782479.796875\n",
      "loss 4484271517.526701\n",
      "Epoch [42/50], Step [10/15], Loss: [4484271517.526701]\n",
      "loss 3442729538.5772977\n",
      "loss 3180617987.7303653\n",
      "loss 2971912327.2043176\n",
      "loss 5314179966.104755\n",
      "loss 4438353106.11385\n",
      "loss 3600483646.444807\n",
      "loss 2966191052.559165\n",
      "loss 3889987857.401804\n",
      "loss 6573267642.109762\n",
      "loss 4154763894.2321496\n",
      "loss 2022949059.541023\n",
      "loss 5346553163.706725\n",
      "loss 4639207432.291824\n",
      "loss 3745429113.958871\n",
      "loss 4478913441.149293\n",
      "Epoch [43/50], Step [10/15], Loss: [4478913441.149293]\n",
      "loss 3436126505.476585\n",
      "loss 3180571821.1230154\n",
      "loss 2972539971.588633\n",
      "loss 5310119802.685817\n",
      "loss 4426683790.4786825\n",
      "loss 3592255052.200742\n",
      "loss 2961238710.3520355\n",
      "loss 3881354368.7645864\n",
      "loss 6577372934.136703\n",
      "loss 4153790599.2076793\n",
      "loss 2024269857.8674676\n",
      "loss 5344951487.640009\n",
      "loss 4641272457.789634\n",
      "loss 3747270666.801672\n",
      "loss 4472414151.338837\n",
      "Epoch [44/50], Step [10/15], Loss: [4472414151.338837]\n",
      "loss 3430400765.790916\n",
      "loss 3180446351.1745596\n",
      "loss 2973072623.256634\n",
      "loss 5306215981.032109\n",
      "loss 4416778916.209743\n",
      "loss 3584453113.0176806\n",
      "loss 2956779204.0703897\n",
      "loss 3873216252.7081757\n",
      "loss 6580536919.6597\n",
      "loss 4153056405.4701614\n",
      "loss 2025388360.3211412\n",
      "loss 5343571091.272908\n",
      "loss 4642492934.067492\n",
      "loss 3748448780.5024843\n",
      "loss 4465002330.598527\n",
      "Epoch [45/50], Step [10/15], Loss: [4465002330.598527]\n",
      "loss 3425381660.6034956\n",
      "loss 3180249338.8564277\n",
      "loss 2973515602.375292\n",
      "loss 5302440476.6985235\n",
      "loss 4408319275.702606\n",
      "loss 3576986119.28783\n",
      "loss 2952717700.2057643\n",
      "loss 3865472464.267313\n",
      "loss 6582927300.725428\n",
      "loss 4152483003.3380156\n",
      "loss 2026339421.9125338\n",
      "loss 5342380422.536355\n",
      "loss 4643034003.976525\n",
      "loss 3749081352.232002\n",
      "loss 4456863739.823624\n",
      "Epoch [46/50], Step [10/15], Loss: [4456863739.823624]\n",
      "loss 3420933731.34415\n",
      "loss 3179987660.278277\n",
      "loss 2973876389.415815\n",
      "loss 5298769828.323174\n",
      "loss 4401042840.629782\n",
      "loss 3569783982.163136\n",
      "loss 2948978648.4065685\n",
      "loss 3858045399.6323633\n",
      "loss 6584681925.40748\n",
      "loss 4152014864.7688856\n",
      "loss 2027151738.7038372\n",
      "loss 5341352487.096762\n",
      "loss 4643028807.009383\n",
      "loss 3749266226.335577\n",
      "loss 4448149132.849153\n",
      "Epoch [47/50], Step [10/15], Loss: [4448149132.849153]\n",
      "loss 3416948985.1928687\n",
      "loss 3179667321.6313863\n",
      "loss 2974163339.6441736\n",
      "loss 5295184301.828676\n",
      "loss 4394734416.301003\n",
      "loss 3562792865.0700636\n",
      "loss 2945501595.3147287\n",
      "loss 3850875139.975517\n",
      "loss 6585914030.012496\n",
      "loss 4151612828.256145\n",
      "loss 2027848902.0405972\n",
      "loss 5340464098.354538\n",
      "loss 4642584612.256175\n",
      "loss 3749084418.025066\n",
      "loss 4438980761.555396\n",
      "Epoch [48/50], Step [10/15], Loss: [4438980761.555396]\n",
      "loss 3413340986.3211803\n",
      "loss 3179293532.9065704\n",
      "loss 2974384897.006658\n",
      "loss 5291667250.109843\n",
      "loss 4389217229.642402\n",
      "loss 3555971227.9385\n",
      "loss 2942237988.2694697\n",
      "loss 3843915184.9467096\n",
      "loss 6586716590.189333\n",
      "loss 4151249462.35068\n",
      "loss 2028450298.2002752\n",
      "loss 5339695296.516359\n",
      "loss 4641787809.758978\n",
      "loss 3748602885.4030056\n",
      "loss 4429457703.8825035\n",
      "Epoch [49/50], Step [10/15], Loss: [4429457703.8825035]\n",
      "loss 3410040307.5809565\n",
      "loss 3178870803.6196866\n",
      "loss 2974549133.73074\n",
      "loss 5288204608.508987\n",
      "loss 4384346062.844706\n",
      "loss 3549286887.9804173\n",
      "loss 2939148710.7558813\n",
      "loss 3837129270.9724927\n",
      "loss 6587165919.284914\n",
      "loss 4150905717.835294\n",
      "loss 2028971862.3647907\n",
      "loss 5339028883.285503\n",
      "loss 4640707958.210087\n",
      "loss 3747876886.754323\n",
      "loss 4419660214.034257\n",
      "Epoch [50/50], Step [10/15], Loss: [4419660214.034257]\n",
      "loss 3406991005.929942\n",
      "loss 3178403040.1997867\n",
      "loss 2974663500.5344086\n",
      "loss 5284784487.150019\n",
      "loss 4380001632.128907\n",
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "#working\n",
    "learning_rate = 0.001\n",
    "num_epochs = 50\n",
    "# model = FilterNet()\n",
    "model = MyFilterNet()\n",
    "model.double()\n",
    "criterion = nn.MSELoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (forecast, billing) in enumerate(train_loader):\n",
    "        # forward pass\n",
    "        outputs = model(forecast)\n",
    "#         print(outputs.shape, outputs)\n",
    "        loss = criterion(outputs, billing)\n",
    "        print(f\"loss {loss}\")\n",
    "        # backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1)%10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: [{loss}]\")\n",
    "\n",
    "print(\"Finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4332428629.368768"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "47573002.76013965 - 4380001632.128907"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 shape: torch.Size([13, 12]), val: tensor([[-172414.0013, -143333.5949, -154662.7487, -145522.7895, -165297.9841,\n",
      "         -146443.1816, -133796.1751, -130963.4998, -134787.2875, -128287.9064,\n",
      "         -161150.3340, -128518.9904],\n",
      "        [ 104669.3925,  125468.5342,  118088.6164,  124266.8576,  133786.0180,\n",
      "          143071.5307,  157441.3265,  136848.0527,  169345.6111,  217143.5445,\n",
      "          219557.7327,  163959.4942],\n",
      "        [ 231737.4804,  230039.7239,  236626.2664,  223615.8127,  237106.6546,\n",
      "          244573.9568,  253897.2386,  251773.6052,  283831.6355,  248612.7986,\n",
      "          239482.5319,  234139.7431],\n",
      "        [ 185649.0389,  156597.7066,  132781.8672,  138189.4854,  146840.6994,\n",
      "          149390.6646,  171385.8557,  180473.4178,  188756.4276,  205675.9539,\n",
      "          167418.1954,  174158.6518],\n",
      "        [  -1667.2661,   28659.1843,   48224.0852,   33768.5914,   41741.3655,\n",
      "           47672.1510,   44930.6838,   55382.3053,   63828.5345,   53753.1080,\n",
      "           95559.4576,   38796.1728],\n",
      "        [ 177512.1934,  169552.4623,  156438.2919,  165321.1721,  174077.0667,\n",
      "          167490.6746,  174823.3034,  188730.5690,  172945.5361,  178259.0453,\n",
      "          178722.5423,  167224.0050],\n",
      "        [-151135.1794, -180194.1004, -161732.5199, -159757.5997, -155753.0602,\n",
      "         -159848.5113, -144653.5535, -160469.6124, -165156.8436, -155920.1008,\n",
      "         -155528.6423, -123377.1032],\n",
      "        [-212811.0237, -242289.0024, -244599.7158, -253135.9368, -254245.8201,\n",
      "         -260106.6580, -278977.4303, -300133.1737, -233188.1270, -296634.8349,\n",
      "         -336206.9223, -255056.7705],\n",
      "        [ -38012.5604,  -25698.8125,  -19726.7341,  -41618.9532,  -37417.8253,\n",
      "          -38365.7934,  -43474.4036,  -27969.5842,  -51680.2070,  -69423.5942,\n",
      "          -54485.1782,  -52442.8133],\n",
      "        [  67665.9807,   94572.8889,   92483.9785,   94085.3396,   97779.1988,\n",
      "           97786.6328,  110109.9576,   84446.8667,  112387.1716,  109663.1068,\n",
      "          128808.5674,  117029.7132],\n",
      "        [ -93288.6969,  -85835.0243,  -78063.9879, -103116.4627,  -93787.7841,\n",
      "          -84761.7328, -113664.5861,  -86416.5947,  -87268.7769,  -54014.2256,\n",
      "          -96292.7823,  -95957.6564],\n",
      "        [ 141538.1297,  116678.2747,  121636.0295,  144750.4978,  145848.6516,\n",
      "          154413.4141,  173255.7338,  144196.9405,  182917.6314,  149584.7773,\n",
      "          179906.2797,  202780.8965],\n",
      "        [ 226338.1058,  202498.3176,  233862.8769,  242160.9910,  244416.0813,\n",
      "          252284.2872,  256413.0244,  246018.4944,  243579.0317,  260269.0734,\n",
      "          236430.1026,  264508.6612]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "bn1 shape: torch.Size([13, 12]), val: tensor([[-1.3759, -1.1883, -1.2813, -1.1766, -1.2974, -1.1956, -1.0938, -1.0604,\n",
      "         -1.1682, -1.0669, -1.1669, -1.1369],\n",
      "        [ 0.4548,  0.6092,  0.5418,  0.5758,  0.5962,  0.6255,  0.6557,  0.5557,\n",
      "          0.6810,  0.9407,  0.9430,  0.6809],\n",
      "        [ 1.2944,  1.3085,  1.3341,  1.2212,  1.2503,  1.2640,  1.2351,  1.2492,\n",
      "          1.3771,  1.1237,  1.0535,  1.1171],\n",
      "        [ 0.9899,  0.8174,  0.6400,  0.6663,  0.6788,  0.6653,  0.7395,  0.8189,\n",
      "          0.7990,  0.8741,  0.6541,  0.7443],\n",
      "        [-0.2477, -0.0381,  0.0748, -0.0120,  0.0134,  0.0254, -0.0201,  0.0641,\n",
      "          0.0394, -0.0089,  0.2558, -0.0970],\n",
      "        [ 0.9361,  0.9040,  0.7982,  0.8425,  0.8513,  0.7791,  0.7601,  0.8688,\n",
      "          0.7029,  0.7147,  0.7167,  0.7012],\n",
      "        [-1.2353, -1.4348, -1.3285, -1.2691, -1.2370, -1.2799, -1.1590, -1.2385,\n",
      "         -1.3529, -1.2275, -1.1358, -1.1049],\n",
      "        [-1.6428, -1.8500, -1.8824, -1.8756, -1.8606, -1.9106, -1.9659, -2.0812,\n",
      "         -1.7666, -2.0454, -2.1371, -1.9233],\n",
      "        [-0.4879, -0.4016, -0.3794, -0.5017, -0.4878, -0.5158, -0.5512, -0.4389,\n",
      "         -0.6629, -0.7248, -0.5758, -0.6640],\n",
      "        [ 0.2103,  0.4026,  0.3707,  0.3798,  0.3682,  0.3407,  0.3714,  0.2395,\n",
      "          0.3347,  0.3161,  0.4401,  0.3892],\n",
      "        [-0.8531, -0.8038, -0.7693, -0.9011, -0.8447, -0.8076, -0.9728, -0.7916,\n",
      "         -0.8793, -0.6352, -0.8075, -0.9345],\n",
      "        [ 0.6984,  0.5505,  0.5655,  0.7089,  0.6725,  0.6969,  0.7507,  0.6000,\n",
      "          0.7635,  0.5481,  0.7233,  0.9222],\n",
      "        [ 1.2587,  1.1243,  1.3157,  1.3416,  1.2966,  1.3125,  1.2503,  1.2145,\n",
      "          1.1324,  1.1914,  1.0366,  1.3058]], dtype=torch.float64,\n",
      "       grad_fn=<NativeBatchNormBackward0>)\n",
      "relu shape: torch.Size([13, 12]), val: tensor([[-1.3759e-01, -1.1883e-01, -1.2813e-01, -1.1766e-01, -1.2974e-01,\n",
      "         -1.1956e-01, -1.0938e-01, -1.0604e-01, -1.1682e-01, -1.0669e-01,\n",
      "         -1.1669e-01, -1.1369e-01],\n",
      "        [ 4.5484e-01,  6.0924e-01,  5.4182e-01,  5.7584e-01,  5.9618e-01,\n",
      "          6.2553e-01,  6.5572e-01,  5.5568e-01,  6.8099e-01,  9.4075e-01,\n",
      "          9.4305e-01,  6.8089e-01],\n",
      "        [ 1.2944e+00,  1.3085e+00,  1.3341e+00,  1.2212e+00,  1.2503e+00,\n",
      "          1.2640e+00,  1.2351e+00,  1.2492e+00,  1.3771e+00,  1.1237e+00,\n",
      "          1.0535e+00,  1.1171e+00],\n",
      "        [ 9.8988e-01,  8.1740e-01,  6.4004e-01,  6.6627e-01,  6.7883e-01,\n",
      "          6.6528e-01,  7.3949e-01,  8.1893e-01,  7.9902e-01,  8.7410e-01,\n",
      "          6.5408e-01,  7.4428e-01],\n",
      "        [-2.4775e-02, -3.8141e-03,  7.4840e-02, -1.1999e-03,  1.3415e-02,\n",
      "          2.5442e-02, -2.0143e-03,  6.4082e-02,  3.9410e-02, -8.8927e-04,\n",
      "          2.5582e-01, -9.6999e-03],\n",
      "        [ 9.3612e-01,  9.0403e-01,  7.9816e-01,  8.4251e-01,  8.5127e-01,\n",
      "          7.7914e-01,  7.6014e-01,  8.6876e-01,  7.0288e-01,  7.1475e-01,\n",
      "          7.1673e-01,  7.0118e-01],\n",
      "        [-1.2353e-01, -1.4348e-01, -1.3285e-01, -1.2691e-01, -1.2370e-01,\n",
      "         -1.2799e-01, -1.1590e-01, -1.2385e-01, -1.3529e-01, -1.2275e-01,\n",
      "         -1.1358e-01, -1.1049e-01],\n",
      "        [-1.6428e-01, -1.8500e-01, -1.8824e-01, -1.8756e-01, -1.8606e-01,\n",
      "         -1.9106e-01, -1.9659e-01, -2.0812e-01, -1.7666e-01, -2.0454e-01,\n",
      "         -2.1371e-01, -1.9233e-01],\n",
      "        [-4.8788e-02, -4.0164e-02, -3.7935e-02, -5.0168e-02, -4.8777e-02,\n",
      "         -5.1576e-02, -5.5121e-02, -4.3890e-02, -6.6293e-02, -7.2481e-02,\n",
      "         -5.7577e-02, -6.6405e-02],\n",
      "        [ 2.1035e-01,  4.0263e-01,  3.7068e-01,  3.7979e-01,  3.6821e-01,\n",
      "          3.4068e-01,  3.7140e-01,  2.3947e-01,  3.3467e-01,  3.1606e-01,\n",
      "          4.4009e-01,  3.8922e-01],\n",
      "        [-8.5310e-02, -8.0378e-02, -7.6929e-02, -9.0115e-02, -8.4466e-02,\n",
      "         -8.0761e-02, -9.7285e-02, -7.9159e-02, -8.7932e-02, -6.3525e-02,\n",
      "         -8.0748e-02, -9.3449e-02],\n",
      "        [ 6.9843e-01,  5.5045e-01,  5.6553e-01,  7.0889e-01,  6.7255e-01,\n",
      "          6.9688e-01,  7.5072e-01,  6.0002e-01,  7.6352e-01,  5.4809e-01,\n",
      "          7.2329e-01,  9.2216e-01],\n",
      "        [ 1.2587e+00,  1.1243e+00,  1.3157e+00,  1.3416e+00,  1.2966e+00,\n",
      "          1.3125e+00,  1.2503e+00,  1.2145e+00,  1.1324e+00,  1.1914e+00,\n",
      "          1.0366e+00,  1.3058e+00]], dtype=torch.float64,\n",
      "       grad_fn=<LeakyReluBackward1>)\n",
      "pool shape: torch.Size([13, 6]), val: tensor([[-0.1282, -0.1229, -0.1247, -0.1077, -0.1118, -0.1152],\n",
      "        [ 0.5320,  0.5588,  0.6109,  0.6057,  0.8109,  0.8120],\n",
      "        [ 1.3015,  1.2777,  1.2572,  1.2422,  1.2504,  1.0853],\n",
      "        [ 0.9036,  0.6532,  0.6721,  0.7792,  0.8366,  0.6992],\n",
      "        [-0.0143,  0.0368,  0.0194,  0.0310,  0.0193,  0.1231],\n",
      "        [ 0.9201,  0.8203,  0.8152,  0.8144,  0.7088,  0.7090],\n",
      "        [-0.1335, -0.1299, -0.1258, -0.1199, -0.1290, -0.1120],\n",
      "        [-0.1746, -0.1879, -0.1886, -0.2024, -0.1906, -0.2030],\n",
      "        [-0.0445, -0.0441, -0.0502, -0.0495, -0.0694, -0.0620],\n",
      "        [ 0.3065,  0.3752,  0.3544,  0.3054,  0.3254,  0.4147],\n",
      "        [-0.0828, -0.0835, -0.0826, -0.0882, -0.0757, -0.0871],\n",
      "        [ 0.6244,  0.6372,  0.6847,  0.6754,  0.6558,  0.8227],\n",
      "        [ 1.1915,  1.3287,  1.3046,  1.2324,  1.1619,  1.1712]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([13, 6]), val: tensor([[-0.0111, -0.0358, -0.0355, -0.0259, -0.0481, -0.0925],\n",
      "        [ 0.2196,  0.1670,  0.1513,  0.1716,  0.1388,  0.0537],\n",
      "        [-0.3708, -0.3508, -0.3491, -0.3415, -0.2835, -0.2719],\n",
      "        [ 0.0448,  0.1007,  0.0793,  0.0462,  0.0364,  0.0158],\n",
      "        [ 0.6069,  0.5535,  0.5623,  0.5848,  0.6188,  0.5725],\n",
      "        [-0.3484, -0.2930, -0.3045, -0.3270, -0.3547, -0.3345],\n",
      "        [-0.0703, -0.1728, -0.1650, -0.1134, -0.1214, -0.1008],\n",
      "        [-0.9741, -0.9694, -0.9711, -0.9599, -0.9885, -0.9895],\n",
      "        [-0.1724, -0.2132, -0.2272, -0.2143, -0.2518, -0.2796],\n",
      "        [-0.5534, -0.5759, -0.5810, -0.5606, -0.5871, -0.5766],\n",
      "        [-0.3136, -0.2804, -0.2461, -0.2614, -0.1909, -0.1238],\n",
      "        [ 0.3779,  0.3328,  0.3203,  0.3376,  0.3322,  0.2838],\n",
      "        [ 0.0829,  0.1427,  0.1242,  0.0842,  0.0665,  0.0680]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([13, 6]), val: tensor([[-0.0011, -0.0036, -0.0036, -0.0026, -0.0048, -0.0092],\n",
      "        [ 0.2196,  0.1670,  0.1513,  0.1716,  0.1388,  0.0537],\n",
      "        [-0.0371, -0.0351, -0.0349, -0.0342, -0.0283, -0.0272],\n",
      "        [ 0.0448,  0.1007,  0.0793,  0.0462,  0.0364,  0.0158],\n",
      "        [ 0.6069,  0.5535,  0.5623,  0.5848,  0.6188,  0.5725],\n",
      "        [-0.0348, -0.0293, -0.0304, -0.0327, -0.0355, -0.0335],\n",
      "        [-0.0070, -0.0173, -0.0165, -0.0113, -0.0121, -0.0101],\n",
      "        [-0.0974, -0.0969, -0.0971, -0.0960, -0.0989, -0.0990],\n",
      "        [-0.0172, -0.0213, -0.0227, -0.0214, -0.0252, -0.0280],\n",
      "        [-0.0553, -0.0576, -0.0581, -0.0561, -0.0587, -0.0577],\n",
      "        [-0.0314, -0.0280, -0.0246, -0.0261, -0.0191, -0.0124],\n",
      "        [ 0.3779,  0.3328,  0.3203,  0.3376,  0.3322,  0.2838],\n",
      "        [ 0.0829,  0.1427,  0.1242,  0.0842,  0.0665,  0.0680]],\n",
      "       dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([13, 6]), val: tensor([[-0.0011, -0.0036, -0.0036, -0.0026, -0.0048, -0.0092],\n",
      "        [ 0.2196,  0.1670,  0.1513,  0.1716,  0.1388,  0.0537],\n",
      "        [-0.0371, -0.0351, -0.0349, -0.0342, -0.0283, -0.0272],\n",
      "        [ 0.0448,  0.1007,  0.0793,  0.0462,  0.0364,  0.0158],\n",
      "        [ 0.6069,  0.5535,  0.5623,  0.5848,  0.6188,  0.5725],\n",
      "        [-0.0348, -0.0293, -0.0304, -0.0327, -0.0355, -0.0335],\n",
      "        [-0.0070, -0.0173, -0.0165, -0.0113, -0.0121, -0.0101],\n",
      "        [-0.0974, -0.0969, -0.0971, -0.0960, -0.0989, -0.0990],\n",
      "        [-0.0172, -0.0213, -0.0227, -0.0214, -0.0252, -0.0280],\n",
      "        [-0.0553, -0.0576, -0.0581, -0.0561, -0.0587, -0.0577],\n",
      "        [-0.0314, -0.0280, -0.0246, -0.0261, -0.0191, -0.0124],\n",
      "        [ 0.3779,  0.3328,  0.3203,  0.3376,  0.3322,  0.2838],\n",
      "        [ 0.0829,  0.1427,  0.1242,  0.0842,  0.0665,  0.0680]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([13, 6]), val: tensor([[-0.0011, -0.0036, -0.0036, -0.0026, -0.0048, -0.0092],\n",
      "        [ 0.2196,  0.1670,  0.1513,  0.1716,  0.1388,  0.0537],\n",
      "        [-0.0371, -0.0351, -0.0349, -0.0342, -0.0283, -0.0272],\n",
      "        [ 0.0448,  0.1007,  0.0793,  0.0462,  0.0364,  0.0158],\n",
      "        [ 0.6069,  0.5535,  0.5623,  0.5848,  0.6188,  0.5725],\n",
      "        [-0.0348, -0.0293, -0.0304, -0.0327, -0.0355, -0.0335],\n",
      "        [-0.0070, -0.0173, -0.0165, -0.0113, -0.0121, -0.0101],\n",
      "        [-0.0974, -0.0969, -0.0971, -0.0960, -0.0989, -0.0990],\n",
      "        [-0.0172, -0.0213, -0.0227, -0.0214, -0.0252, -0.0280],\n",
      "        [-0.0553, -0.0576, -0.0581, -0.0561, -0.0587, -0.0577],\n",
      "        [-0.0314, -0.0280, -0.0246, -0.0261, -0.0191, -0.0124],\n",
      "        [ 0.3779,  0.3328,  0.3203,  0.3376,  0.3322,  0.2838],\n",
      "        [ 0.0829,  0.1427,  0.1242,  0.0842,  0.0665,  0.0680]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([13, 1]), val: tensor([[-0.0691],\n",
      "        [-0.0363],\n",
      "        [-0.0757],\n",
      "        [-0.0475],\n",
      "        [ 0.0538],\n",
      "        [-0.0748],\n",
      "        [-0.0719],\n",
      "        [-0.0896],\n",
      "        [-0.0730],\n",
      "        [-0.0810],\n",
      "        [-0.0732],\n",
      "        [ 0.0021],\n",
      "        [-0.0368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 78188710340.10406, val: 78188710340.10406\n",
      "conv1 shape: torch.Size([13, 12]), val: tensor([[-137100.1066, -143953.9347, -139632.5075, -119813.7203, -136981.2243,\n",
      "         -154574.4553, -166660.4713, -160263.0558, -205838.8923, -146603.8366,\n",
      "         -140269.6560, -173251.7152],\n",
      "        [ 107048.7432,  106238.7798,  116067.1638,  129194.7929,  123734.6920,\n",
      "          121029.4408,  141166.4819,  118874.5052,   48723.8840,   92914.2259,\n",
      "          119458.2274,   65572.8259],\n",
      "        [ 204618.2972,  204706.7688,  217689.7429,  213471.7356,  186008.0597,\n",
      "          207804.1411,  192937.8477,  176606.7779,  221090.4232,  228343.0775,\n",
      "          218996.4643,  254291.0214],\n",
      "        [ 105656.9354,  103335.6952,   89204.5230,  118054.1213,  104574.0402,\n",
      "          130392.3827,  106868.2641,  129404.4076,   97714.5578,  133219.9059,\n",
      "          109935.7116,  120290.6828],\n",
      "        [  85644.9719,   86893.6480,   69861.1259,   72477.9467,   73605.5710,\n",
      "           90577.2430,   29738.4451,   54139.2100,  124443.1120,  103452.2938,\n",
      "          106777.4065,   94818.8493],\n",
      "        [ 182428.5330,  182318.0333,  192687.9535,  195783.3187,  202293.3381,\n",
      "          198611.7798,  231080.9479,  204156.7463,  225499.0143,  218990.8636,\n",
      "          158178.6462,  189847.1180],\n",
      "        [-163775.5041, -162649.3427, -166271.1654, -164774.3133, -137731.6210,\n",
      "         -139812.6149, -145785.3821, -140770.7687, -171713.4276, -140783.5004,\n",
      "         -150217.6049, -104548.7142],\n",
      "        [-224218.1581, -220915.0525, -227485.5970, -203579.3475, -233731.2975,\n",
      "         -195549.5746, -197857.5323, -246388.3442, -278502.8422, -295450.0968,\n",
      "         -283092.3375, -324059.0141],\n",
      "        [ -63017.1861,  -63688.6207,  -65989.2171,  -74704.1500,  -54570.6994,\n",
      "          -59210.3422,  -55330.6113,  -70372.1922,  -20060.5289,  -29074.1632,\n",
      "          -75821.8480,  -16948.1267],\n",
      "        [  78346.5680,   81722.3412,   77898.3541,   82087.7522,   78775.0565,\n",
      "           96157.5765,   74302.4931,  114617.4392,  109524.7500,   82930.1773,\n",
      "          130771.8156,  119852.4371],\n",
      "        [-128552.9991, -129831.4407, -124606.3778, -121381.3250, -124318.7289,\n",
      "          -66275.7287, -111305.5771,  -84396.8976, -173207.5012, -143371.9851,\n",
      "         -107093.0262, -180370.9455],\n",
      "        [ 130650.2450,  133691.8799,  119532.1254,  138651.2886,  154025.5066,\n",
      "          184534.1558,  157521.9227,  173296.2330,  183732.3063,  195728.7612,\n",
      "          202567.4896,  146321.9187],\n",
      "        [ 282041.9393,  284879.5044,  297678.7833,  304247.1976,  295209.5525,\n",
      "          287159.9906,  281129.9738,  297931.3154,  294767.4953,  347576.7650,\n",
      "          273081.4193,  262926.7120]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "bn1 shape: torch.Size([13, 12]), val: tensor([[-1.1205, -1.1612, -1.1095, -1.0486, -1.1453, -1.3723, -1.3394, -1.2754,\n",
      "         -1.3347, -1.0917, -1.1054, -1.1927],\n",
      "        [ 0.4657,  0.4569,  0.5139,  0.5470,  0.5339,  0.4418,  0.6425,  0.4709,\n",
      "          0.0755,  0.2394,  0.4583,  0.1752],\n",
      "        [ 1.0996,  1.0937,  1.1590,  1.0871,  0.9350,  1.0130,  0.9759,  0.8321,\n",
      "          1.0304,  0.9921,  1.0576,  1.2562],\n",
      "        [ 0.4567,  0.4381,  0.3433,  0.4757,  0.4105,  0.5034,  0.4217,  0.5368,\n",
      "          0.3469,  0.4634,  0.4010,  0.4886],\n",
      "        [ 0.3266,  0.3318,  0.2205,  0.1836,  0.2110,  0.2414, -0.0749,  0.0659,\n",
      "          0.4950,  0.2980,  0.3820,  0.3427],\n",
      "        [ 0.9554,  0.9489,  1.0003,  0.9737,  1.0399,  0.9525,  1.2214,  1.0044,\n",
      "          1.0548,  0.9401,  0.6915,  0.8870],\n",
      "        [-1.2938, -1.2822, -1.2786, -1.3367, -1.1501, -1.2752, -1.2050, -1.1534,\n",
      "         -1.1457, -1.0594, -1.1653, -0.7992],\n",
      "        [-1.6865, -1.6590, -1.6672, -1.5853, -1.7684, -1.6420, -1.5403, -1.8141,\n",
      "         -1.7373, -1.9190, -1.9653, -2.0565],\n",
      "        [-0.6392, -0.6421, -0.6420, -0.7595, -0.6145, -0.7446, -0.6226, -0.7130,\n",
      "         -0.3055, -0.4386, -0.7174, -0.2974],\n",
      "        [ 0.2792,  0.2983,  0.2716,  0.2452,  0.2443,  0.2781,  0.2120,  0.4443,\n",
      "          0.4124,  0.1839,  0.5265,  0.4861],\n",
      "        [-1.0650, -1.0699, -1.0141, -1.0586, -1.0637, -0.7911, -0.9830, -0.8007,\n",
      "         -1.1539, -1.0738, -0.9056, -1.2335],\n",
      "        [ 0.6190,  0.6344,  0.5359,  0.6076,  0.7290,  0.8598,  0.7478,  0.8113,\n",
      "          0.8235,  0.8108,  0.9587,  0.6377],\n",
      "        [ 1.6026,  1.6122,  1.6669,  1.6687,  1.6383,  1.5353,  1.5437,  1.5910,\n",
      "          1.4386,  1.6547,  1.3833,  1.3056]], dtype=torch.float64,\n",
      "       grad_fn=<NativeBatchNormBackward0>)\n",
      "relu shape: torch.Size([13, 12]), val: tensor([[-0.1120, -0.1161, -0.1109, -0.1049, -0.1145, -0.1372, -0.1339, -0.1275,\n",
      "         -0.1335, -0.1092, -0.1105, -0.1193],\n",
      "        [ 0.4657,  0.4569,  0.5139,  0.5470,  0.5339,  0.4418,  0.6425,  0.4709,\n",
      "          0.0755,  0.2394,  0.4583,  0.1752],\n",
      "        [ 1.0996,  1.0937,  1.1590,  1.0871,  0.9350,  1.0130,  0.9759,  0.8321,\n",
      "          1.0304,  0.9921,  1.0576,  1.2562],\n",
      "        [ 0.4567,  0.4381,  0.3433,  0.4757,  0.4105,  0.5034,  0.4217,  0.5368,\n",
      "          0.3469,  0.4634,  0.4010,  0.4886],\n",
      "        [ 0.3266,  0.3318,  0.2205,  0.1836,  0.2110,  0.2414, -0.0075,  0.0659,\n",
      "          0.4950,  0.2980,  0.3820,  0.3427],\n",
      "        [ 0.9554,  0.9489,  1.0003,  0.9737,  1.0399,  0.9525,  1.2214,  1.0044,\n",
      "          1.0548,  0.9401,  0.6915,  0.8870],\n",
      "        [-0.1294, -0.1282, -0.1279, -0.1337, -0.1150, -0.1275, -0.1205, -0.1153,\n",
      "         -0.1146, -0.1059, -0.1165, -0.0799],\n",
      "        [-0.1686, -0.1659, -0.1667, -0.1585, -0.1768, -0.1642, -0.1540, -0.1814,\n",
      "         -0.1737, -0.1919, -0.1965, -0.2057],\n",
      "        [-0.0639, -0.0642, -0.0642, -0.0760, -0.0615, -0.0745, -0.0623, -0.0713,\n",
      "         -0.0306, -0.0439, -0.0717, -0.0297],\n",
      "        [ 0.2792,  0.2983,  0.2716,  0.2452,  0.2443,  0.2781,  0.2120,  0.4443,\n",
      "          0.4124,  0.1839,  0.5265,  0.4861],\n",
      "        [-0.1065, -0.1070, -0.1014, -0.1059, -0.1064, -0.0791, -0.0983, -0.0801,\n",
      "         -0.1154, -0.1074, -0.0906, -0.1233],\n",
      "        [ 0.6190,  0.6344,  0.5359,  0.6076,  0.7290,  0.8598,  0.7478,  0.8113,\n",
      "          0.8235,  0.8108,  0.9587,  0.6377],\n",
      "        [ 1.6026,  1.6122,  1.6669,  1.6687,  1.6383,  1.5353,  1.5437,  1.5910,\n",
      "          1.4386,  1.6547,  1.3833,  1.3056]], dtype=torch.float64,\n",
      "       grad_fn=<LeakyReluBackward1>)\n",
      "pool shape: torch.Size([13, 6]), val: tensor([[-0.1141, -0.1079, -0.1259, -0.1307, -0.1213, -0.1149],\n",
      "        [ 0.4613,  0.5305,  0.4879,  0.5567,  0.1575,  0.3168],\n",
      "        [ 1.0967,  1.1231,  0.9740,  0.9040,  1.0112,  1.1569],\n",
      "        [ 0.4474,  0.4095,  0.4570,  0.4792,  0.4052,  0.4448],\n",
      "        [ 0.3292,  0.2021,  0.2262,  0.0292,  0.3965,  0.3624],\n",
      "        [ 0.9522,  0.9870,  0.9962,  1.1129,  0.9975,  0.7893],\n",
      "        [-0.1288, -0.1308, -0.1213, -0.1179, -0.1103, -0.0982],\n",
      "        [-0.1673, -0.1626, -0.1705, -0.1677, -0.1828, -0.2011],\n",
      "        [-0.0641, -0.0701, -0.0680, -0.0668, -0.0372, -0.0507],\n",
      "        [ 0.2888,  0.2584,  0.2612,  0.3281,  0.2981,  0.5063],\n",
      "        [-0.1067, -0.1036, -0.0927, -0.0892, -0.1114, -0.1070],\n",
      "        [ 0.6267,  0.5718,  0.7944,  0.7796,  0.8171,  0.7982],\n",
      "        [ 1.6074,  1.6678,  1.5868,  1.5674,  1.5466,  1.3444]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([13, 6]), val: tensor([[-0.0027,  0.0457,  0.0157,  0.0685, -0.0452, -0.1393],\n",
      "        [ 0.1935,  0.2027,  0.1379,  0.1102,  0.1707,  0.1027],\n",
      "        [-0.3273, -0.3443, -0.3688, -0.3998, -0.4035, -0.3476],\n",
      "        [ 0.1096,  0.1384,  0.0498,  0.0636,  0.0581,  0.0968],\n",
      "        [ 0.5458,  0.5571,  0.5285,  0.5335,  0.4623,  0.4657],\n",
      "        [-0.2644, -0.2795, -0.2959, -0.3495, -0.2148, -0.1957],\n",
      "        [-0.1935, -0.2433, -0.1784, -0.1678, -0.1312, -0.1339],\n",
      "        [-0.9907, -0.9908, -0.9648, -0.9680, -0.9193, -0.9633],\n",
      "        [-0.1685, -0.1824, -0.2010, -0.2126, -0.1418, -0.1858],\n",
      "        [-0.5343, -0.5462, -0.5231, -0.5121, -0.4927, -0.5502],\n",
      "        [-0.3531, -0.3296, -0.2592, -0.1936, -0.3886, -0.3289],\n",
      "        [ 0.3271,  0.3071,  0.2609,  0.2246,  0.3159,  0.3502],\n",
      "        [ 0.1941,  0.1982,  0.1529,  0.1271,  0.1934,  0.1950]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([13, 6]), val: tensor([[-2.6671e-04,  4.5652e-02,  1.5731e-02,  6.8534e-02, -4.5200e-03,\n",
      "         -1.3928e-02],\n",
      "        [ 1.9350e-01,  2.0268e-01,  1.3790e-01,  1.1017e-01,  1.7071e-01,\n",
      "          1.0275e-01],\n",
      "        [-3.2732e-02, -3.4427e-02, -3.6877e-02, -3.9979e-02, -4.0348e-02,\n",
      "         -3.4757e-02],\n",
      "        [ 1.0962e-01,  1.3844e-01,  4.9819e-02,  6.3646e-02,  5.8052e-02,\n",
      "          9.6758e-02],\n",
      "        [ 5.4578e-01,  5.5707e-01,  5.2847e-01,  5.3353e-01,  4.6233e-01,\n",
      "          4.6574e-01],\n",
      "        [-2.6437e-02, -2.7947e-02, -2.9586e-02, -3.4949e-02, -2.1476e-02,\n",
      "         -1.9566e-02],\n",
      "        [-1.9350e-02, -2.4326e-02, -1.7843e-02, -1.6784e-02, -1.3116e-02,\n",
      "         -1.3389e-02],\n",
      "        [-9.9074e-02, -9.9080e-02, -9.6477e-02, -9.6799e-02, -9.1932e-02,\n",
      "         -9.6332e-02],\n",
      "        [-1.6852e-02, -1.8240e-02, -2.0097e-02, -2.1261e-02, -1.4178e-02,\n",
      "         -1.8579e-02],\n",
      "        [-5.3425e-02, -5.4615e-02, -5.2314e-02, -5.1213e-02, -4.9274e-02,\n",
      "         -5.5017e-02],\n",
      "        [-3.5306e-02, -3.2960e-02, -2.5924e-02, -1.9364e-02, -3.8857e-02,\n",
      "         -3.2889e-02],\n",
      "        [ 3.2713e-01,  3.0712e-01,  2.6094e-01,  2.2465e-01,  3.1588e-01,\n",
      "          3.5021e-01],\n",
      "        [ 1.9408e-01,  1.9816e-01,  1.5291e-01,  1.2712e-01,  1.9338e-01,\n",
      "          1.9501e-01]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([13, 6]), val: tensor([[-2.6671e-04,  4.5652e-02,  1.5731e-02,  6.8534e-02, -4.5200e-03,\n",
      "         -1.3928e-02],\n",
      "        [ 1.9350e-01,  2.0268e-01,  1.3790e-01,  1.1017e-01,  1.7071e-01,\n",
      "          1.0275e-01],\n",
      "        [-3.2732e-02, -3.4427e-02, -3.6877e-02, -3.9979e-02, -4.0348e-02,\n",
      "         -3.4757e-02],\n",
      "        [ 1.0962e-01,  1.3844e-01,  4.9819e-02,  6.3646e-02,  5.8052e-02,\n",
      "          9.6758e-02],\n",
      "        [ 5.4578e-01,  5.5707e-01,  5.2847e-01,  5.3353e-01,  4.6233e-01,\n",
      "          4.6574e-01],\n",
      "        [-2.6437e-02, -2.7947e-02, -2.9586e-02, -3.4949e-02, -2.1476e-02,\n",
      "         -1.9566e-02],\n",
      "        [-1.9350e-02, -2.4326e-02, -1.7843e-02, -1.6784e-02, -1.3116e-02,\n",
      "         -1.3389e-02],\n",
      "        [-9.9074e-02, -9.9080e-02, -9.6477e-02, -9.6799e-02, -9.1932e-02,\n",
      "         -9.6332e-02],\n",
      "        [-1.6852e-02, -1.8240e-02, -2.0097e-02, -2.1261e-02, -1.4178e-02,\n",
      "         -1.8579e-02],\n",
      "        [-5.3425e-02, -5.4615e-02, -5.2314e-02, -5.1213e-02, -4.9274e-02,\n",
      "         -5.5017e-02],\n",
      "        [-3.5306e-02, -3.2960e-02, -2.5924e-02, -1.9364e-02, -3.8857e-02,\n",
      "         -3.2889e-02],\n",
      "        [ 3.2713e-01,  3.0712e-01,  2.6094e-01,  2.2465e-01,  3.1588e-01,\n",
      "          3.5021e-01],\n",
      "        [ 1.9408e-01,  1.9816e-01,  1.5291e-01,  1.2712e-01,  1.9338e-01,\n",
      "          1.9501e-01]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([13, 6]), val: tensor([[-2.6671e-04,  4.5652e-02,  1.5731e-02,  6.8534e-02, -4.5200e-03,\n",
      "         -1.3928e-02],\n",
      "        [ 1.9350e-01,  2.0268e-01,  1.3790e-01,  1.1017e-01,  1.7071e-01,\n",
      "          1.0275e-01],\n",
      "        [-3.2732e-02, -3.4427e-02, -3.6877e-02, -3.9979e-02, -4.0348e-02,\n",
      "         -3.4757e-02],\n",
      "        [ 1.0962e-01,  1.3844e-01,  4.9819e-02,  6.3646e-02,  5.8052e-02,\n",
      "          9.6758e-02],\n",
      "        [ 5.4578e-01,  5.5707e-01,  5.2847e-01,  5.3353e-01,  4.6233e-01,\n",
      "          4.6574e-01],\n",
      "        [-2.6437e-02, -2.7947e-02, -2.9586e-02, -3.4949e-02, -2.1476e-02,\n",
      "         -1.9566e-02],\n",
      "        [-1.9350e-02, -2.4326e-02, -1.7843e-02, -1.6784e-02, -1.3116e-02,\n",
      "         -1.3389e-02],\n",
      "        [-9.9074e-02, -9.9080e-02, -9.6477e-02, -9.6799e-02, -9.1932e-02,\n",
      "         -9.6332e-02],\n",
      "        [-1.6852e-02, -1.8240e-02, -2.0097e-02, -2.1261e-02, -1.4178e-02,\n",
      "         -1.8579e-02],\n",
      "        [-5.3425e-02, -5.4615e-02, -5.2314e-02, -5.1213e-02, -4.9274e-02,\n",
      "         -5.5017e-02],\n",
      "        [-3.5306e-02, -3.2960e-02, -2.5924e-02, -1.9364e-02, -3.8857e-02,\n",
      "         -3.2889e-02],\n",
      "        [ 3.2713e-01,  3.0712e-01,  2.6094e-01,  2.2465e-01,  3.1588e-01,\n",
      "          3.5021e-01],\n",
      "        [ 1.9408e-01,  1.9816e-01,  1.5291e-01,  1.2712e-01,  1.9338e-01,\n",
      "          1.9501e-01]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([13, 1]), val: tensor([[-0.0904],\n",
      "        [-0.0200],\n",
      "        [-0.0744],\n",
      "        [-0.0547],\n",
      "        [ 0.0403],\n",
      "        [-0.0713],\n",
      "        [-0.0721],\n",
      "        [-0.0888],\n",
      "        [-0.0712],\n",
      "        [-0.0798],\n",
      "        [-0.0783],\n",
      "        [ 0.0156],\n",
      "        [-0.0172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 74902402135.19188, val: 74902402135.19188\n",
      "conv1 shape: torch.Size([13, 12]), val: tensor([[-136246.7894, -141922.4431, -151293.8826, -141516.5222, -144189.0989,\n",
      "         -136703.6052, -123468.6834, -122752.5264, -140489.1277, -157341.2000,\n",
      "         -137163.3628, -146734.1821],\n",
      "        [ 124910.8279,  132863.2879,  124302.5784,  125674.2203,  106796.2704,\n",
      "          120981.3382,  126734.4269,  136071.4273,  142082.5648,  117720.9990,\n",
      "          126395.9883,  106907.0267],\n",
      "        [ 215996.8421,  222640.8721,  225500.9291,  227370.5054,  239713.2719,\n",
      "          243090.2915,  249450.4821,  246999.5977,  237609.5398,  233752.6737,\n",
      "          240768.5576,  248295.4000],\n",
      "        [ 119399.0937,  133353.1573,  125316.2576,  131762.0997,  131344.0549,\n",
      "          142707.4396,  131705.4547,  142083.5883,  152505.1003,  165667.1783,\n",
      "          149412.4530,  150448.0047],\n",
      "        [  43260.1402,   38050.5127,   35438.3087,   30786.1041,   33673.1738,\n",
      "           44043.5984,   38228.9221,   32303.8787,   51356.8197,   60144.1852,\n",
      "           84790.1407,   69341.1821],\n",
      "        [ 142985.3677,  138657.7463,  140227.7634,  141035.7446,  146404.3829,\n",
      "          135690.3820,  144806.6545,  140508.2693,  137710.7965,  144117.6234,\n",
      "          180815.3940,  181155.1857],\n",
      "        [-122848.8077, -119131.9381, -129257.6222, -134886.4457, -149486.5850,\n",
      "         -140400.8431, -137456.1256, -144382.1839, -142039.7194, -172447.2732,\n",
      "         -221184.3991, -191048.1397],\n",
      "        [-274569.6870, -270811.8715, -259481.8639, -267014.1539, -266444.2577,\n",
      "         -265061.4000, -271679.2314, -258678.5599, -260354.8768, -221443.3561,\n",
      "         -238528.4610, -230970.3409],\n",
      "        [ -68680.4961,  -90066.1951,  -82877.1522,  -85370.3463,  -77399.8328,\n",
      "          -62150.6642,  -53755.1515,  -75209.5789,  -68735.5286,  -42791.0434,\n",
      "          -26691.2466,  -26820.8942],\n",
      "        [  88356.3064,   87678.8696,   75705.9025,   81685.9783,   81539.1747,\n",
      "           87772.7839,   78331.4690,   89187.9579,   94069.6195,   84582.9064,\n",
      "           74198.1997,   73259.0362],\n",
      "        [ -99413.6565, -102850.4631, -107773.8943, -114975.4634, -117866.5140,\n",
      "         -105092.8075,  -97050.9958,  -75164.3923,  -76513.3096,  -45217.8215,\n",
      "          -68720.6449,  -78977.4035],\n",
      "        [ 122815.7522,  128356.7110,  117853.5392,  115343.7154,  117102.1169,\n",
      "          127744.4261,  122708.0994,  119713.8205,  137258.2832,  146905.3969,\n",
      "          134522.2436,  133453.9648],\n",
      "        [ 201521.2941,  199965.9704,  194608.5739,  197686.3463,  205960.2388,\n",
      "          193062.8088,  203497.1271,  205211.2447,  200710.6696,  201998.2737,\n",
      "          177709.2024,  193537.6499]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "bn1 shape: torch.Size([13, 12]), val: tensor([[-1.1235, -1.1411, -1.1956, -1.1107, -1.1080, -1.1180, -1.0427, -1.0566,\n",
      "         -1.1779, -1.3501, -1.1367, -1.2213],\n",
      "        [ 0.6684,  0.7102,  0.6872,  0.6860,  0.5491,  0.6136,  0.6387,  0.6933,\n",
      "          0.7104,  0.5349,  0.5870,  0.4641],\n",
      "        [ 1.2933,  1.3151,  1.3786,  1.3698,  1.4267,  1.4342,  1.4633,  1.4432,\n",
      "          1.3487,  1.3300,  1.3350,  1.4035],\n",
      "        [ 0.6306,  0.7135,  0.6941,  0.7269,  0.7112,  0.7596,  0.6721,  0.7339,\n",
      "          0.7800,  0.8635,  0.7375,  0.7534],\n",
      "        [ 0.1081,  0.0715,  0.0801,  0.0479,  0.0663,  0.0966,  0.0439, -0.0083,\n",
      "          0.1041,  0.1403,  0.3149,  0.2145],\n",
      "        [ 0.7924,  0.7493,  0.7960,  0.7893,  0.8106,  0.7125,  0.7601,  0.7233,\n",
      "          0.6811,  0.7158,  0.9429,  0.9574],\n",
      "        [-1.0316, -0.9875, -1.0451, -1.0661, -1.1430, -1.1429, -1.1367, -1.2028,\n",
      "         -1.1883, -1.4536, -1.6861, -1.5157],\n",
      "        [-2.0726, -2.0094, -1.9348, -1.9546, -1.9152, -1.9806, -2.0387, -1.9756,\n",
      "         -1.9790, -1.7893, -1.7996, -1.7810],\n",
      "        [-0.6599, -0.7917, -0.7282, -0.7332, -0.6670, -0.6170, -0.5742, -0.7352,\n",
      "         -0.6984, -0.5651, -0.4142, -0.4245],\n",
      "        [ 0.4176,  0.4058,  0.3552,  0.3902,  0.3824,  0.3905,  0.3134,  0.3763,\n",
      "          0.3895,  0.3078,  0.2456,  0.2405],\n",
      "        [-0.8708, -0.8778, -0.8983, -0.9322, -0.9342, -0.9056, -0.8652, -0.7349,\n",
      "         -0.7504, -0.5817, -0.6890, -0.7711],\n",
      "        [ 0.6540,  0.6799,  0.6432,  0.6165,  0.6172,  0.6591,  0.6116,  0.5827,\n",
      "          0.6781,  0.7349,  0.6401,  0.6405],\n",
      "        [ 1.1940,  1.1623,  1.1675,  1.1702,  1.2038,  1.0980,  1.1545,  1.1607,\n",
      "          1.1021,  1.1124,  0.9226,  1.0397]], dtype=torch.float64,\n",
      "       grad_fn=<NativeBatchNormBackward0>)\n",
      "relu shape: torch.Size([13, 12]), val: tensor([[-1.1235e-01, -1.1411e-01, -1.1956e-01, -1.1107e-01, -1.1080e-01,\n",
      "         -1.1180e-01, -1.0427e-01, -1.0566e-01, -1.1779e-01, -1.3501e-01,\n",
      "         -1.1367e-01, -1.2213e-01],\n",
      "        [ 6.6837e-01,  7.1023e-01,  6.8722e-01,  6.8599e-01,  5.4912e-01,\n",
      "          6.1364e-01,  6.3867e-01,  6.9326e-01,  7.1036e-01,  5.3490e-01,\n",
      "          5.8699e-01,  4.6407e-01],\n",
      "        [ 1.2933e+00,  1.3151e+00,  1.3786e+00,  1.3698e+00,  1.4267e+00,\n",
      "          1.4342e+00,  1.4633e+00,  1.4432e+00,  1.3487e+00,  1.3300e+00,\n",
      "          1.3350e+00,  1.4035e+00],\n",
      "        [ 6.3055e-01,  7.1353e-01,  6.9414e-01,  7.2692e-01,  7.1119e-01,\n",
      "          7.5964e-01,  6.7207e-01,  7.3390e-01,  7.8001e-01,  8.6347e-01,\n",
      "          7.3752e-01,  7.5338e-01],\n",
      "        [ 1.0814e-01,  7.1453e-02,  8.0106e-02,  4.7918e-02,  6.6331e-02,\n",
      "          9.6606e-02,  4.3900e-02, -8.2908e-04,  1.0408e-01,  1.4034e-01,\n",
      "          3.1489e-01,  2.1446e-01],\n",
      "        [ 7.9238e-01,  7.4927e-01,  7.9602e-01,  7.8928e-01,  8.1062e-01,\n",
      "          7.1248e-01,  7.6012e-01,  7.2325e-01,  6.8114e-01,  7.1579e-01,\n",
      "          9.4289e-01,  9.5742e-01],\n",
      "        [-1.0316e-01, -9.8753e-02, -1.0451e-01, -1.0661e-01, -1.1430e-01,\n",
      "         -1.1429e-01, -1.1367e-01, -1.2028e-01, -1.1883e-01, -1.4536e-01,\n",
      "         -1.6861e-01, -1.5157e-01],\n",
      "        [-2.0726e-01, -2.0094e-01, -1.9348e-01, -1.9546e-01, -1.9152e-01,\n",
      "         -1.9806e-01, -2.0387e-01, -1.9756e-01, -1.9790e-01, -1.7893e-01,\n",
      "         -1.7996e-01, -1.7810e-01],\n",
      "        [-6.5991e-02, -7.9171e-02, -7.2822e-02, -7.3317e-02, -6.6701e-02,\n",
      "         -6.1703e-02, -5.7425e-02, -7.3516e-02, -6.9845e-02, -5.6506e-02,\n",
      "         -4.1418e-02, -4.2450e-02],\n",
      "        [ 4.1756e-01,  4.0581e-01,  3.5521e-01,  3.9019e-01,  3.8236e-01,\n",
      "          3.9047e-01,  3.1339e-01,  3.7629e-01,  3.8951e-01,  3.0781e-01,\n",
      "          2.4562e-01,  2.4049e-01],\n",
      "        [-8.7078e-02, -8.7784e-02, -8.9831e-02, -9.3225e-02, -9.3419e-02,\n",
      "         -9.0561e-02, -8.6520e-02, -7.3486e-02, -7.5042e-02, -5.8169e-02,\n",
      "         -6.8905e-02, -7.7105e-02],\n",
      "        [ 6.5399e-01,  6.7987e-01,  6.4316e-01,  6.1652e-01,  6.1716e-01,\n",
      "          6.5909e-01,  6.1161e-01,  5.8267e-01,  6.7812e-01,  7.3490e-01,\n",
      "          6.4014e-01,  6.4046e-01],\n",
      "        [ 1.1940e+00,  1.1623e+00,  1.1675e+00,  1.1702e+00,  1.2038e+00,\n",
      "          1.0980e+00,  1.1545e+00,  1.1607e+00,  1.1021e+00,  1.1124e+00,\n",
      "          9.2258e-01,  1.0397e+00]], dtype=torch.float64,\n",
      "       grad_fn=<LeakyReluBackward1>)\n",
      "pool shape: torch.Size([13, 6]), val: tensor([[-0.1132, -0.1153, -0.1113, -0.1050, -0.1264, -0.1179],\n",
      "        [ 0.6893,  0.6866,  0.5814,  0.6660,  0.6226,  0.5255],\n",
      "        [ 1.3042,  1.3742,  1.4305,  1.4533,  1.3394,  1.3693],\n",
      "        [ 0.6720,  0.7105,  0.7354,  0.7030,  0.8217,  0.7454],\n",
      "        [ 0.0898,  0.0640,  0.0815,  0.0215,  0.1222,  0.2647],\n",
      "        [ 0.7708,  0.7927,  0.7616,  0.7417,  0.6985,  0.9502],\n",
      "        [-0.1010, -0.1056, -0.1143, -0.1170, -0.1321, -0.1601],\n",
      "        [-0.2041, -0.1945, -0.1948, -0.2007, -0.1884, -0.1790],\n",
      "        [-0.0726, -0.0731, -0.0642, -0.0655, -0.0632, -0.0419],\n",
      "        [ 0.4117,  0.3727,  0.3864,  0.3448,  0.3487,  0.2431],\n",
      "        [-0.0874, -0.0915, -0.0920, -0.0800, -0.0666, -0.0730],\n",
      "        [ 0.6669,  0.6298,  0.6381,  0.5971,  0.7065,  0.6403],\n",
      "        [ 1.1782,  1.1689,  1.1509,  1.1576,  1.1073,  0.9811]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([13, 6]), val: tensor([[-0.0811, -0.0711, -0.0913, -0.0736, -0.1089, -0.1209],\n",
      "        [ 0.1307,  0.1633,  0.1798,  0.1830,  0.1562,  0.2440],\n",
      "        [-0.3053, -0.3134, -0.3332, -0.3214, -0.3089, -0.2914],\n",
      "        [ 0.0986,  0.1051,  0.1018,  0.1199,  0.0384,  0.0824],\n",
      "        [ 0.5807,  0.5992,  0.5816,  0.5906,  0.5831,  0.5970],\n",
      "        [-0.2915, -0.2909, -0.2670, -0.2743, -0.2972, -0.2628],\n",
      "        [-0.1360, -0.1363, -0.1300, -0.1670, -0.1034, -0.0226],\n",
      "        [-0.9925, -0.9990, -0.9920, -0.9799, -0.9746, -0.9616],\n",
      "        [-0.2233, -0.2108, -0.1967, -0.2130, -0.2142, -0.1129],\n",
      "        [-0.6039, -0.6142, -0.6114, -0.6260, -0.5956, -0.5954],\n",
      "        [-0.2417, -0.2676, -0.3089, -0.2869, -0.2666, -0.3870],\n",
      "        [ 0.3601,  0.3832,  0.4042,  0.3877,  0.3748,  0.4438],\n",
      "        [ 0.1202,  0.1217,  0.1325,  0.1341,  0.1038,  0.1381]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([13, 6]), val: tensor([[-0.0081, -0.0071, -0.0091, -0.0074, -0.0109, -0.0121],\n",
      "        [ 0.1307,  0.1633,  0.1798,  0.1830,  0.1562,  0.2440],\n",
      "        [-0.0305, -0.0313, -0.0333, -0.0321, -0.0309, -0.0291],\n",
      "        [ 0.0986,  0.1051,  0.1018,  0.1199,  0.0384,  0.0824],\n",
      "        [ 0.5807,  0.5992,  0.5816,  0.5906,  0.5831,  0.5970],\n",
      "        [-0.0292, -0.0291, -0.0267, -0.0274, -0.0297, -0.0263],\n",
      "        [-0.0136, -0.0136, -0.0130, -0.0167, -0.0103, -0.0023],\n",
      "        [-0.0993, -0.0999, -0.0992, -0.0980, -0.0975, -0.0962],\n",
      "        [-0.0223, -0.0211, -0.0197, -0.0213, -0.0214, -0.0113],\n",
      "        [-0.0604, -0.0614, -0.0611, -0.0626, -0.0596, -0.0595],\n",
      "        [-0.0242, -0.0268, -0.0309, -0.0287, -0.0267, -0.0387],\n",
      "        [ 0.3601,  0.3832,  0.4042,  0.3877,  0.3748,  0.4438],\n",
      "        [ 0.1202,  0.1217,  0.1325,  0.1341,  0.1038,  0.1381]],\n",
      "       dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([13, 6]), val: tensor([[-0.0081, -0.0071, -0.0091, -0.0074, -0.0109, -0.0121],\n",
      "        [ 0.1307,  0.1633,  0.1798,  0.1830,  0.1562,  0.2440],\n",
      "        [-0.0305, -0.0313, -0.0333, -0.0321, -0.0309, -0.0291],\n",
      "        [ 0.0986,  0.1051,  0.1018,  0.1199,  0.0384,  0.0824],\n",
      "        [ 0.5807,  0.5992,  0.5816,  0.5906,  0.5831,  0.5970],\n",
      "        [-0.0292, -0.0291, -0.0267, -0.0274, -0.0297, -0.0263],\n",
      "        [-0.0136, -0.0136, -0.0130, -0.0167, -0.0103, -0.0023],\n",
      "        [-0.0993, -0.0999, -0.0992, -0.0980, -0.0975, -0.0962],\n",
      "        [-0.0223, -0.0211, -0.0197, -0.0213, -0.0214, -0.0113],\n",
      "        [-0.0604, -0.0614, -0.0611, -0.0626, -0.0596, -0.0595],\n",
      "        [-0.0242, -0.0268, -0.0309, -0.0287, -0.0267, -0.0387],\n",
      "        [ 0.3601,  0.3832,  0.4042,  0.3877,  0.3748,  0.4438],\n",
      "        [ 0.1202,  0.1217,  0.1325,  0.1341,  0.1038,  0.1381]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([13, 6]), val: tensor([[-0.0081, -0.0071, -0.0091, -0.0074, -0.0109, -0.0121],\n",
      "        [ 0.1307,  0.1633,  0.1798,  0.1830,  0.1562,  0.2440],\n",
      "        [-0.0305, -0.0313, -0.0333, -0.0321, -0.0309, -0.0291],\n",
      "        [ 0.0986,  0.1051,  0.1018,  0.1199,  0.0384,  0.0824],\n",
      "        [ 0.5807,  0.5992,  0.5816,  0.5906,  0.5831,  0.5970],\n",
      "        [-0.0292, -0.0291, -0.0267, -0.0274, -0.0297, -0.0263],\n",
      "        [-0.0136, -0.0136, -0.0130, -0.0167, -0.0103, -0.0023],\n",
      "        [-0.0993, -0.0999, -0.0992, -0.0980, -0.0975, -0.0962],\n",
      "        [-0.0223, -0.0211, -0.0197, -0.0213, -0.0214, -0.0113],\n",
      "        [-0.0604, -0.0614, -0.0611, -0.0626, -0.0596, -0.0595],\n",
      "        [-0.0242, -0.0268, -0.0309, -0.0287, -0.0267, -0.0387],\n",
      "        [ 0.3601,  0.3832,  0.4042,  0.3877,  0.3748,  0.4438],\n",
      "        [ 0.1202,  0.1217,  0.1325,  0.1341,  0.1038,  0.1381]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([13, 1]), val: tensor([[-0.0709],\n",
      "        [-0.0378],\n",
      "        [-0.0752],\n",
      "        [-0.0578],\n",
      "        [ 0.0529],\n",
      "        [-0.0740],\n",
      "        [-0.0692],\n",
      "        [-0.0897],\n",
      "        [-0.0720],\n",
      "        [-0.0804],\n",
      "        [-0.0747],\n",
      "        [ 0.0181],\n",
      "        [-0.0433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 68227332702.688805, val: 68227332702.688805\n",
      "conv1 shape: torch.Size([13, 12]), val: tensor([[ -82783.4998,  -94490.2083, -100863.6098, -110962.8769,  -97965.9450,\n",
      "          -81541.0012,  -79728.5305,  -63313.1562,  -89656.9974,  -86153.8343,\n",
      "          -76378.0843,  -89023.5952],\n",
      "        [ 134536.2312,  121441.7766,  124862.5832,  111180.3781,  127661.7364,\n",
      "          152701.7605,  135851.3418,  120669.2820,  117258.7547,  158000.5462,\n",
      "          127134.2220,  100683.4608],\n",
      "        [ 259840.2724,  264279.5793,  260067.8663,  254458.9890,  265318.1435,\n",
      "          238715.6560,  211776.9996,  216770.5325,  231380.3466,  209828.9167,\n",
      "          197889.2948,  229188.9896],\n",
      "        [ 159753.4854,  160597.5733,  147483.2895,  142583.7201,  144794.9707,\n",
      "          130299.9974,  108146.9135,  107898.1353,  116273.2662,   99956.8253,\n",
      "          100152.6204,   92470.1279],\n",
      "        [  32939.7546,   30022.1225,   28434.8553,   42593.1740,   43453.3192,\n",
      "           62741.6640,   69072.5269,   73976.7186,   63931.7735,   57977.6883,\n",
      "           60864.2709,   56662.0434],\n",
      "        [ 163579.3691,  161966.5056,  168555.9837,  167434.1016,  164103.9991,\n",
      "          144622.3471,  131602.8124,  128512.6316,  136580.0429,  152969.8841,\n",
      "          132809.4268,  149301.8100],\n",
      "        [-183083.3863, -185427.7176, -186821.5596, -184518.8606, -185795.6529,\n",
      "         -166417.0750, -148404.6920, -150283.7269, -150799.7807, -151514.1789,\n",
      "         -130893.5901, -133535.8900],\n",
      "        [-200823.9810, -203313.2127, -194522.3918, -192812.8493, -203683.5637,\n",
      "         -206575.4463, -202507.7844, -200377.7886, -158756.9065, -177642.2624,\n",
      "         -166017.9706, -174037.6561],\n",
      "        [  25647.8504,   -6286.3783,   -8101.1838,    8744.9798,    2113.3934,\n",
      "          -18141.5552,  -13658.1388,  -20551.2945,  -18876.8852,    4150.4424,\n",
      "          -22278.5017,  -20069.1112],\n",
      "        [  56134.5200,   49574.5531,   43184.0676,   40073.0828,   46956.9643,\n",
      "           63356.9955,   57287.9138,   61090.4040,   47455.7219,   43684.8374,\n",
      "           32417.9822,   30174.7549],\n",
      "        [ -91986.0587,  -88287.3971,  -96680.2648,  -92548.8608, -101226.2495,\n",
      "         -103891.1158, -110348.9559, -107674.5963,  -83790.9039, -105593.6116,\n",
      "         -105357.4439, -109337.6429],\n",
      "        [  89836.7563,   90310.8730,   84699.8919,   92851.1773,   89044.2064,\n",
      "           85760.4229,   83273.5125,   87234.0620,   95782.9922,   77455.3419,\n",
      "           89087.8713,   58490.6355],\n",
      "        [ 176958.0921,  193075.4312,  182718.9369,  182170.0307,  184532.7295,\n",
      "          186777.0881,  203886.2931,  208695.8966,  208260.4816,  171767.8162,\n",
      "          185373.2374,  193283.8444]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "bn1 shape: torch.Size([13, 12]), val: tensor([[-0.8988, -0.9381, -0.9725, -1.0635, -0.9556, -0.8800, -0.8862, -0.7738,\n",
      "         -1.0389, -0.9759, -0.9312, -0.9671],\n",
      "        [ 0.6718,  0.5913,  0.6450,  0.5497,  0.6434,  0.8506,  0.7889,  0.6657,\n",
      "          0.6239,  0.9909,  0.8066,  0.5801],\n",
      "        [ 1.5775,  1.6029,  1.6139,  1.5902,  1.6190,  1.4860,  1.3788,  1.4175,\n",
      "          1.5410,  1.4084,  1.4107,  1.6281],\n",
      "        [ 0.8541,  0.8686,  0.8071,  0.7778,  0.7648,  0.6851,  0.5736,  0.5657,\n",
      "          0.6160,  0.5233,  0.5762,  0.5131],\n",
      "        [-0.0625, -0.0562, -0.0460,  0.0517,  0.0467,  0.1860,  0.2700,  0.3003,\n",
      "          0.1954,  0.1852,  0.2407,  0.2210],\n",
      "        [ 0.8818,  0.8783,  0.9581,  0.9582,  0.9017,  0.7909,  0.7559,  0.7270,\n",
      "          0.7792,  0.9504,  0.8550,  0.9766],\n",
      "        [-1.6238, -1.5821, -1.5885, -1.5976, -1.5780, -1.5070, -1.4199, -1.4542,\n",
      "         -1.5302, -1.5024, -1.3967, -1.3301],\n",
      "        [-1.7520, -1.7088, -1.6436, -1.6578, -1.7048, -1.8037, -1.8403, -1.8462,\n",
      "         -1.5942, -1.7128, -1.6967, -1.6604],\n",
      "        [-0.1152, -0.3134, -0.3078, -0.1942, -0.2463, -0.4116, -0.3729, -0.4392,\n",
      "         -0.4701, -0.2484, -0.4693, -0.4047],\n",
      "        [ 0.1052,  0.0823,  0.0597,  0.0334,  0.0715,  0.1905,  0.1784,  0.1995,\n",
      "          0.0630,  0.0700, -0.0022,  0.0050],\n",
      "        [-0.9654, -0.8941, -0.9425, -0.9297, -0.9787, -1.0451, -1.1242, -1.1209,\n",
      "         -0.9917, -1.1325, -1.1787, -1.1328],\n",
      "        [ 0.3488,  0.3708,  0.3572,  0.4166,  0.3698,  0.3560,  0.3803,  0.4041,\n",
      "          0.4513,  0.3421,  0.4817,  0.2360],\n",
      "        [ 0.9784,  1.0986,  1.0596,  1.0652,  1.0465,  1.1023,  1.3175,  1.3544,\n",
      "          1.3552,  1.1018,  1.3039,  1.3353]], dtype=torch.float64,\n",
      "       grad_fn=<NativeBatchNormBackward0>)\n",
      "relu shape: torch.Size([13, 12]), val: tensor([[-8.9885e-02, -9.3807e-02, -9.7249e-02, -1.0635e-01, -9.5556e-02,\n",
      "         -8.7998e-02, -8.8624e-02, -7.7379e-02, -1.0389e-01, -9.7587e-02,\n",
      "         -9.3122e-02, -9.6709e-02],\n",
      "        [ 6.7184e-01,  5.9127e-01,  6.4504e-01,  5.4972e-01,  6.4343e-01,\n",
      "          8.5058e-01,  7.8887e-01,  6.6566e-01,  6.2393e-01,  9.9088e-01,\n",
      "          8.0657e-01,  5.8006e-01],\n",
      "        [ 1.5775e+00,  1.6029e+00,  1.6139e+00,  1.5902e+00,  1.6190e+00,\n",
      "          1.4860e+00,  1.3788e+00,  1.4175e+00,  1.5410e+00,  1.4084e+00,\n",
      "          1.4107e+00,  1.6281e+00],\n",
      "        [ 8.5410e-01,  8.6859e-01,  8.0714e-01,  7.7777e-01,  7.6485e-01,\n",
      "          6.8508e-01,  5.7360e-01,  5.6574e-01,  6.1601e-01,  5.2332e-01,\n",
      "          5.7617e-01,  5.1308e-01],\n",
      "        [-6.2453e-03, -5.6211e-03, -4.5953e-03,  5.1652e-02,  4.6656e-02,\n",
      "          1.8597e-01,  2.6998e-01,  3.0035e-01,  1.9538e-01,  1.8516e-01,\n",
      "          2.4069e-01,  2.2105e-01],\n",
      "        [ 8.8175e-01,  8.7829e-01,  9.5814e-01,  9.5823e-01,  9.0169e-01,\n",
      "          7.9089e-01,  7.5586e-01,  7.2703e-01,  7.7920e-01,  9.5036e-01,\n",
      "          8.5503e-01,  9.7657e-01],\n",
      "        [-1.6238e-01, -1.5821e-01, -1.5885e-01, -1.5976e-01, -1.5780e-01,\n",
      "         -1.5070e-01, -1.4199e-01, -1.4542e-01, -1.5302e-01, -1.5024e-01,\n",
      "         -1.3967e-01, -1.3301e-01],\n",
      "        [-1.7520e-01, -1.7088e-01, -1.6436e-01, -1.6578e-01, -1.7048e-01,\n",
      "         -1.8037e-01, -1.8403e-01, -1.8462e-01, -1.5942e-01, -1.7128e-01,\n",
      "         -1.6967e-01, -1.6604e-01],\n",
      "        [-1.1516e-02, -3.1337e-02, -3.0777e-02, -1.9415e-02, -2.4631e-02,\n",
      "         -4.1159e-02, -3.7286e-02, -4.3923e-02, -4.7008e-02, -2.4844e-02,\n",
      "         -4.6927e-02, -4.0473e-02],\n",
      "        [ 1.0519e-01,  8.2269e-02,  5.9739e-02,  3.3351e-02,  7.1486e-02,\n",
      "          1.9051e-01,  1.7841e-01,  1.9953e-01,  6.2980e-02,  7.0029e-02,\n",
      "         -2.2133e-04,  5.0313e-03],\n",
      "        [-9.6536e-02, -8.9414e-02, -9.4252e-02, -9.2974e-02, -9.7866e-02,\n",
      "         -1.0451e-01, -1.1242e-01, -1.1209e-01, -9.9174e-02, -1.1325e-01,\n",
      "         -1.1787e-01, -1.1328e-01],\n",
      "        [ 3.4877e-01,  3.7078e-01,  3.5724e-01,  4.1662e-01,  3.6975e-01,\n",
      "          3.5603e-01,  3.8033e-01,  4.0407e-01,  4.5135e-01,  3.4206e-01,\n",
      "          4.8169e-01,  2.3596e-01],\n",
      "        [ 9.7845e-01,  1.0986e+00,  1.0596e+00,  1.0652e+00,  1.0465e+00,\n",
      "          1.1023e+00,  1.3175e+00,  1.3544e+00,  1.3552e+00,  1.1018e+00,\n",
      "          1.3039e+00,  1.3353e+00]], dtype=torch.float64,\n",
      "       grad_fn=<LeakyReluBackward1>)\n",
      "pool shape: torch.Size([13, 6]), val: tensor([[-0.0918, -0.1018, -0.0918, -0.0830, -0.1007, -0.0949],\n",
      "        [ 0.6316,  0.5974,  0.7470,  0.7273,  0.8074,  0.6933],\n",
      "        [ 1.5902,  1.6021,  1.5525,  1.3982,  1.4747,  1.5194],\n",
      "        [ 0.8613,  0.7925,  0.7250,  0.5697,  0.5697,  0.5446],\n",
      "        [-0.0059,  0.0235,  0.1163,  0.2852,  0.1903,  0.2309],\n",
      "        [ 0.8800,  0.9582,  0.8463,  0.7414,  0.8648,  0.9158],\n",
      "        [-0.1603, -0.1593, -0.1543, -0.1437, -0.1516, -0.1363],\n",
      "        [-0.1730, -0.1651, -0.1754, -0.1843, -0.1654, -0.1679],\n",
      "        [-0.0214, -0.0251, -0.0329, -0.0406, -0.0359, -0.0437],\n",
      "        [ 0.0937,  0.0465,  0.1310,  0.1890,  0.0665,  0.0024],\n",
      "        [-0.0930, -0.0936, -0.1012, -0.1123, -0.1062, -0.1156],\n",
      "        [ 0.3598,  0.3869,  0.3629,  0.3922,  0.3967,  0.3588],\n",
      "        [ 1.0385,  1.0624,  1.0744,  1.3359,  1.2285,  1.3196]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([13, 6]), val: tensor([[-0.0198, -0.0178, -0.0517, -0.0501, -0.0362, -0.0088],\n",
      "        [ 0.3517,  0.3655,  0.3167,  0.2684,  0.3030,  0.3498],\n",
      "        [-0.3057, -0.3206, -0.2467, -0.2171, -0.2298, -0.2591],\n",
      "        [ 0.1354,  0.1457,  0.1655,  0.1643,  0.1734,  0.1859],\n",
      "        [ 0.6575,  0.6538,  0.6589,  0.6228,  0.6523,  0.6545],\n",
      "        [-0.2930, -0.2731, -0.2706, -0.2519, -0.2565, -0.2361],\n",
      "        [-0.1048, -0.1119, -0.1314, -0.1994, -0.1874, -0.2032],\n",
      "        [-0.9366, -0.9337, -0.9656, -0.9837, -0.9681, -0.9613],\n",
      "        [-0.1329, -0.1149, -0.1501, -0.1851, -0.1712, -0.1397],\n",
      "        [-0.6123, -0.6231, -0.6353, -0.6067, -0.6426, -0.6336],\n",
      "        [-0.4259, -0.4453, -0.3974, -0.3816, -0.3710, -0.4428],\n",
      "        [ 0.4700,  0.4745,  0.4627,  0.4112,  0.4217,  0.4471],\n",
      "        [ 0.1210,  0.1411,  0.1480,  0.1790,  0.1742,  0.1915]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([13, 6]), val: tensor([[-0.0020, -0.0018, -0.0052, -0.0050, -0.0036, -0.0009],\n",
      "        [ 0.3517,  0.3655,  0.3167,  0.2684,  0.3030,  0.3498],\n",
      "        [-0.0306, -0.0321, -0.0247, -0.0217, -0.0230, -0.0259],\n",
      "        [ 0.1354,  0.1457,  0.1655,  0.1643,  0.1734,  0.1859],\n",
      "        [ 0.6575,  0.6538,  0.6589,  0.6228,  0.6523,  0.6545],\n",
      "        [-0.0293, -0.0273, -0.0271, -0.0252, -0.0256, -0.0236],\n",
      "        [-0.0105, -0.0112, -0.0131, -0.0199, -0.0187, -0.0203],\n",
      "        [-0.0937, -0.0934, -0.0966, -0.0984, -0.0968, -0.0961],\n",
      "        [-0.0133, -0.0115, -0.0150, -0.0185, -0.0171, -0.0140],\n",
      "        [-0.0612, -0.0623, -0.0635, -0.0607, -0.0643, -0.0634],\n",
      "        [-0.0426, -0.0445, -0.0397, -0.0382, -0.0371, -0.0443],\n",
      "        [ 0.4700,  0.4745,  0.4627,  0.4112,  0.4217,  0.4471],\n",
      "        [ 0.1210,  0.1411,  0.1480,  0.1790,  0.1742,  0.1915]],\n",
      "       dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([13, 6]), val: tensor([[-0.0020, -0.0018, -0.0052, -0.0050, -0.0036, -0.0009],\n",
      "        [ 0.3517,  0.3655,  0.3167,  0.2684,  0.3030,  0.3498],\n",
      "        [-0.0306, -0.0321, -0.0247, -0.0217, -0.0230, -0.0259],\n",
      "        [ 0.1354,  0.1457,  0.1655,  0.1643,  0.1734,  0.1859],\n",
      "        [ 0.6575,  0.6538,  0.6589,  0.6228,  0.6523,  0.6545],\n",
      "        [-0.0293, -0.0273, -0.0271, -0.0252, -0.0256, -0.0236],\n",
      "        [-0.0105, -0.0112, -0.0131, -0.0199, -0.0187, -0.0203],\n",
      "        [-0.0937, -0.0934, -0.0966, -0.0984, -0.0968, -0.0961],\n",
      "        [-0.0133, -0.0115, -0.0150, -0.0185, -0.0171, -0.0140],\n",
      "        [-0.0612, -0.0623, -0.0635, -0.0607, -0.0643, -0.0634],\n",
      "        [-0.0426, -0.0445, -0.0397, -0.0382, -0.0371, -0.0443],\n",
      "        [ 0.4700,  0.4745,  0.4627,  0.4112,  0.4217,  0.4471],\n",
      "        [ 0.1210,  0.1411,  0.1480,  0.1790,  0.1742,  0.1915]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([13, 6]), val: tensor([[-0.0020, -0.0018, -0.0052, -0.0050, -0.0036, -0.0009],\n",
      "        [ 0.3517,  0.3655,  0.3167,  0.2684,  0.3030,  0.3498],\n",
      "        [-0.0306, -0.0321, -0.0247, -0.0217, -0.0230, -0.0259],\n",
      "        [ 0.1354,  0.1457,  0.1655,  0.1643,  0.1734,  0.1859],\n",
      "        [ 0.6575,  0.6538,  0.6589,  0.6228,  0.6523,  0.6545],\n",
      "        [-0.0293, -0.0273, -0.0271, -0.0252, -0.0256, -0.0236],\n",
      "        [-0.0105, -0.0112, -0.0131, -0.0199, -0.0187, -0.0203],\n",
      "        [-0.0937, -0.0934, -0.0966, -0.0984, -0.0968, -0.0961],\n",
      "        [-0.0133, -0.0115, -0.0150, -0.0185, -0.0171, -0.0140],\n",
      "        [-0.0612, -0.0623, -0.0635, -0.0607, -0.0643, -0.0634],\n",
      "        [-0.0426, -0.0445, -0.0397, -0.0382, -0.0371, -0.0443],\n",
      "        [ 0.4700,  0.4745,  0.4627,  0.4112,  0.4217,  0.4471],\n",
      "        [ 0.1210,  0.1411,  0.1480,  0.1790,  0.1742,  0.1915]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([13, 1]), val: tensor([[-0.0688],\n",
      "        [ 0.0227],\n",
      "        [-0.0752],\n",
      "        [-0.0354],\n",
      "        [ 0.0868],\n",
      "        [-0.0749],\n",
      "        [-0.0684],\n",
      "        [-0.0878],\n",
      "        [-0.0699],\n",
      "        [-0.0826],\n",
      "        [-0.0776],\n",
      "        [ 0.0496],\n",
      "        [-0.0502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 55358630315.580696, val: 55358630315.580696\n",
      "conv1 shape: torch.Size([13, 12]), val: tensor([[-154466.7389, -107919.0886, -131436.3916, -125365.0486,  -98299.4478,\n",
      "          -84545.6400,  -80433.7257,  -79762.7357,  -70277.7244, -141492.9726,\n",
      "         -124041.2485, -162154.3239],\n",
      "        [ 144187.7196,   89548.0645,  101194.4457,   77847.3122,   88980.6832,\n",
      "          112902.3191,  134535.8310,  150688.0009,  150424.4552,  223480.7351,\n",
      "          185242.1505,  147790.7522],\n",
      "        [ 166827.9678,  191208.3202,  183229.9376,  188957.7450,  198530.7594,\n",
      "          207707.7680,  192366.3521,  187145.1400,  200214.6198,  193442.6620,\n",
      "          172187.1642,  197072.0898],\n",
      "        [ 110159.0924,  144913.8211,  156741.2554,  142815.9641,  133849.5973,\n",
      "          142720.2616,  152301.1938,  129500.4756,  146803.7367,  159205.1978,\n",
      "          215351.7386,  131263.3085],\n",
      "        [ 104066.5152,  102493.9700,  122466.8775,   95519.0725,  108066.7743,\n",
      "          106954.2510,  109064.0042,   95207.7929,   97535.2814,  149027.7444,\n",
      "          130022.6670,  134598.5631],\n",
      "        [ 188039.4848,  151979.4582,  139443.4169,  158462.8911,  148824.4906,\n",
      "          146503.2254,  151677.3427,  147179.4435,  134163.0431,   96971.7825,\n",
      "          126681.0591,  130426.8242],\n",
      "        [-192193.7811, -165426.2219, -139494.5025, -157716.4740, -175619.8990,\n",
      "         -161167.5217, -142297.7186, -144140.7258, -154607.0960,  -97536.8211,\n",
      "         -118721.9728, -121953.9680],\n",
      "        [-161829.8938, -192768.1844, -195621.3983, -209205.8945, -193018.3820,\n",
      "         -190032.2986, -198532.7715, -196225.3953, -185722.9747, -199034.7572,\n",
      "         -245295.5866, -260704.1419],\n",
      "        [ -53837.3443,  -64265.5513,  -64752.2784,  -66424.1654,  -57835.8489,\n",
      "          -78175.7773,  -66277.3870,  -49036.1565,  -63774.1413,  -58017.7533,\n",
      "         -136368.9766, -129257.4106],\n",
      "        [  76614.3721,  112615.4842,  123505.6740,   93723.4270,   99851.2719,\n",
      "           93492.7528,   81324.6086,   84689.2095,  101449.0537,  148290.6256,\n",
      "          132654.0110,  112394.6374],\n",
      "        [-148736.2951, -124864.4514, -131165.5197, -133320.4140, -135313.7495,\n",
      "         -120425.8763,  -99638.5711,  -90742.7308,  -82378.8096,  -96619.3555,\n",
      "          -98526.4641, -185270.5076],\n",
      "        [ 161169.0237,  160702.6804,  180037.2920,  149631.9304,  151220.0624,\n",
      "          155631.9586,  143312.1691,  130044.6350,  123065.5120,  176688.7290,\n",
      "          151740.5250,  132582.3072],\n",
      "        [ 251071.1015,  264145.0962,  259877.6463,  262128.0739,  261863.6555,\n",
      "          260215.1085,  245838.0148,  238639.0330,  238840.2891,  232592.2278,\n",
      "          224987.6464,  204014.1030]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "bn1 shape: torch.Size([13, 12]), val: tensor([[-1.2756, -1.0319, -1.1986, -1.0939, -0.9512, -0.8980, -0.9232, -0.9392,\n",
      "         -0.8820, -1.3625, -1.0870, -1.1658],\n",
      "        [ 0.7061,  0.3160,  0.3688,  0.2778,  0.3290,  0.4652,  0.6227,  0.7764,\n",
      "          0.7513,  1.0989,  0.8742,  0.7603],\n",
      "        [ 0.8563,  1.0099,  0.9215,  1.0278,  1.0778,  1.1197,  1.0386,  1.0478,\n",
      "          1.1198,  0.8963,  0.7914,  1.0666],\n",
      "        [ 0.4803,  0.6939,  0.7430,  0.7163,  0.6357,  0.6710,  0.7505,  0.6187,\n",
      "          0.7245,  0.6654,  1.0651,  0.6576],\n",
      "        [ 0.4399,  0.4043,  0.5121,  0.3971,  0.4595,  0.4241,  0.4396,  0.3634,\n",
      "          0.3599,  0.5968,  0.5241,  0.6783],\n",
      "        [ 0.9970,  0.7421,  0.6265,  0.8219,  0.7381,  0.6972,  0.7460,  0.7503,\n",
      "          0.6310,  0.2457,  0.5029,  0.6524],\n",
      "        [-1.5259, -1.4244, -1.2529, -1.3123, -1.4798, -1.4270, -1.3681, -1.4185,\n",
      "         -1.5061, -1.0660, -1.0533, -0.9160],\n",
      "        [-1.3244, -1.6110, -1.6311, -1.6599, -1.5987, -1.6262, -1.7725, -1.8063,\n",
      "         -1.7364, -1.7505, -1.8559, -1.7783],\n",
      "        [-0.6079, -0.7339, -0.7493, -0.6961, -0.6746, -0.8540, -0.8214, -0.7105,\n",
      "         -0.8339, -0.7995, -1.1652, -0.9614],\n",
      "        [ 0.2577,  0.4734,  0.5191,  0.3849,  0.4033,  0.3312,  0.2401,  0.2851,\n",
      "          0.3889,  0.5918,  0.5407,  0.5403],\n",
      "        [-1.2375, -1.1476, -1.1968, -1.1476, -1.2042, -1.1457, -1.0613, -1.0210,\n",
      "         -0.9716, -1.0599, -0.9252, -1.3095],\n",
      "        [ 0.8187,  0.8016,  0.9000,  0.7623,  0.7544,  0.7602,  0.6859,  0.6227,\n",
      "          0.5489,  0.7833,  0.6618,  0.6658],\n",
      "        [ 1.4153,  1.5077,  1.4379,  1.5217,  1.5108,  1.4822,  1.4232,  1.4312,\n",
      "          1.4057,  1.1603,  1.1262,  1.1097]], dtype=torch.float64,\n",
      "       grad_fn=<NativeBatchNormBackward0>)\n",
      "relu shape: torch.Size([13, 12]), val: tensor([[-0.1276, -0.1032, -0.1199, -0.1094, -0.0951, -0.0898, -0.0923, -0.0939,\n",
      "         -0.0882, -0.1362, -0.1087, -0.1166],\n",
      "        [ 0.7061,  0.3160,  0.3688,  0.2778,  0.3290,  0.4652,  0.6227,  0.7764,\n",
      "          0.7513,  1.0989,  0.8742,  0.7603],\n",
      "        [ 0.8563,  1.0099,  0.9215,  1.0278,  1.0778,  1.1197,  1.0386,  1.0478,\n",
      "          1.1198,  0.8963,  0.7914,  1.0666],\n",
      "        [ 0.4803,  0.6939,  0.7430,  0.7163,  0.6357,  0.6710,  0.7505,  0.6187,\n",
      "          0.7245,  0.6654,  1.0651,  0.6576],\n",
      "        [ 0.4399,  0.4043,  0.5121,  0.3971,  0.4595,  0.4241,  0.4396,  0.3634,\n",
      "          0.3599,  0.5968,  0.5241,  0.6783],\n",
      "        [ 0.9970,  0.7421,  0.6265,  0.8219,  0.7381,  0.6972,  0.7460,  0.7503,\n",
      "          0.6310,  0.2457,  0.5029,  0.6524],\n",
      "        [-0.1526, -0.1424, -0.1253, -0.1312, -0.1480, -0.1427, -0.1368, -0.1419,\n",
      "         -0.1506, -0.1066, -0.1053, -0.0916],\n",
      "        [-0.1324, -0.1611, -0.1631, -0.1660, -0.1599, -0.1626, -0.1773, -0.1806,\n",
      "         -0.1736, -0.1751, -0.1856, -0.1778],\n",
      "        [-0.0608, -0.0734, -0.0749, -0.0696, -0.0675, -0.0854, -0.0821, -0.0710,\n",
      "         -0.0834, -0.0800, -0.1165, -0.0961],\n",
      "        [ 0.2577,  0.4734,  0.5191,  0.3849,  0.4033,  0.3312,  0.2401,  0.2851,\n",
      "          0.3889,  0.5918,  0.5407,  0.5403],\n",
      "        [-0.1238, -0.1148, -0.1197, -0.1148, -0.1204, -0.1146, -0.1061, -0.1021,\n",
      "         -0.0972, -0.1060, -0.0925, -0.1309],\n",
      "        [ 0.8187,  0.8016,  0.9000,  0.7623,  0.7544,  0.7602,  0.6859,  0.6227,\n",
      "          0.5489,  0.7833,  0.6618,  0.6658],\n",
      "        [ 1.4153,  1.5077,  1.4379,  1.5217,  1.5108,  1.4822,  1.4232,  1.4312,\n",
      "          1.4057,  1.1603,  1.1262,  1.1097]], dtype=torch.float64,\n",
      "       grad_fn=<LeakyReluBackward1>)\n",
      "pool shape: torch.Size([13, 6]), val: tensor([[-0.1154, -0.1146, -0.0925, -0.0931, -0.1122, -0.1126],\n",
      "        [ 0.5110,  0.3233,  0.3971,  0.6996,  0.9251,  0.8173],\n",
      "        [ 0.9331,  0.9746,  1.0988,  1.0432,  1.0080,  0.9290],\n",
      "        [ 0.5871,  0.7297,  0.6534,  0.6846,  0.6950,  0.8614],\n",
      "        [ 0.4221,  0.4546,  0.4418,  0.4015,  0.4783,  0.6012],\n",
      "        [ 0.8696,  0.7242,  0.7176,  0.7482,  0.4383,  0.5776],\n",
      "        [-0.1475, -0.1283, -0.1453, -0.1393, -0.1286, -0.0985],\n",
      "        [-0.1468, -0.1645, -0.1612, -0.1789, -0.1743, -0.1817],\n",
      "        [-0.0671, -0.0723, -0.0764, -0.0766, -0.0817, -0.1063],\n",
      "        [ 0.3656,  0.4520,  0.3672,  0.2626,  0.4903,  0.5405],\n",
      "        [-0.1193, -0.1172, -0.1175, -0.1041, -0.1016, -0.1117],\n",
      "        [ 0.8102,  0.8312,  0.7573,  0.6543,  0.6661,  0.6638],\n",
      "        [ 1.4615,  1.4798,  1.4965,  1.4272,  1.2830,  1.1180]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([13, 6]), val: tensor([[-0.0672, -0.0919, -0.0834, -0.0436, -0.1609, -0.1739],\n",
      "        [ 0.1163,  0.1118,  0.1544,  0.1612,  0.0387,  0.0515],\n",
      "        [-0.2972, -0.3316, -0.3155, -0.2287, -0.1209, -0.1156],\n",
      "        [ 0.0044, -0.0354,  0.0095,  0.0192,  0.0195, -0.0173],\n",
      "        [ 0.5414,  0.5163,  0.5373,  0.6082,  0.5916,  0.6213],\n",
      "        [-0.2929, -0.2847, -0.2570, -0.3248, -0.3210, -0.3544],\n",
      "        [-0.0884, -0.0738, -0.1263, -0.1185, -0.1236,  0.0259],\n",
      "        [-1.0171, -1.0109, -1.0200, -1.0150, -1.0443, -1.0585],\n",
      "        [-0.1968, -0.1948, -0.1908, -0.2130, -0.2982, -0.2242],\n",
      "        [-0.5074, -0.4771, -0.5155, -0.5300, -0.5599, -0.5135],\n",
      "        [-0.2651, -0.3127, -0.3396, -0.2743, -0.1509, -0.2097],\n",
      "        [ 0.3107,  0.3352,  0.3558,  0.3321,  0.3178,  0.3866],\n",
      "        [ 0.1344,  0.1212,  0.1495,  0.1066,  0.1046,  0.0602]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([13, 6]), val: tensor([[-0.0067, -0.0092, -0.0083, -0.0044, -0.0161, -0.0174],\n",
      "        [ 0.1163,  0.1118,  0.1544,  0.1612,  0.0387,  0.0515],\n",
      "        [-0.0297, -0.0332, -0.0315, -0.0229, -0.0121, -0.0116],\n",
      "        [ 0.0044, -0.0035,  0.0095,  0.0192,  0.0195, -0.0017],\n",
      "        [ 0.5414,  0.5163,  0.5373,  0.6082,  0.5916,  0.6213],\n",
      "        [-0.0293, -0.0285, -0.0257, -0.0325, -0.0321, -0.0354],\n",
      "        [-0.0088, -0.0074, -0.0126, -0.0119, -0.0124,  0.0259],\n",
      "        [-0.1017, -0.1011, -0.1020, -0.1015, -0.1044, -0.1059],\n",
      "        [-0.0197, -0.0195, -0.0191, -0.0213, -0.0298, -0.0224],\n",
      "        [-0.0507, -0.0477, -0.0515, -0.0530, -0.0560, -0.0514],\n",
      "        [-0.0265, -0.0313, -0.0340, -0.0274, -0.0151, -0.0210],\n",
      "        [ 0.3107,  0.3352,  0.3558,  0.3321,  0.3178,  0.3866],\n",
      "        [ 0.1344,  0.1212,  0.1495,  0.1066,  0.1046,  0.0602]],\n",
      "       dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([13, 6]), val: tensor([[-0.0067, -0.0092, -0.0083, -0.0044, -0.0161, -0.0174],\n",
      "        [ 0.1163,  0.1118,  0.1544,  0.1612,  0.0387,  0.0515],\n",
      "        [-0.0297, -0.0332, -0.0315, -0.0229, -0.0121, -0.0116],\n",
      "        [ 0.0044, -0.0035,  0.0095,  0.0192,  0.0195, -0.0017],\n",
      "        [ 0.5414,  0.5163,  0.5373,  0.6082,  0.5916,  0.6213],\n",
      "        [-0.0293, -0.0285, -0.0257, -0.0325, -0.0321, -0.0354],\n",
      "        [-0.0088, -0.0074, -0.0126, -0.0119, -0.0124,  0.0259],\n",
      "        [-0.1017, -0.1011, -0.1020, -0.1015, -0.1044, -0.1059],\n",
      "        [-0.0197, -0.0195, -0.0191, -0.0213, -0.0298, -0.0224],\n",
      "        [-0.0507, -0.0477, -0.0515, -0.0530, -0.0560, -0.0514],\n",
      "        [-0.0265, -0.0313, -0.0340, -0.0274, -0.0151, -0.0210],\n",
      "        [ 0.3107,  0.3352,  0.3558,  0.3321,  0.3178,  0.3866],\n",
      "        [ 0.1344,  0.1212,  0.1495,  0.1066,  0.1046,  0.0602]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([13, 6]), val: tensor([[-0.0067, -0.0092, -0.0083, -0.0044, -0.0161, -0.0174],\n",
      "        [ 0.1163,  0.1118,  0.1544,  0.1612,  0.0387,  0.0515],\n",
      "        [-0.0297, -0.0332, -0.0315, -0.0229, -0.0121, -0.0116],\n",
      "        [ 0.0044, -0.0035,  0.0095,  0.0192,  0.0195, -0.0017],\n",
      "        [ 0.5414,  0.5163,  0.5373,  0.6082,  0.5916,  0.6213],\n",
      "        [-0.0293, -0.0285, -0.0257, -0.0325, -0.0321, -0.0354],\n",
      "        [-0.0088, -0.0074, -0.0126, -0.0119, -0.0124,  0.0259],\n",
      "        [-0.1017, -0.1011, -0.1020, -0.1015, -0.1044, -0.1059],\n",
      "        [-0.0197, -0.0195, -0.0191, -0.0213, -0.0298, -0.0224],\n",
      "        [-0.0507, -0.0477, -0.0515, -0.0530, -0.0560, -0.0514],\n",
      "        [-0.0265, -0.0313, -0.0340, -0.0274, -0.0151, -0.0210],\n",
      "        [ 0.3107,  0.3352,  0.3558,  0.3321,  0.3178,  0.3866],\n",
      "        [ 0.1344,  0.1212,  0.1495,  0.1066,  0.1046,  0.0602]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([13, 1]), val: tensor([[-0.0718],\n",
      "        [-0.0506],\n",
      "        [-0.0766],\n",
      "        [-0.0701],\n",
      "        [ 0.0251],\n",
      "        [-0.0720],\n",
      "        [-0.0699],\n",
      "        [-0.0904],\n",
      "        [-0.0721],\n",
      "        [-0.0788],\n",
      "        [-0.0757],\n",
      "        [ 0.0098],\n",
      "        [-0.0250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 54502409451.6827, val: 54502409451.6827\n",
      "conv1 shape: torch.Size([13, 12]), val: tensor([[-144815.7889, -152182.4670, -169187.2241, -155748.8578, -166270.7842,\n",
      "         -138877.7637, -126123.6150, -127549.0558, -166708.6289, -193897.5254,\n",
      "         -166370.8423, -172410.4335],\n",
      "        [ 115790.7042,  122131.9195,  102455.6896,  111923.4588,   81213.8336,\n",
      "          113948.4398,   90043.8744,   76951.4695,   64955.3056,   99190.9043,\n",
      "           70859.3175,   48120.3921],\n",
      "        [ 218290.9446,  204065.6547,  228814.0243,  225370.1894,  235973.0809,\n",
      "          228821.6890,  220586.9214,  224111.9577,  236290.0132,  240948.7568,\n",
      "          216044.4139,  238398.5927],\n",
      "        [ 150101.0917,  175638.4975,  154601.1137,  155506.5598,  151928.5164,\n",
      "          136751.5461,  127164.2401,  115145.7354,  128007.4287,  102148.7192,\n",
      "          120873.2566,  116316.4555],\n",
      "        [  38278.5132,   48528.0923,   52831.2131,   52023.2121,   61207.4390,\n",
      "           85043.6956,   85601.1778,   93492.4092,   98026.0770,   84246.9569,\n",
      "           70965.9329,   79286.1909],\n",
      "        [ 192583.4497,  182560.7563,  179200.1772,  171495.3830,  182166.6512,\n",
      "          172131.4642,  157641.3793,  178151.6661,  140309.4890,  199228.8211,\n",
      "          202130.5112,  201046.4974],\n",
      "        [-152457.1951, -140475.1483, -144640.9057, -146552.9436, -138966.9962,\n",
      "         -161586.5129, -145810.6856, -153807.3018, -167284.3570, -198857.2881,\n",
      "         -175922.6727, -198974.4839],\n",
      "        [-226137.4246, -236693.1175, -231184.1447, -227898.2260, -237905.7543,\n",
      "         -219255.3680, -196226.5069, -205419.4332, -146761.8589, -175446.3476,\n",
      "         -198552.2240, -163449.4702],\n",
      "        [ -70578.3139,  -86917.0158,  -65518.3740,  -69393.0120,  -61880.9378,\n",
      "          -76157.1233,  -88543.3139,  -61395.6334, -106007.1727,  -94465.4110,\n",
      "          -99915.7840,  -84625.8876],\n",
      "        [  93187.7162,   99847.2099,   86186.5820,  106178.8961,   94833.4886,\n",
      "           80031.0808,   73573.7966,   39308.7113,   77049.8594,   37958.1240,\n",
      "           60445.9484,   48267.2894],\n",
      "        [-102398.0235, -105697.0862, -116977.2737, -131599.4038, -145796.2730,\n",
      "         -147918.1186, -173902.8123, -154533.6961, -134688.9085, -181578.6954,\n",
      "         -186242.3272, -172928.9118],\n",
      "        [ 147399.7592,  155399.0593,  136588.8567,  139085.4536,  147326.2452,\n",
      "          126890.8579,  116087.0361,  109649.0196,  121419.9907,   76802.2356,\n",
      "          128181.7247,   96524.7283],\n",
      "        [ 227470.2389,  224305.4216,  209354.7950,  232776.0927,  236635.1813,\n",
      "          231114.3433,  255099.9075,  230617.7443,  243738.0948,  251950.3743,\n",
      "          263982.4727,  270500.7219]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "bn1 shape: torch.Size([13, 12]), val: tensor([[-1.2060, -1.2365, -1.3303, -1.2439, -1.2704, -1.1255, -1.0465, -1.0676,\n",
      "         -1.3373, -1.3151, -1.1789, -1.2297],\n",
      "        [ 0.5185,  0.5495,  0.4614,  0.4959,  0.3004,  0.5286,  0.3988,  0.3355,\n",
      "          0.2387,  0.4945,  0.2935,  0.1542],\n",
      "        [ 1.1967,  1.0830,  1.2949,  1.2333,  1.2827,  1.2802,  1.2716,  1.3451,\n",
      "          1.4042,  1.3698,  1.1946,  1.3483],\n",
      "        [ 0.7455,  0.8979,  0.8054,  0.7792,  0.7492,  0.6778,  0.6470,  0.5975,\n",
      "          0.6676,  0.5128,  0.6039,  0.5822],\n",
      "        [ 0.0056,  0.0703,  0.1341,  0.1066,  0.1734,  0.3395,  0.3691,  0.4490,\n",
      "          0.4636,  0.4023,  0.2941,  0.3498],\n",
      "        [ 1.0266,  0.9430,  0.9676,  0.8831,  0.9412,  0.9093,  0.8507,  1.0298,\n",
      "          0.7513,  1.1122,  1.1082,  1.1139],\n",
      "        [-1.2566, -1.1603, -1.1684, -1.1841, -1.0971, -1.2741, -1.1781, -1.2477,\n",
      "         -1.3412, -1.3457, -1.2382, -1.3964],\n",
      "        [-1.7441, -1.7868, -1.7393, -1.7128, -1.7250, -1.6514, -1.5152, -1.6018,\n",
      "         -1.2016, -1.2011, -1.3787, -1.1735],\n",
      "        [-0.7148, -0.8116, -0.6465, -0.6826, -0.6078, -0.7152, -0.7952, -0.6137,\n",
      "         -0.9244, -0.7011, -0.7665, -0.6788],\n",
      "        [ 0.3689,  0.4044,  0.3541,  0.4586,  0.3869,  0.3067,  0.2887,  0.0772,\n",
      "          0.3209,  0.1165,  0.2288,  0.1552],\n",
      "        [-0.9253, -0.9339, -0.9860, -1.0869, -1.1404, -1.1847, -1.3659, -1.2527,\n",
      "         -1.1195, -1.2390, -1.3023, -1.2330],\n",
      "        [ 0.7276,  0.7661,  0.6866,  0.6724,  0.7200,  0.6133,  0.5729,  0.5598,\n",
      "          0.6228,  0.3563,  0.6493,  0.4580],\n",
      "        [ 1.2575,  1.2148,  1.1665,  1.2814,  1.2869,  1.2952,  1.5023,  1.3897,\n",
      "          1.4549,  1.4377,  1.4921,  1.5498]], dtype=torch.float64,\n",
      "       grad_fn=<NativeBatchNormBackward0>)\n",
      "relu shape: torch.Size([13, 12]), val: tensor([[-0.1206, -0.1237, -0.1330, -0.1244, -0.1270, -0.1126, -0.1046, -0.1068,\n",
      "         -0.1337, -0.1315, -0.1179, -0.1230],\n",
      "        [ 0.5185,  0.5495,  0.4614,  0.4959,  0.3004,  0.5286,  0.3988,  0.3355,\n",
      "          0.2387,  0.4945,  0.2935,  0.1542],\n",
      "        [ 1.1967,  1.0830,  1.2949,  1.2333,  1.2827,  1.2802,  1.2716,  1.3451,\n",
      "          1.4042,  1.3698,  1.1946,  1.3483],\n",
      "        [ 0.7455,  0.8979,  0.8054,  0.7792,  0.7492,  0.6778,  0.6470,  0.5975,\n",
      "          0.6676,  0.5128,  0.6039,  0.5822],\n",
      "        [ 0.0056,  0.0703,  0.1341,  0.1066,  0.1734,  0.3395,  0.3691,  0.4490,\n",
      "          0.4636,  0.4023,  0.2941,  0.3498],\n",
      "        [ 1.0266,  0.9430,  0.9676,  0.8831,  0.9412,  0.9093,  0.8507,  1.0298,\n",
      "          0.7513,  1.1122,  1.1082,  1.1139],\n",
      "        [-0.1257, -0.1160, -0.1168, -0.1184, -0.1097, -0.1274, -0.1178, -0.1248,\n",
      "         -0.1341, -0.1346, -0.1238, -0.1396],\n",
      "        [-0.1744, -0.1787, -0.1739, -0.1713, -0.1725, -0.1651, -0.1515, -0.1602,\n",
      "         -0.1202, -0.1201, -0.1379, -0.1173],\n",
      "        [-0.0715, -0.0812, -0.0647, -0.0683, -0.0608, -0.0715, -0.0795, -0.0614,\n",
      "         -0.0924, -0.0701, -0.0766, -0.0679],\n",
      "        [ 0.3689,  0.4044,  0.3541,  0.4586,  0.3869,  0.3067,  0.2887,  0.0772,\n",
      "          0.3209,  0.1165,  0.2288,  0.1552],\n",
      "        [-0.0925, -0.0934, -0.0986, -0.1087, -0.1140, -0.1185, -0.1366, -0.1253,\n",
      "         -0.1119, -0.1239, -0.1302, -0.1233],\n",
      "        [ 0.7276,  0.7661,  0.6866,  0.6724,  0.7200,  0.6133,  0.5729,  0.5598,\n",
      "          0.6228,  0.3563,  0.6493,  0.4580],\n",
      "        [ 1.2575,  1.2148,  1.1665,  1.2814,  1.2869,  1.2952,  1.5023,  1.3897,\n",
      "          1.4549,  1.4377,  1.4921,  1.5498]], dtype=torch.float64,\n",
      "       grad_fn=<LeakyReluBackward1>)\n",
      "pool shape: torch.Size([13, 6]), val: tensor([[-0.1221, -0.1287, -0.1198, -0.1057, -0.1326, -0.1204],\n",
      "        [ 0.5340,  0.4787,  0.4145,  0.3671,  0.3666,  0.2239],\n",
      "        [ 1.1399,  1.2641,  1.2814,  1.3083,  1.3870,  1.2715],\n",
      "        [ 0.8217,  0.7923,  0.7135,  0.6222,  0.5902,  0.5930],\n",
      "        [ 0.0379,  0.1203,  0.2565,  0.4090,  0.4329,  0.3220],\n",
      "        [ 0.9848,  0.9253,  0.9252,  0.9403,  0.9317,  1.1111],\n",
      "        [-0.1208, -0.1176, -0.1186, -0.1213, -0.1343, -0.1317],\n",
      "        [-0.1765, -0.1726, -0.1688, -0.1559, -0.1201, -0.1276],\n",
      "        [-0.0763, -0.0665, -0.0661, -0.0704, -0.0813, -0.0723],\n",
      "        [ 0.3867,  0.4063,  0.3468,  0.1829,  0.2187,  0.1920],\n",
      "        [-0.0930, -0.1036, -0.1163, -0.1309, -0.1179, -0.1268],\n",
      "        [ 0.7469,  0.6795,  0.6667,  0.5663,  0.4895,  0.5536],\n",
      "        [ 1.2361,  1.2239,  1.2910,  1.4460,  1.4463,  1.5210]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([13, 6]), val: tensor([[-0.0110, -0.0523, -0.0639, -0.0354, -0.0606,  0.0127],\n",
      "        [ 0.1500,  0.1805,  0.2109,  0.2858,  0.2978,  0.3066],\n",
      "        [-0.3802, -0.3633, -0.3514, -0.3302, -0.3186, -0.4009],\n",
      "        [ 0.0236,  0.0620,  0.0725,  0.0929,  0.1337,  0.1086],\n",
      "        [ 0.5874,  0.5822,  0.5771,  0.5936,  0.5919,  0.5712],\n",
      "        [-0.3547, -0.3072, -0.2677, -0.2342, -0.2132, -0.2409],\n",
      "        [-0.0506, -0.0660, -0.0792, -0.1230, -0.1534, -0.1248],\n",
      "        [-1.0019, -1.0070, -1.0068, -0.9972, -1.0115, -0.9877],\n",
      "        [-0.1787, -0.1626, -0.1415, -0.1110, -0.0971, -0.0739],\n",
      "        [-0.5353, -0.5572, -0.5572, -0.5511, -0.5757, -0.5270],\n",
      "        [-0.2550, -0.3205, -0.3786, -0.4690, -0.4949, -0.4996],\n",
      "        [ 0.3446,  0.3933,  0.4153,  0.4420,  0.4710,  0.4356],\n",
      "        [ 0.0746,  0.1135,  0.1432,  0.1803,  0.2286,  0.2025]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([13, 6]), val: tensor([[-0.0011, -0.0052, -0.0064, -0.0035, -0.0061,  0.0127],\n",
      "        [ 0.1500,  0.1805,  0.2109,  0.2858,  0.2978,  0.3066],\n",
      "        [-0.0380, -0.0363, -0.0351, -0.0330, -0.0319, -0.0401],\n",
      "        [ 0.0236,  0.0620,  0.0725,  0.0929,  0.1337,  0.1086],\n",
      "        [ 0.5874,  0.5822,  0.5771,  0.5936,  0.5919,  0.5712],\n",
      "        [-0.0355, -0.0307, -0.0268, -0.0234, -0.0213, -0.0241],\n",
      "        [-0.0051, -0.0066, -0.0079, -0.0123, -0.0153, -0.0125],\n",
      "        [-0.1002, -0.1007, -0.1007, -0.0997, -0.1012, -0.0988],\n",
      "        [-0.0179, -0.0163, -0.0141, -0.0111, -0.0097, -0.0074],\n",
      "        [-0.0535, -0.0557, -0.0557, -0.0551, -0.0576, -0.0527],\n",
      "        [-0.0255, -0.0320, -0.0379, -0.0469, -0.0495, -0.0500],\n",
      "        [ 0.3446,  0.3933,  0.4153,  0.4420,  0.4710,  0.4356],\n",
      "        [ 0.0746,  0.1135,  0.1432,  0.1803,  0.2286,  0.2025]],\n",
      "       dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([13, 6]), val: tensor([[-0.0011, -0.0052, -0.0064, -0.0035, -0.0061,  0.0127],\n",
      "        [ 0.1500,  0.1805,  0.2109,  0.2858,  0.2978,  0.3066],\n",
      "        [-0.0380, -0.0363, -0.0351, -0.0330, -0.0319, -0.0401],\n",
      "        [ 0.0236,  0.0620,  0.0725,  0.0929,  0.1337,  0.1086],\n",
      "        [ 0.5874,  0.5822,  0.5771,  0.5936,  0.5919,  0.5712],\n",
      "        [-0.0355, -0.0307, -0.0268, -0.0234, -0.0213, -0.0241],\n",
      "        [-0.0051, -0.0066, -0.0079, -0.0123, -0.0153, -0.0125],\n",
      "        [-0.1002, -0.1007, -0.1007, -0.0997, -0.1012, -0.0988],\n",
      "        [-0.0179, -0.0163, -0.0141, -0.0111, -0.0097, -0.0074],\n",
      "        [-0.0535, -0.0557, -0.0557, -0.0551, -0.0576, -0.0527],\n",
      "        [-0.0255, -0.0320, -0.0379, -0.0469, -0.0495, -0.0500],\n",
      "        [ 0.3446,  0.3933,  0.4153,  0.4420,  0.4710,  0.4356],\n",
      "        [ 0.0746,  0.1135,  0.1432,  0.1803,  0.2286,  0.2025]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([13, 6]), val: tensor([[-0.0011, -0.0052, -0.0064, -0.0035, -0.0061,  0.0127],\n",
      "        [ 0.1500,  0.1805,  0.2109,  0.2858,  0.2978,  0.3066],\n",
      "        [-0.0380, -0.0363, -0.0351, -0.0330, -0.0319, -0.0401],\n",
      "        [ 0.0236,  0.0620,  0.0725,  0.0929,  0.1337,  0.1086],\n",
      "        [ 0.5874,  0.5822,  0.5771,  0.5936,  0.5919,  0.5712],\n",
      "        [-0.0355, -0.0307, -0.0268, -0.0234, -0.0213, -0.0241],\n",
      "        [-0.0051, -0.0066, -0.0079, -0.0123, -0.0153, -0.0125],\n",
      "        [-0.1002, -0.1007, -0.1007, -0.0997, -0.1012, -0.0988],\n",
      "        [-0.0179, -0.0163, -0.0141, -0.0111, -0.0097, -0.0074],\n",
      "        [-0.0535, -0.0557, -0.0557, -0.0551, -0.0576, -0.0527],\n",
      "        [-0.0255, -0.0320, -0.0379, -0.0469, -0.0495, -0.0500],\n",
      "        [ 0.3446,  0.3933,  0.4153,  0.4420,  0.4710,  0.4356],\n",
      "        [ 0.0746,  0.1135,  0.1432,  0.1803,  0.2286,  0.2025]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([13, 1]), val: tensor([[-0.0695],\n",
      "        [-0.0541],\n",
      "        [-0.0768],\n",
      "        [-0.0638],\n",
      "        [ 0.0512],\n",
      "        [-0.0761],\n",
      "        [-0.0682],\n",
      "        [-0.0900],\n",
      "        [-0.0725],\n",
      "        [-0.0800],\n",
      "        [-0.0719],\n",
      "        [ 0.0037],\n",
      "        [-0.0559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 58605333331.74775, val: 58605333331.74775\n",
      "conv1 shape: torch.Size([13, 12]), val: tensor([[-147040.7425, -147830.9206, -145443.9909, -133259.2519, -129560.7816,\n",
      "         -139421.1141, -152599.5205, -139818.6896, -139988.1910, -120934.7931,\n",
      "         -122445.6292, -144902.2770],\n",
      "        [ 105851.2036,   84728.0855,   77764.3547,   92771.7474,   93623.4470,\n",
      "           91880.2314,  103822.8158,  108904.0970,  111304.1638,   84506.6968,\n",
      "           70244.1450,   61356.1862],\n",
      "        [ 191905.6884,  187580.8490,  198926.7156,  202784.6174,  204131.6302,\n",
      "          209825.6800,  210085.8460,  189297.4476,  188331.9081,  154872.2580,\n",
      "          179493.1099,  235939.3488],\n",
      "        [ 137413.7166,  134142.5177,  124264.6928,  120739.5325,  128017.0622,\n",
      "          134177.4590,  123517.3034,  110363.2797,   90695.6128,   99311.3599,\n",
      "          101917.6275,   68881.5476],\n",
      "        [  52615.0624,   43581.3833,   42942.0852,   29600.4771,   40802.9291,\n",
      "           37488.9788,   16287.4486,   -9101.3157,   14817.8681,   37585.3205,\n",
      "           68284.5379,   53615.3117],\n",
      "        [ 207924.3567,  202951.9432,  190420.3838,  193362.0004,  186561.8426,\n",
      "          192158.8903,  208817.2886,  203640.8989,  210274.4885,  198231.3985,\n",
      "          187431.0371,  211701.1308],\n",
      "        [-156568.3733, -158075.9908, -163116.3860, -172080.5839, -163770.0473,\n",
      "         -162159.8121, -147479.3190, -130815.2554, -127802.1815, -138423.3096,\n",
      "         -157708.0458, -156523.3540],\n",
      "        [-206109.0879, -199120.3420, -197098.5202, -192997.4690, -197179.6543,\n",
      "         -182913.7726, -173902.3635, -156267.6311, -157143.6813, -135213.9320,\n",
      "         -156695.9607, -150068.4352],\n",
      "        [ -70510.4113,  -67582.5941,  -68988.3394,  -80224.5651,  -69742.2239,\n",
      "          -76991.3229,  -64877.0987,  -62081.9091,  -68192.8274, -101273.3269,\n",
      "         -126278.0095, -106996.2406],\n",
      "        [  69248.1155,   64479.8206,   71300.8645,   50955.3937,   55768.4150,\n",
      "           55088.2390,   33453.7801,   32503.3598,   26051.1569,     835.1845,\n",
      "           11888.8581,    2287.2336],\n",
      "        [-108433.9334,  -98289.2767,  -94260.9115,  -94007.2025,  -98157.1491,\n",
      "          -95055.5182,  -93680.8629,  -85218.3270, -100659.3311, -120843.2887,\n",
      "         -146615.9248, -154496.2788],\n",
      "        [ 132455.1874,  133436.7032,  130836.0198,  118753.1581,  117280.2371,\n",
      "          125359.2279,  111167.3047,   99657.8905,   95188.5580,   98287.7552,\n",
      "           68808.4091,   73547.0869],\n",
      "        [ 198287.3025,  198192.0374,  188899.7985,  182269.2809,  181450.8218,\n",
      "          177745.2409,  151984.9376,  159687.0509,  155475.8107,  171157.6650,\n",
      "          160542.6211,  181978.3439]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "bn1 shape: torch.Size([13, 12]), val: tensor([[-1.2441, -1.2647, -1.2527, -1.1503, -1.1496, -1.2265, -1.3342, -1.3189,\n",
      "         -1.3039, -1.1348, -1.0272, -1.1397],\n",
      "        [ 0.5200,  0.3977,  0.3649,  0.4977,  0.4906,  0.4655,  0.5908,  0.6754,\n",
      "          0.7071,  0.5487,  0.4596,  0.3437],\n",
      "        [ 1.1202,  1.1329,  1.2429,  1.2997,  1.3028,  1.3283,  1.3885,  1.3200,\n",
      "          1.3235,  1.1254,  1.3025,  1.5993],\n",
      "        [ 0.7401,  0.7509,  0.7018,  0.7016,  0.7434,  0.7749,  0.7387,  0.6871,\n",
      "          0.5421,  0.6701,  0.7040,  0.3978],\n",
      "        [ 0.1486,  0.1036,  0.1125,  0.0371,  0.1024,  0.0676, -0.0663, -0.2708,\n",
      "         -0.0651,  0.1642,  0.4445,  0.2880],\n",
      "        [ 1.2320,  1.2428,  1.1813,  1.2310,  1.1737,  1.1991,  1.3790,  1.4350,\n",
      "          1.4991,  1.4807,  1.3638,  1.4250],\n",
      "        [-1.3105, -1.3379, -1.3808, -1.4333, -1.4010, -1.3929, -1.2957, -1.2468,\n",
      "         -1.2064, -1.2781, -1.2993, -1.2232],\n",
      "        [-1.6561, -1.6313, -1.6271, -1.5858, -1.6466, -1.5447, -1.4941, -1.4508,\n",
      "         -1.4412, -1.2518, -1.2915, -1.1768],\n",
      "        [-0.7103, -0.6911, -0.6987, -0.7636, -0.7100, -0.7698, -0.6756, -0.6956,\n",
      "         -0.7294, -0.9737, -1.0568, -0.8670],\n",
      "        [ 0.2646,  0.2530,  0.3180,  0.1928,  0.2124,  0.1964,  0.0625,  0.0628,\n",
      "          0.0248, -0.1369,  0.0093, -0.0811],\n",
      "        [-0.9748, -0.9106, -0.8818, -0.8641, -0.9188, -0.9020, -0.8919, -0.8811,\n",
      "         -0.9892, -1.1341, -1.2137, -1.2087],\n",
      "        [ 0.7055,  0.7459,  0.7495,  0.6871,  0.6645,  0.7104,  0.6459,  0.6013,\n",
      "          0.5781,  0.6617,  0.4485,  0.4314],\n",
      "        [ 1.1647,  1.2088,  1.1703,  1.1502,  1.1361,  1.0936,  0.9524,  1.0826,\n",
      "          1.0606,  1.2588,  1.1563,  1.2112]], dtype=torch.float64,\n",
      "       grad_fn=<NativeBatchNormBackward0>)\n",
      "relu shape: torch.Size([13, 12]), val: tensor([[-0.1244, -0.1265, -0.1253, -0.1150, -0.1150, -0.1227, -0.1334, -0.1319,\n",
      "         -0.1304, -0.1135, -0.1027, -0.1140],\n",
      "        [ 0.5200,  0.3977,  0.3649,  0.4977,  0.4906,  0.4655,  0.5908,  0.6754,\n",
      "          0.7071,  0.5487,  0.4596,  0.3437],\n",
      "        [ 1.1202,  1.1329,  1.2429,  1.2997,  1.3028,  1.3283,  1.3885,  1.3200,\n",
      "          1.3235,  1.1254,  1.3025,  1.5993],\n",
      "        [ 0.7401,  0.7509,  0.7018,  0.7016,  0.7434,  0.7749,  0.7387,  0.6871,\n",
      "          0.5421,  0.6701,  0.7040,  0.3978],\n",
      "        [ 0.1486,  0.1036,  0.1125,  0.0371,  0.1024,  0.0676, -0.0066, -0.0271,\n",
      "         -0.0065,  0.1642,  0.4445,  0.2880],\n",
      "        [ 1.2320,  1.2428,  1.1813,  1.2310,  1.1737,  1.1991,  1.3790,  1.4350,\n",
      "          1.4991,  1.4807,  1.3638,  1.4250],\n",
      "        [-0.1311, -0.1338, -0.1381, -0.1433, -0.1401, -0.1393, -0.1296, -0.1247,\n",
      "         -0.1206, -0.1278, -0.1299, -0.1223],\n",
      "        [-0.1656, -0.1631, -0.1627, -0.1586, -0.1647, -0.1545, -0.1494, -0.1451,\n",
      "         -0.1441, -0.1252, -0.1291, -0.1177],\n",
      "        [-0.0710, -0.0691, -0.0699, -0.0764, -0.0710, -0.0770, -0.0676, -0.0696,\n",
      "         -0.0729, -0.0974, -0.1057, -0.0867],\n",
      "        [ 0.2646,  0.2530,  0.3180,  0.1928,  0.2124,  0.1964,  0.0625,  0.0628,\n",
      "          0.0248, -0.0137,  0.0093, -0.0081],\n",
      "        [-0.0975, -0.0911, -0.0882, -0.0864, -0.0919, -0.0902, -0.0892, -0.0881,\n",
      "         -0.0989, -0.1134, -0.1214, -0.1209],\n",
      "        [ 0.7055,  0.7459,  0.7495,  0.6871,  0.6645,  0.7104,  0.6459,  0.6013,\n",
      "          0.5781,  0.6617,  0.4485,  0.4314],\n",
      "        [ 1.1647,  1.2088,  1.1703,  1.1502,  1.1361,  1.0936,  0.9524,  1.0826,\n",
      "          1.0606,  1.2588,  1.1563,  1.2112]], dtype=torch.float64,\n",
      "       grad_fn=<LeakyReluBackward1>)\n",
      "pool shape: torch.Size([13, 6]), val: tensor([[-1.2544e-01, -1.2015e-01, -1.1881e-01, -1.3266e-01, -1.2194e-01,\n",
      "         -1.0834e-01],\n",
      "        [ 4.5883e-01,  4.3125e-01,  4.7807e-01,  6.3310e-01,  6.2790e-01,\n",
      "          4.0165e-01],\n",
      "        [ 1.1266e+00,  1.2713e+00,  1.3156e+00,  1.3543e+00,  1.2244e+00,\n",
      "          1.4509e+00],\n",
      "        [ 7.4553e-01,  7.0170e-01,  7.5917e-01,  7.1287e-01,  6.0610e-01,\n",
      "          5.5090e-01],\n",
      "        [ 1.2609e-01,  7.4794e-02,  8.5034e-02, -1.6857e-02,  7.8855e-02,\n",
      "          3.6625e-01],\n",
      "        [ 1.2374e+00,  1.2061e+00,  1.1864e+00,  1.4070e+00,  1.4899e+00,\n",
      "          1.3944e+00],\n",
      "        [-1.3242e-01, -1.4071e-01, -1.3969e-01, -1.2712e-01, -1.2423e-01,\n",
      "         -1.2613e-01],\n",
      "        [-1.6437e-01, -1.6064e-01, -1.5956e-01, -1.4725e-01, -1.3465e-01,\n",
      "         -1.2341e-01],\n",
      "        [-7.0066e-02, -7.3114e-02, -7.3991e-02, -6.8563e-02, -8.5155e-02,\n",
      "         -9.6190e-02],\n",
      "        [ 2.5880e-01,  2.5540e-01,  2.0440e-01,  6.2663e-02,  5.5606e-03,\n",
      "          6.0107e-04],\n",
      "        [-9.4268e-02, -8.7295e-02, -9.1040e-02, -8.8651e-02, -1.0616e-01,\n",
      "         -1.2112e-01],\n",
      "        [ 7.2571e-01,  7.1827e-01,  6.8746e-01,  6.2360e-01,  6.1988e-01,\n",
      "          4.3995e-01],\n",
      "        [ 1.1868e+00,  1.1602e+00,  1.1149e+00,  1.0175e+00,  1.1597e+00,\n",
      "          1.1838e+00]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([13, 6]), val: tensor([[ 0.0061, -0.0132, -0.0147,  0.0309,  0.0693, -0.0020],\n",
      "        [ 0.2170,  0.2330,  0.2553,  0.3010,  0.3034,  0.3994],\n",
      "        [-0.3864, -0.4097, -0.3911, -0.3842, -0.3767, -0.3443],\n",
      "        [ 0.0467,  0.0791,  0.0714,  0.1129,  0.1120,  0.1820],\n",
      "        [ 0.6021,  0.5913,  0.6180,  0.6707,  0.6717,  0.6630],\n",
      "        [-0.3392, -0.2994, -0.3060, -0.3311, -0.3321, -0.2260],\n",
      "        [ 0.0034, -0.0373, -0.0239, -0.0048, -0.0170, -0.0366],\n",
      "        [-0.9855, -0.9856, -0.9874, -0.9727, -0.9803, -0.9921],\n",
      "        [-0.1002, -0.0981, -0.0954, -0.0714, -0.0596,  0.0203],\n",
      "        [-0.5315, -0.5682, -0.5767, -0.6094, -0.5836, -0.5986],\n",
      "        [-0.3351, -0.3575, -0.3674, -0.3565, -0.3617, -0.5671],\n",
      "        [ 0.3809,  0.4025,  0.4229,  0.4328,  0.4140,  0.5450],\n",
      "        [ 0.0964,  0.1271,  0.1135,  0.1061,  0.1172,  0.2087]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([13, 6]), val: tensor([[ 6.1076e-03, -1.3162e-03, -1.4703e-03,  3.0886e-02,  6.9265e-02,\n",
      "         -1.9766e-04],\n",
      "        [ 2.1699e-01,  2.3296e-01,  2.5532e-01,  3.0100e-01,  3.0344e-01,\n",
      "          3.9943e-01],\n",
      "        [-3.8643e-02, -4.0967e-02, -3.9113e-02, -3.8419e-02, -3.7666e-02,\n",
      "         -3.4434e-02],\n",
      "        [ 4.6676e-02,  7.9073e-02,  7.1446e-02,  1.1287e-01,  1.1205e-01,\n",
      "          1.8203e-01],\n",
      "        [ 6.0212e-01,  5.9127e-01,  6.1798e-01,  6.7067e-01,  6.7170e-01,\n",
      "          6.6298e-01],\n",
      "        [-3.3921e-02, -2.9939e-02, -3.0601e-02, -3.3115e-02, -3.3212e-02,\n",
      "         -2.2595e-02],\n",
      "        [ 3.4155e-03, -3.7315e-03, -2.3922e-03, -4.7897e-04, -1.7009e-03,\n",
      "         -3.6586e-03],\n",
      "        [-9.8552e-02, -9.8555e-02, -9.8745e-02, -9.7271e-02, -9.8026e-02,\n",
      "         -9.9208e-02],\n",
      "        [-1.0023e-02, -9.8093e-03, -9.5391e-03, -7.1376e-03, -5.9623e-03,\n",
      "          2.0271e-02],\n",
      "        [-5.3147e-02, -5.6819e-02, -5.7666e-02, -6.0942e-02, -5.8355e-02,\n",
      "         -5.9859e-02],\n",
      "        [-3.3510e-02, -3.5751e-02, -3.6743e-02, -3.5653e-02, -3.6175e-02,\n",
      "         -5.6713e-02],\n",
      "        [ 3.8086e-01,  4.0246e-01,  4.2287e-01,  4.3279e-01,  4.1403e-01,\n",
      "          5.4503e-01],\n",
      "        [ 9.6360e-02,  1.2715e-01,  1.1346e-01,  1.0613e-01,  1.1717e-01,\n",
      "          2.0875e-01]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([13, 6]), val: tensor([[ 6.1076e-03, -1.3162e-03, -1.4703e-03,  3.0886e-02,  6.9265e-02,\n",
      "         -1.9766e-04],\n",
      "        [ 2.1699e-01,  2.3296e-01,  2.5532e-01,  3.0100e-01,  3.0344e-01,\n",
      "          3.9943e-01],\n",
      "        [-3.8643e-02, -4.0967e-02, -3.9113e-02, -3.8419e-02, -3.7666e-02,\n",
      "         -3.4434e-02],\n",
      "        [ 4.6676e-02,  7.9073e-02,  7.1446e-02,  1.1287e-01,  1.1205e-01,\n",
      "          1.8203e-01],\n",
      "        [ 6.0212e-01,  5.9127e-01,  6.1798e-01,  6.7067e-01,  6.7170e-01,\n",
      "          6.6298e-01],\n",
      "        [-3.3921e-02, -2.9939e-02, -3.0601e-02, -3.3115e-02, -3.3212e-02,\n",
      "         -2.2595e-02],\n",
      "        [ 3.4155e-03, -3.7315e-03, -2.3922e-03, -4.7897e-04, -1.7009e-03,\n",
      "         -3.6586e-03],\n",
      "        [-9.8552e-02, -9.8555e-02, -9.8745e-02, -9.7271e-02, -9.8026e-02,\n",
      "         -9.9208e-02],\n",
      "        [-1.0023e-02, -9.8093e-03, -9.5391e-03, -7.1376e-03, -5.9623e-03,\n",
      "          2.0271e-02],\n",
      "        [-5.3147e-02, -5.6819e-02, -5.7666e-02, -6.0942e-02, -5.8355e-02,\n",
      "         -5.9859e-02],\n",
      "        [-3.3510e-02, -3.5751e-02, -3.6743e-02, -3.5653e-02, -3.6175e-02,\n",
      "         -5.6713e-02],\n",
      "        [ 3.8086e-01,  4.0246e-01,  4.2287e-01,  4.3279e-01,  4.1403e-01,\n",
      "          5.4503e-01],\n",
      "        [ 9.6360e-02,  1.2715e-01,  1.1346e-01,  1.0613e-01,  1.1717e-01,\n",
      "          2.0875e-01]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([13, 6]), val: tensor([[ 6.1076e-03, -1.3162e-03, -1.4703e-03,  3.0886e-02,  6.9265e-02,\n",
      "         -1.9766e-04],\n",
      "        [ 2.1699e-01,  2.3296e-01,  2.5532e-01,  3.0100e-01,  3.0344e-01,\n",
      "          3.9943e-01],\n",
      "        [-3.8643e-02, -4.0967e-02, -3.9113e-02, -3.8419e-02, -3.7666e-02,\n",
      "         -3.4434e-02],\n",
      "        [ 4.6676e-02,  7.9073e-02,  7.1446e-02,  1.1287e-01,  1.1205e-01,\n",
      "          1.8203e-01],\n",
      "        [ 6.0212e-01,  5.9127e-01,  6.1798e-01,  6.7067e-01,  6.7170e-01,\n",
      "          6.6298e-01],\n",
      "        [-3.3921e-02, -2.9939e-02, -3.0601e-02, -3.3115e-02, -3.3212e-02,\n",
      "         -2.2595e-02],\n",
      "        [ 3.4155e-03, -3.7315e-03, -2.3922e-03, -4.7897e-04, -1.7009e-03,\n",
      "         -3.6586e-03],\n",
      "        [-9.8552e-02, -9.8555e-02, -9.8745e-02, -9.7271e-02, -9.8026e-02,\n",
      "         -9.9208e-02],\n",
      "        [-1.0023e-02, -9.8093e-03, -9.5391e-03, -7.1376e-03, -5.9623e-03,\n",
      "          2.0271e-02],\n",
      "        [-5.3147e-02, -5.6819e-02, -5.7666e-02, -6.0942e-02, -5.8355e-02,\n",
      "         -5.9859e-02],\n",
      "        [-3.3510e-02, -3.5751e-02, -3.6743e-02, -3.5653e-02, -3.6175e-02,\n",
      "         -5.6713e-02],\n",
      "        [ 3.8086e-01,  4.0246e-01,  4.2287e-01,  4.3279e-01,  4.1403e-01,\n",
      "          5.4503e-01],\n",
      "        [ 9.6360e-02,  1.2715e-01,  1.1346e-01,  1.0613e-01,  1.1717e-01,\n",
      "          2.0875e-01]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([13, 1]), val: tensor([[-0.0762],\n",
      "        [-0.0310],\n",
      "        [-0.0765],\n",
      "        [-0.0686],\n",
      "        [ 0.0458],\n",
      "        [-0.0742],\n",
      "        [-0.0685],\n",
      "        [-0.0899],\n",
      "        [-0.0704],\n",
      "        [-0.0786],\n",
      "        [-0.0764],\n",
      "        [ 0.0149],\n",
      "        [-0.0417]], dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 54223252994.61732, val: 54223252994.61732\n",
      "conv1 shape: torch.Size([13, 12]), val: tensor([[-138890.5740, -133221.3017, -127512.3711, -118984.8816, -144261.8490,\n",
      "         -137386.0795, -113842.8308, -123942.8732, -136219.8018, -103507.6390,\n",
      "         -115434.8957, -160801.5235],\n",
      "        [ 137287.5144,  142264.1967,  118928.6877,  144109.6547,  123195.9705,\n",
      "          142361.9358,  151075.1147,  132149.8350,  113400.4994,  149592.9900,\n",
      "          146600.9665,  145745.0255],\n",
      "        [ 220194.5459,  243381.7208,  229511.2398,  231937.8796,  239680.3689,\n",
      "          240594.0456,  213932.4944,  205573.2501,  212385.1235,  195585.9533,\n",
      "          214132.8880,  203623.0396],\n",
      "        [ 133573.9129,  151475.6027,  146916.8029,  126326.2412,  149204.6115,\n",
      "          141795.1620,  137369.4756,  120955.5863,   87364.4701,   82635.1776,\n",
      "          135068.7216,  127245.8123],\n",
      "        [  27338.2893,   19659.3025,   16439.4275,   51748.9125,   32843.7934,\n",
      "           31659.2299,   17798.8076,   11155.0946,   29062.6269,   49339.6268,\n",
      "           91268.7073,   84994.8081],\n",
      "        [ 175966.9274,  193171.1610,  184508.1535,  188848.9347,  195799.5882,\n",
      "          190969.5041,  184653.6086,  168767.4283,  186151.5822,  179696.3078,\n",
      "          194135.0153,  182618.4391],\n",
      "        [-155580.5176, -159565.3851, -157458.1277, -186599.9026, -154944.1650,\n",
      "         -162362.1732, -155902.6949, -158169.0887, -170748.0246, -191857.5727,\n",
      "         -199244.2144, -154656.2113],\n",
      "        [-206724.6267, -223627.0038, -215260.4809, -232419.9400, -214820.4534,\n",
      "         -227064.5518, -215509.3854, -181807.5458, -216664.6794, -167835.3379,\n",
      "         -199174.7880, -214496.2231],\n",
      "        [-107363.9401, -104180.8410,  -76373.6030, -104172.0354, -115217.4618,\n",
      "         -102574.6630, -116202.1035,  -87241.4580,  -57977.8150, -104033.3062,\n",
      "         -126311.9808, -110655.4299],\n",
      "        [  67652.7981,   68916.6669,   61681.5790,   87245.1849,   60872.5041,\n",
      "           76855.3019,   63276.9256,   47263.9102,   54505.1937,   39371.2569,\n",
      "           77902.8704,   85931.2737],\n",
      "        [-135992.8612, -137849.9881, -132526.1578, -151522.3881, -151902.4317,\n",
      "         -139581.1650, -114845.8732, -102409.3075, -114121.2871, -111599.5134,\n",
      "         -132468.3583, -153277.1943],\n",
      "        [  94691.1778,   82974.3543,   85406.4338,   79428.1318,  101429.9743,\n",
      "           83878.1204,   83876.7641,   73720.7192,   51425.4849,   71723.0929,\n",
      "           84865.6940,   86060.0190],\n",
      "        [ 210080.8323,  204543.1853,  213002.8541,  218538.9802,  245152.0647,\n",
      "          226912.6916,  216819.8403,  206332.3984,  166634.7946,  244074.1543,\n",
      "          241523.9528,  245872.9746]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "bn1 shape: torch.Size([13, 12]), val: tensor([[-1.1097, -1.0405, -1.0496, -0.9242, -1.0904, -1.0607, -0.9592, -1.0954,\n",
      "         -1.1134, -0.9151, -0.9437, -1.2134],\n",
      "        [ 0.7627,  0.7512,  0.6276,  0.7560,  0.6003,  0.7317,  0.8435,  0.8004,\n",
      "          0.7150,  0.8784,  0.7363,  0.7534],\n",
      "        [ 1.3248,  1.4088,  1.3801,  1.3169,  1.3366,  1.3612,  1.2712,  1.3439,\n",
      "          1.4401,  1.2044,  1.1692,  1.1247],\n",
      "        [ 0.7375,  0.8111,  0.8180,  0.6424,  0.7647,  0.7281,  0.7502,  0.7175,\n",
      "          0.5243,  0.4040,  0.6623,  0.6347],\n",
      "        [ 0.0173, -0.0462, -0.0699,  0.1662,  0.0291,  0.0224, -0.0634, -0.0953,\n",
      "          0.0973,  0.1680,  0.3815,  0.3636],\n",
      "        [ 1.0250,  1.0822,  1.0739,  1.0417,  1.0592,  1.0432,  1.0720,  1.0715,\n",
      "          1.2480,  1.0918,  1.0410,  0.9899],\n",
      "        [-1.2228, -1.2118, -1.2534, -1.3560, -1.1579, -1.2207, -1.2453, -1.3487,\n",
      "         -1.3664, -1.5412, -1.4810, -1.1740],\n",
      "        [-1.5696, -1.6284, -1.6467, -1.6486, -1.5364, -1.6353, -1.6509, -1.5237,\n",
      "         -1.7027, -1.3710, -1.4805, -1.5579],\n",
      "        [-0.8959, -0.8516, -0.7015, -0.8296, -0.9068, -0.8377, -0.9752, -0.8237,\n",
      "         -0.5403, -0.9188, -1.0134, -0.8917],\n",
      "        [ 0.2906,  0.2741,  0.2380,  0.3929,  0.2063,  0.3120,  0.2461,  0.1720,\n",
      "          0.2836,  0.0974,  0.2958,  0.3696],\n",
      "        [-1.0900, -1.0706, -1.0837, -1.1320, -1.1387, -1.0748, -0.9660, -0.9360,\n",
      "         -0.9516, -0.9724, -1.0529, -1.1651],\n",
      "        [ 0.4739,  0.3656,  0.3994,  0.3429,  0.4627,  0.3570,  0.3862,  0.3679,\n",
      "          0.2611,  0.3266,  0.3405,  0.3704],\n",
      "        [ 1.2562,  1.1562,  1.2678,  1.2313,  1.3712,  1.2735,  1.2908,  1.3495,\n",
      "          1.1050,  1.5480,  1.3448,  1.3958]], dtype=torch.float64,\n",
      "       grad_fn=<NativeBatchNormBackward0>)\n",
      "relu shape: torch.Size([13, 12]), val: tensor([[-0.1110, -0.1040, -0.1050, -0.0924, -0.1090, -0.1061, -0.0959, -0.1095,\n",
      "         -0.1113, -0.0915, -0.0944, -0.1213],\n",
      "        [ 0.7627,  0.7512,  0.6276,  0.7560,  0.6003,  0.7317,  0.8435,  0.8004,\n",
      "          0.7150,  0.8784,  0.7363,  0.7534],\n",
      "        [ 1.3248,  1.4088,  1.3801,  1.3169,  1.3366,  1.3612,  1.2712,  1.3439,\n",
      "          1.4401,  1.2044,  1.1692,  1.1247],\n",
      "        [ 0.7375,  0.8111,  0.8180,  0.6424,  0.7647,  0.7281,  0.7502,  0.7175,\n",
      "          0.5243,  0.4040,  0.6623,  0.6347],\n",
      "        [ 0.0173, -0.0046, -0.0070,  0.1662,  0.0291,  0.0224, -0.0063, -0.0095,\n",
      "          0.0973,  0.1680,  0.3815,  0.3636],\n",
      "        [ 1.0250,  1.0822,  1.0739,  1.0417,  1.0592,  1.0432,  1.0720,  1.0715,\n",
      "          1.2480,  1.0918,  1.0410,  0.9899],\n",
      "        [-0.1223, -0.1212, -0.1253, -0.1356, -0.1158, -0.1221, -0.1245, -0.1349,\n",
      "         -0.1366, -0.1541, -0.1481, -0.1174],\n",
      "        [-0.1570, -0.1628, -0.1647, -0.1649, -0.1536, -0.1635, -0.1651, -0.1524,\n",
      "         -0.1703, -0.1371, -0.1481, -0.1558],\n",
      "        [-0.0896, -0.0852, -0.0702, -0.0830, -0.0907, -0.0838, -0.0975, -0.0824,\n",
      "         -0.0540, -0.0919, -0.1013, -0.0892],\n",
      "        [ 0.2906,  0.2741,  0.2380,  0.3929,  0.2063,  0.3120,  0.2461,  0.1720,\n",
      "          0.2836,  0.0974,  0.2958,  0.3696],\n",
      "        [-0.1090, -0.1071, -0.1084, -0.1132, -0.1139, -0.1075, -0.0966, -0.0936,\n",
      "         -0.0952, -0.0972, -0.1053, -0.1165],\n",
      "        [ 0.4739,  0.3656,  0.3994,  0.3429,  0.4627,  0.3570,  0.3862,  0.3679,\n",
      "          0.2611,  0.3266,  0.3405,  0.3704],\n",
      "        [ 1.2562,  1.1562,  1.2678,  1.2313,  1.3712,  1.2735,  1.2908,  1.3495,\n",
      "          1.1050,  1.5480,  1.3448,  1.3958]], dtype=torch.float64,\n",
      "       grad_fn=<LeakyReluBackward1>)\n",
      "pool shape: torch.Size([13, 6]), val: tensor([[-0.1075, -0.0987, -0.1076, -0.1027, -0.1014, -0.1079],\n",
      "        [ 0.7569,  0.6918,  0.6660,  0.8219,  0.7967,  0.7448],\n",
      "        [ 1.3668,  1.3485,  1.3489,  1.3076,  1.3222,  1.1470],\n",
      "        [ 0.7743,  0.7302,  0.7464,  0.7339,  0.4641,  0.6485],\n",
      "        [ 0.0063,  0.0796,  0.0258, -0.0079,  0.1326,  0.3726],\n",
      "        [ 1.0536,  1.0578,  1.0512,  1.0717,  1.1699,  1.0155],\n",
      "        [-0.1217, -0.1305, -0.1189, -0.1297, -0.1454, -0.1327],\n",
      "        [-0.1599, -0.1648, -0.1586, -0.1587, -0.1537, -0.1519],\n",
      "        [-0.0874, -0.0766, -0.0872, -0.0899, -0.0730, -0.0953],\n",
      "        [ 0.2824,  0.3154,  0.2592,  0.2090,  0.1905,  0.3327],\n",
      "        [-0.1080, -0.1108, -0.1107, -0.0951, -0.0962, -0.1109],\n",
      "        [ 0.4197,  0.3712,  0.4099,  0.3770,  0.2939,  0.3555],\n",
      "        [ 1.2062,  1.2496,  1.3223,  1.3202,  1.3265,  1.3703]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([13, 6]), val: tensor([[ 0.0285,  0.0228,  0.0512,  0.0753,  0.0435, -0.0031],\n",
      "        [ 0.2552,  0.2683,  0.2698,  0.2666,  0.2914,  0.2369],\n",
      "        [-0.3055, -0.2928, -0.3292, -0.2933, -0.2528, -0.2013],\n",
      "        [ 0.1488,  0.1711,  0.1470,  0.1518,  0.2456,  0.1547],\n",
      "        [ 0.6778,  0.6627,  0.6621,  0.6900,  0.6588,  0.6646],\n",
      "        [-0.3511, -0.3360, -0.3419, -0.3753, -0.3053, -0.3410],\n",
      "        [-0.1002, -0.1018, -0.1289, -0.1421, -0.1744, -0.0939],\n",
      "        [-1.0359, -1.0362, -1.0250, -1.0283, -1.0088, -1.0607],\n",
      "        [-0.1442, -0.1254, -0.1391, -0.1552, -0.1144, -0.1229],\n",
      "        [-0.5958, -0.5804, -0.5778, -0.5863, -0.6095, -0.5477],\n",
      "        [-0.3280, -0.3715, -0.3580, -0.3141, -0.3759, -0.3702],\n",
      "        [ 0.4286,  0.4382,  0.4197,  0.3937,  0.4124,  0.4240],\n",
      "        [ 0.1144,  0.1376,  0.1283,  0.1160,  0.1997,  0.1578]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([13, 6]), val: tensor([[ 2.8528e-02,  2.2772e-02,  5.1173e-02,  7.5317e-02,  4.3460e-02,\n",
      "         -3.1287e-04],\n",
      "        [ 2.5524e-01,  2.6834e-01,  2.6978e-01,  2.6658e-01,  2.9136e-01,\n",
      "          2.3691e-01],\n",
      "        [-3.0547e-02, -2.9284e-02, -3.2922e-02, -2.9329e-02, -2.5277e-02,\n",
      "         -2.0134e-02],\n",
      "        [ 1.4884e-01,  1.7111e-01,  1.4696e-01,  1.5179e-01,  2.4561e-01,\n",
      "          1.5465e-01],\n",
      "        [ 6.7782e-01,  6.6272e-01,  6.6207e-01,  6.9004e-01,  6.5881e-01,\n",
      "          6.6455e-01],\n",
      "        [-3.5112e-02, -3.3598e-02, -3.4191e-02, -3.7526e-02, -3.0535e-02,\n",
      "         -3.4101e-02],\n",
      "        [-1.0018e-02, -1.0177e-02, -1.2892e-02, -1.4211e-02, -1.7442e-02,\n",
      "         -9.3859e-03],\n",
      "        [-1.0359e-01, -1.0362e-01, -1.0250e-01, -1.0283e-01, -1.0088e-01,\n",
      "         -1.0607e-01],\n",
      "        [-1.4417e-02, -1.2538e-02, -1.3909e-02, -1.5524e-02, -1.1438e-02,\n",
      "         -1.2293e-02],\n",
      "        [-5.9580e-02, -5.8039e-02, -5.7778e-02, -5.8628e-02, -6.0950e-02,\n",
      "         -5.4774e-02],\n",
      "        [-3.2796e-02, -3.7145e-02, -3.5799e-02, -3.1411e-02, -3.7585e-02,\n",
      "         -3.7020e-02],\n",
      "        [ 4.2856e-01,  4.3819e-01,  4.1971e-01,  3.9373e-01,  4.1236e-01,\n",
      "          4.2399e-01],\n",
      "        [ 1.1444e-01,  1.3760e-01,  1.2835e-01,  1.1600e-01,  1.9973e-01,\n",
      "          1.5782e-01]], dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([13, 6]), val: tensor([[ 2.8528e-02,  2.2772e-02,  5.1173e-02,  7.5317e-02,  4.3460e-02,\n",
      "         -3.1287e-04],\n",
      "        [ 2.5524e-01,  2.6834e-01,  2.6978e-01,  2.6658e-01,  2.9136e-01,\n",
      "          2.3691e-01],\n",
      "        [-3.0547e-02, -2.9284e-02, -3.2922e-02, -2.9329e-02, -2.5277e-02,\n",
      "         -2.0134e-02],\n",
      "        [ 1.4884e-01,  1.7111e-01,  1.4696e-01,  1.5179e-01,  2.4561e-01,\n",
      "          1.5465e-01],\n",
      "        [ 6.7782e-01,  6.6272e-01,  6.6207e-01,  6.9004e-01,  6.5881e-01,\n",
      "          6.6455e-01],\n",
      "        [-3.5112e-02, -3.3598e-02, -3.4191e-02, -3.7526e-02, -3.0535e-02,\n",
      "         -3.4101e-02],\n",
      "        [-1.0018e-02, -1.0177e-02, -1.2892e-02, -1.4211e-02, -1.7442e-02,\n",
      "         -9.3859e-03],\n",
      "        [-1.0359e-01, -1.0362e-01, -1.0250e-01, -1.0283e-01, -1.0088e-01,\n",
      "         -1.0607e-01],\n",
      "        [-1.4417e-02, -1.2538e-02, -1.3909e-02, -1.5524e-02, -1.1438e-02,\n",
      "         -1.2293e-02],\n",
      "        [-5.9580e-02, -5.8039e-02, -5.7778e-02, -5.8628e-02, -6.0950e-02,\n",
      "         -5.4774e-02],\n",
      "        [-3.2796e-02, -3.7145e-02, -3.5799e-02, -3.1411e-02, -3.7585e-02,\n",
      "         -3.7020e-02],\n",
      "        [ 4.2856e-01,  4.3819e-01,  4.1971e-01,  3.9373e-01,  4.1236e-01,\n",
      "          4.2399e-01],\n",
      "        [ 1.1444e-01,  1.3760e-01,  1.2835e-01,  1.1600e-01,  1.9973e-01,\n",
      "          1.5782e-01]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([13, 6]), val: tensor([[ 2.8528e-02,  2.2772e-02,  5.1173e-02,  7.5317e-02,  4.3460e-02,\n",
      "         -3.1287e-04],\n",
      "        [ 2.5524e-01,  2.6834e-01,  2.6978e-01,  2.6658e-01,  2.9136e-01,\n",
      "          2.3691e-01],\n",
      "        [-3.0547e-02, -2.9284e-02, -3.2922e-02, -2.9329e-02, -2.5277e-02,\n",
      "         -2.0134e-02],\n",
      "        [ 1.4884e-01,  1.7111e-01,  1.4696e-01,  1.5179e-01,  2.4561e-01,\n",
      "          1.5465e-01],\n",
      "        [ 6.7782e-01,  6.6272e-01,  6.6207e-01,  6.9004e-01,  6.5881e-01,\n",
      "          6.6455e-01],\n",
      "        [-3.5112e-02, -3.3598e-02, -3.4191e-02, -3.7526e-02, -3.0535e-02,\n",
      "         -3.4101e-02],\n",
      "        [-1.0018e-02, -1.0177e-02, -1.2892e-02, -1.4211e-02, -1.7442e-02,\n",
      "         -9.3859e-03],\n",
      "        [-1.0359e-01, -1.0362e-01, -1.0250e-01, -1.0283e-01, -1.0088e-01,\n",
      "         -1.0607e-01],\n",
      "        [-1.4417e-02, -1.2538e-02, -1.3909e-02, -1.5524e-02, -1.1438e-02,\n",
      "         -1.2293e-02],\n",
      "        [-5.9580e-02, -5.8039e-02, -5.7778e-02, -5.8628e-02, -6.0950e-02,\n",
      "         -5.4774e-02],\n",
      "        [-3.2796e-02, -3.7145e-02, -3.5799e-02, -3.1411e-02, -3.7585e-02,\n",
      "         -3.7020e-02],\n",
      "        [ 4.2856e-01,  4.3819e-01,  4.1971e-01,  3.9373e-01,  4.1236e-01,\n",
      "          4.2399e-01],\n",
      "        [ 1.1444e-01,  1.3760e-01,  1.2835e-01,  1.1600e-01,  1.9973e-01,\n",
      "          1.5782e-01]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([13, 1]), val: tensor([[-0.0708],\n",
      "        [-0.0107],\n",
      "        [-0.0757],\n",
      "        [-0.0335],\n",
      "        [ 0.0652],\n",
      "        [-0.0741],\n",
      "        [-0.0702],\n",
      "        [-0.0901],\n",
      "        [-0.0704],\n",
      "        [-0.0806],\n",
      "        [-0.0772],\n",
      "        [ 0.0326],\n",
      "        [-0.0338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 56784019677.94038, val: 56784019677.94038\n",
      "conv1 shape: torch.Size([13, 12]), val: tensor([[-227908.9216, -200962.5326, -214591.4731, -240043.1397, -237964.5173,\n",
      "         -174347.2367, -166858.4109, -159407.5037, -176784.9412, -182417.8384,\n",
      "         -126657.3560, -191274.4924],\n",
      "        [  91693.8932,   75262.6258,   87530.1845,   89740.4927,   36254.6082,\n",
      "          119606.1951,  126337.4257,  108725.5320,  156093.2726,   90861.3078,\n",
      "          160741.3190,   73944.5147],\n",
      "        [ 140614.2403,  152215.0282,  152459.8379,  147462.5846,  163099.9587,\n",
      "          177321.4733,  171229.2104,  207051.5144,  187706.5992,  229014.4825,\n",
      "          179802.7542,  211784.0592],\n",
      "        [  96331.2548,  131482.4151,  136318.0733,  149711.7025,  155921.2673,\n",
      "          138287.8514,  114741.9285,  166418.1365,  169015.6632,  162992.9922,\n",
      "          153975.8824,  126032.0325],\n",
      "        [ -38596.9715,  -19931.3438,  -28043.1302,  -13385.2683,  -25855.1960,\n",
      "            6336.0768,   36049.8641,    3676.1225,   32041.3629,   44430.9335,\n",
      "           46618.2038,   22630.9490],\n",
      "        [ 233137.5032,  215831.8356,  221066.5814,  231814.8054,  258737.0243,\n",
      "          227282.5569,  208048.7362,  218727.6639,  204785.7951,  211897.8376,\n",
      "          165892.3161,  201473.4173],\n",
      "        [-167198.7206, -133469.8791, -124553.7119, -144989.9815, -139971.1021,\n",
      "         -179624.5850, -137020.5428, -143970.5626, -167785.2560, -190255.8033,\n",
      "         -138426.2506, -159384.1543],\n",
      "        [-131060.6149, -157276.4803, -160421.6871, -164866.6823, -214855.1591,\n",
      "         -146232.4922, -179490.4807, -176922.6233, -193637.1146, -180077.5246,\n",
      "         -210028.2073, -184285.8827],\n",
      "        [ -84520.5387,  -93017.6025, -102402.4840, -115613.4336,  -88860.9242,\n",
      "         -124437.3128,  -92309.2396, -112861.8350, -112648.1230, -108174.0050,\n",
      "         -123434.1496,  -70858.5695],\n",
      "        [  79530.8759,  101129.5384,  101410.1924,  128510.3585,   99906.5626,\n",
      "           64499.8079,   76209.8661,   65343.1865,   78712.2675,   82584.2301,\n",
      "           59247.4106,   88142.9193],\n",
      "        [ -85014.6813,  -93060.7320,  -77605.1185,  -93434.8084, -127115.7333,\n",
      "          -55386.3730, -147179.8090, -106305.0076, -123735.5464, -151188.7228,\n",
      "         -127573.6605, -129693.9314],\n",
      "        [ 126333.2011,  154795.8806,  144128.6960,  164856.3450,  157701.3427,\n",
      "          127011.1474,  117464.6212,  115540.3460,   97726.4020,   99765.0319,\n",
      "           99909.7253,   78664.6222],\n",
      "        [ 210165.9223,  228099.1448,  240041.6464,  267670.5312,  244212.8469,\n",
      "          196621.8199,  231324.3783,  243074.8990,  232256.7158,  220538.1138,\n",
      "          212786.8153,  243232.1285]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "bn1 shape: torch.Size([13, 12]), val: tensor([[-1.7221, -1.5886, -1.6566, -1.6865, -1.5716, -1.4184, -1.3362, -1.2789,\n",
      "         -1.3266, -1.3089, -1.0641, -1.4385],\n",
      "        [ 0.5095,  0.3298,  0.3991,  0.3629,  0.0885,  0.6320,  0.6787,  0.5033,\n",
      "          0.8139,  0.4125,  0.9243,  0.3347],\n",
      "        [ 0.8510,  0.8642,  0.8409,  0.7216,  0.8565,  1.0346,  0.9872,  1.1568,\n",
      "          1.0172,  1.2827,  1.0562,  1.2564],\n",
      "        [ 0.5418,  0.7202,  0.7311,  0.7356,  0.8130,  0.7623,  0.5990,  0.8867,\n",
      "          0.8970,  0.8668,  0.8775,  0.6830],\n",
      "        [-0.4003, -0.3313, -0.3873, -0.2779, -0.2875, -0.1581,  0.0582, -0.1950,\n",
      "          0.0162,  0.1200,  0.1347, -0.0083],\n",
      "        [ 1.4971,  1.3060,  1.3077,  1.2458,  1.4355,  1.3831,  1.2402,  1.2344,\n",
      "          1.1270,  1.1749,  0.9599,  1.1874],\n",
      "        [-1.2982, -1.1198, -1.0439, -1.0958, -0.9784, -1.4552, -1.1312, -1.1763,\n",
      "         -1.2687, -1.3583, -1.1455, -1.2253],\n",
      "        [-1.0459, -1.2852, -1.2880, -1.2193, -1.4317, -1.2223, -1.4230, -1.3953,\n",
      "         -1.4350, -1.2942, -1.6409, -1.3918],\n",
      "        [-0.7209, -0.8389, -0.8932, -0.9132, -0.6689, -1.0702, -0.8239, -0.9695,\n",
      "         -0.9142, -0.8413, -1.0418, -0.6334],\n",
      "        [ 0.4245,  0.5094,  0.4936,  0.6039,  0.4739,  0.2477,  0.3342,  0.2149,\n",
      "          0.3163,  0.3603,  0.2221,  0.4297],\n",
      "        [-0.7244, -0.8392, -0.7245, -0.7754, -0.9005, -0.5886, -1.2010, -0.9260,\n",
      "         -0.9855, -1.1122, -1.0704, -1.0268],\n",
      "        [ 0.7513,  0.8821,  0.7842,  0.8297,  0.8238,  0.6837,  0.6177,  0.5486,\n",
      "          0.4386,  0.4685,  0.5034,  0.3663],\n",
      "        [ 1.3367,  1.3912,  1.4368,  1.4687,  1.3475,  1.1692,  1.4001,  1.3963,\n",
      "          1.3037,  1.2293,  1.2844,  1.4666]], dtype=torch.float64,\n",
      "       grad_fn=<NativeBatchNormBackward0>)\n",
      "relu shape: torch.Size([13, 12]), val: tensor([[-1.7221e-01, -1.5886e-01, -1.6566e-01, -1.6865e-01, -1.5716e-01,\n",
      "         -1.4184e-01, -1.3362e-01, -1.2789e-01, -1.3266e-01, -1.3089e-01,\n",
      "         -1.0641e-01, -1.4385e-01],\n",
      "        [ 5.0946e-01,  3.2978e-01,  3.9912e-01,  3.6292e-01,  8.8529e-02,\n",
      "          6.3204e-01,  6.7867e-01,  5.0328e-01,  8.1392e-01,  4.1245e-01,\n",
      "          9.2431e-01,  3.3475e-01],\n",
      "        [ 8.5104e-01,  8.6420e-01,  8.4092e-01,  7.2163e-01,  8.5646e-01,\n",
      "          1.0346e+00,  9.8717e-01,  1.1568e+00,  1.0172e+00,  1.2827e+00,\n",
      "          1.0562e+00,  1.2564e+00],\n",
      "        [ 5.4184e-01,  7.2022e-01,  7.3109e-01,  7.3561e-01,  8.1300e-01,\n",
      "          7.6235e-01,  5.9898e-01,  8.8674e-01,  8.9702e-01,  8.6681e-01,\n",
      "          8.7750e-01,  6.8301e-01],\n",
      "        [-4.0029e-02, -3.3132e-02, -3.8726e-02, -2.7795e-02, -2.8749e-02,\n",
      "         -1.5805e-02,  5.8202e-02, -1.9495e-02,  1.6220e-02,  1.1999e-01,\n",
      "          1.3474e-01, -8.3345e-04],\n",
      "        [ 1.4971e+00,  1.3060e+00,  1.3077e+00,  1.2458e+00,  1.4355e+00,\n",
      "          1.3831e+00,  1.2402e+00,  1.2344e+00,  1.1270e+00,  1.1749e+00,\n",
      "          9.5995e-01,  1.1874e+00],\n",
      "        [-1.2982e-01, -1.1198e-01, -1.0439e-01, -1.0958e-01, -9.7836e-02,\n",
      "         -1.4552e-01, -1.1312e-01, -1.1763e-01, -1.2687e-01, -1.3583e-01,\n",
      "         -1.1455e-01, -1.2253e-01],\n",
      "        [-1.0459e-01, -1.2852e-01, -1.2880e-01, -1.2193e-01, -1.4317e-01,\n",
      "         -1.2223e-01, -1.4230e-01, -1.3953e-01, -1.4350e-01, -1.2942e-01,\n",
      "         -1.6409e-01, -1.3918e-01],\n",
      "        [-7.2095e-02, -8.3890e-02, -8.9322e-02, -9.1324e-02, -6.6893e-02,\n",
      "         -1.0702e-01, -8.2389e-02, -9.6954e-02, -9.1419e-02, -8.4127e-02,\n",
      "         -1.0418e-01, -6.3341e-02],\n",
      "        [ 4.2453e-01,  5.0942e-01,  4.9357e-01,  6.0385e-01,  4.7389e-01,\n",
      "          2.4765e-01,  3.3418e-01,  2.1493e-01,  3.1633e-01,  3.6032e-01,\n",
      "          2.2212e-01,  4.2968e-01],\n",
      "        [-7.2440e-02, -8.3919e-02, -7.2450e-02, -7.7541e-02, -9.0053e-02,\n",
      "         -5.8859e-02, -1.2010e-01, -9.2596e-02, -9.8549e-02, -1.1122e-01,\n",
      "         -1.0704e-01, -1.0268e-01],\n",
      "        [ 7.5132e-01,  8.8213e-01,  7.8423e-01,  8.2972e-01,  8.2378e-01,\n",
      "          6.8369e-01,  6.1769e-01,  5.4857e-01,  4.3860e-01,  4.6854e-01,\n",
      "          5.0344e-01,  3.6631e-01],\n",
      "        [ 1.3367e+00,  1.3912e+00,  1.4368e+00,  1.4687e+00,  1.3475e+00,\n",
      "          1.1692e+00,  1.4001e+00,  1.3963e+00,  1.3037e+00,  1.2293e+00,\n",
      "          1.2844e+00,  1.4666e+00]], dtype=torch.float64,\n",
      "       grad_fn=<LeakyReluBackward1>)\n",
      "pool shape: torch.Size([13, 6]), val: tensor([[-0.1655, -0.1672, -0.1495, -0.1308, -0.1318, -0.1251],\n",
      "        [ 0.4196,  0.3810,  0.3603,  0.5910,  0.6132,  0.6295],\n",
      "        [ 0.8576,  0.7813,  0.9455,  1.0720,  1.1499,  1.1563],\n",
      "        [ 0.6310,  0.7333,  0.7877,  0.7429,  0.8819,  0.7803],\n",
      "        [-0.0366, -0.0333, -0.0223,  0.0194,  0.0681,  0.0670],\n",
      "        [ 1.4015,  1.2768,  1.4093,  1.2373,  1.1509,  1.0737],\n",
      "        [-0.1209, -0.1070, -0.1217, -0.1154, -0.1314, -0.1185],\n",
      "        [-0.1166, -0.1254, -0.1327, -0.1409, -0.1365, -0.1516],\n",
      "        [-0.0780, -0.0903, -0.0870, -0.0897, -0.0878, -0.0838],\n",
      "        [ 0.4670,  0.5487,  0.3608,  0.2746,  0.3383,  0.3259],\n",
      "        [-0.0782, -0.0750, -0.0745, -0.1063, -0.1049, -0.1049],\n",
      "        [ 0.8167,  0.8070,  0.7537,  0.5831,  0.4536,  0.4349],\n",
      "        [ 1.3639,  1.4527,  1.2584,  1.3982,  1.2665,  1.3755]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([13, 6]), val: tensor([[ 0.0590,  0.0711,  0.0709,  0.0925,  0.0541,  0.0636],\n",
      "        [ 0.1098,  0.0779,  0.1767,  0.2159,  0.2407,  0.2307],\n",
      "        [-0.4630, -0.4645, -0.4640, -0.3803, -0.3275, -0.3230],\n",
      "        [ 0.0525,  0.0155,  0.0278,  0.0762,  0.0882,  0.1111],\n",
      "        [ 0.5391,  0.5304,  0.5765,  0.6361,  0.6615,  0.6433],\n",
      "        [-0.3913, -0.4234, -0.4029, -0.3930, -0.4023, -0.3825],\n",
      "        [-0.0214, -0.0290,  0.0293, -0.0690, -0.0263, -0.0949],\n",
      "        [-0.9980, -1.0030, -0.9792, -1.0171, -1.0330, -1.0173],\n",
      "        [-0.1325, -0.1583, -0.0885, -0.1349, -0.1140, -0.1430],\n",
      "        [-0.4847, -0.4463, -0.4748, -0.5184, -0.5189, -0.5259],\n",
      "        [-0.2185, -0.1999, -0.2914, -0.2941, -0.3364, -0.3270],\n",
      "        [ 0.2793,  0.2588,  0.3349,  0.3493,  0.4087,  0.3810],\n",
      "        [ 0.1139,  0.0899,  0.0820,  0.0948,  0.0905,  0.1108]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([13, 6]), val: tensor([[ 0.0590,  0.0711,  0.0709,  0.0925,  0.0541,  0.0636],\n",
      "        [ 0.1098,  0.0779,  0.1767,  0.2159,  0.2407,  0.2307],\n",
      "        [-0.0463, -0.0464, -0.0464, -0.0380, -0.0327, -0.0323],\n",
      "        [ 0.0525,  0.0155,  0.0278,  0.0762,  0.0882,  0.1111],\n",
      "        [ 0.5391,  0.5304,  0.5765,  0.6361,  0.6615,  0.6433],\n",
      "        [-0.0391, -0.0423, -0.0403, -0.0393, -0.0402, -0.0383],\n",
      "        [-0.0021, -0.0029,  0.0293, -0.0069, -0.0026, -0.0095],\n",
      "        [-0.0998, -0.1003, -0.0979, -0.1017, -0.1033, -0.1017],\n",
      "        [-0.0133, -0.0158, -0.0089, -0.0135, -0.0114, -0.0143],\n",
      "        [-0.0485, -0.0446, -0.0475, -0.0518, -0.0519, -0.0526],\n",
      "        [-0.0219, -0.0200, -0.0291, -0.0294, -0.0336, -0.0327],\n",
      "        [ 0.2793,  0.2588,  0.3349,  0.3493,  0.4087,  0.3810],\n",
      "        [ 0.1139,  0.0899,  0.0820,  0.0948,  0.0905,  0.1108]],\n",
      "       dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([13, 6]), val: tensor([[ 0.0590,  0.0711,  0.0709,  0.0925,  0.0541,  0.0636],\n",
      "        [ 0.1098,  0.0779,  0.1767,  0.2159,  0.2407,  0.2307],\n",
      "        [-0.0463, -0.0464, -0.0464, -0.0380, -0.0327, -0.0323],\n",
      "        [ 0.0525,  0.0155,  0.0278,  0.0762,  0.0882,  0.1111],\n",
      "        [ 0.5391,  0.5304,  0.5765,  0.6361,  0.6615,  0.6433],\n",
      "        [-0.0391, -0.0423, -0.0403, -0.0393, -0.0402, -0.0383],\n",
      "        [-0.0021, -0.0029,  0.0293, -0.0069, -0.0026, -0.0095],\n",
      "        [-0.0998, -0.1003, -0.0979, -0.1017, -0.1033, -0.1017],\n",
      "        [-0.0133, -0.0158, -0.0089, -0.0135, -0.0114, -0.0143],\n",
      "        [-0.0485, -0.0446, -0.0475, -0.0518, -0.0519, -0.0526],\n",
      "        [-0.0219, -0.0200, -0.0291, -0.0294, -0.0336, -0.0327],\n",
      "        [ 0.2793,  0.2588,  0.3349,  0.3493,  0.4087,  0.3810],\n",
      "        [ 0.1139,  0.0899,  0.0820,  0.0948,  0.0905,  0.1108]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([13, 6]), val: tensor([[ 0.0590,  0.0711,  0.0709,  0.0925,  0.0541,  0.0636],\n",
      "        [ 0.1098,  0.0779,  0.1767,  0.2159,  0.2407,  0.2307],\n",
      "        [-0.0463, -0.0464, -0.0464, -0.0380, -0.0327, -0.0323],\n",
      "        [ 0.0525,  0.0155,  0.0278,  0.0762,  0.0882,  0.1111],\n",
      "        [ 0.5391,  0.5304,  0.5765,  0.6361,  0.6615,  0.6433],\n",
      "        [-0.0391, -0.0423, -0.0403, -0.0393, -0.0402, -0.0383],\n",
      "        [-0.0021, -0.0029,  0.0293, -0.0069, -0.0026, -0.0095],\n",
      "        [-0.0998, -0.1003, -0.0979, -0.1017, -0.1033, -0.1017],\n",
      "        [-0.0133, -0.0158, -0.0089, -0.0135, -0.0114, -0.0143],\n",
      "        [-0.0485, -0.0446, -0.0475, -0.0518, -0.0519, -0.0526],\n",
      "        [-0.0219, -0.0200, -0.0291, -0.0294, -0.0336, -0.0327],\n",
      "        [ 0.2793,  0.2588,  0.3349,  0.3493,  0.4087,  0.3810],\n",
      "        [ 0.1139,  0.0899,  0.0820,  0.0948,  0.0905,  0.1108]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([13, 1]), val: tensor([[-0.0642],\n",
      "        [-0.0486],\n",
      "        [-0.0803],\n",
      "        [-0.0722],\n",
      "        [ 0.0336],\n",
      "        [-0.0769],\n",
      "        [-0.0541],\n",
      "        [-0.0883],\n",
      "        [-0.0690],\n",
      "        [-0.0772],\n",
      "        [-0.0737],\n",
      "        [-0.0035],\n",
      "        [-0.0499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 54252639518.60615, val: 54252639518.60615\n",
      "conv1 shape: torch.Size([13, 12]), val: tensor([[-241191.9980, -231841.4213, -247731.4709, -253717.0391, -239873.0537,\n",
      "         -222243.8598, -217168.0474, -212869.0107, -199681.5572, -147678.5078,\n",
      "         -143372.6860, -162257.9798],\n",
      "        [  72535.1628,   79779.2044,   74846.9715,   80862.9179,   84052.1016,\n",
      "          122609.3779,  105841.7215,  134113.4963,  180619.6656,  105811.4268,\n",
      "          114035.4601,  124831.4851],\n",
      "        [ 270864.4135,  282087.4848,  283751.2700,  292864.0052,  307935.1217,\n",
      "          306325.0667,  292968.1727,  315153.9892,  250996.8090,  229331.3464,\n",
      "          223850.4921,  254552.4914],\n",
      "        [ 183384.8953,  184822.7939,  185507.3776,  198352.1524,  193115.5664,\n",
      "          181237.8719,  156698.1694,  165913.7931,  148941.0540,  188496.0652,\n",
      "          134318.0845,  114624.2399],\n",
      "        [ -31820.5584,  -35912.5949,  -33805.9609,  -45128.8556,  -56863.7941,\n",
      "          -41667.4383,  -12461.3296,   15122.7502,   37768.0562,   39422.3638,\n",
      "           68883.3075,   61624.0722],\n",
      "        [ 325038.0049,  333516.7940,  339633.2424,  340526.4199,  345905.1954,\n",
      "          338485.0979,  312839.8458,  315700.4747,  293582.6099,  249963.7393,\n",
      "          233954.9177,  226547.8526],\n",
      "        [-290570.0553, -308236.1941, -293011.0464, -300284.8782, -309065.4188,\n",
      "         -290024.1754, -316693.3446, -304918.0929, -260018.4921, -209015.9301,\n",
      "         -174814.5607, -205409.4278],\n",
      "        [-160147.6302, -168700.4800, -182534.1548, -165275.3839, -175319.7974,\n",
      "         -206167.7958, -186633.1123, -233204.8062, -261698.9782, -243698.7321,\n",
      "         -288627.5731, -241137.4579],\n",
      "        [ -79585.9188,  -95809.9545,  -81816.7431,  -79351.6049,  -71542.3539,\n",
      "          -60458.0165,  -80875.2322, -131616.8514, -120429.0982, -165599.7366,\n",
      "         -104859.6758,  -95413.6986],\n",
      "        [  17437.5982,   25445.9193,   17000.9163,   13614.0470,   10219.9342,\n",
      "            8123.7757,   34995.9524,   26695.2773,   66869.5629,   97933.7405,\n",
      "           40168.6116,   71810.2787],\n",
      "        [-108980.4327, -107528.0659, -125352.0524, -107683.8993, -118243.8991,\n",
      "         -108142.0471, -134348.4818, -171357.5463, -213029.1790, -175826.3758,\n",
      "         -172709.1474, -156351.4975],\n",
      "        [  67067.0964,   62309.9467,   65767.9165,   75781.8120,   64654.2175,\n",
      "           60611.7812,   51140.0442,   84654.1648,   65698.1942,  118545.7821,\n",
      "          102134.4364,   80675.9161],\n",
      "        [ 232056.5680,  240373.1606,  242045.4371,  238536.4742,  229620.3591,\n",
      "          233984.8433,  278124.8323,  310060.8057,  294340.7514,  290031.6606,\n",
      "          233199.2280,  254892.7214]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "bn1 shape: torch.Size([13, 12]), val: tensor([[-1.3884, -1.3004, -1.3609, -1.3990, -1.3054, -1.2600, -1.2171, -1.1214,\n",
      "         -1.0966, -0.9570, -0.9623, -1.1012],\n",
      "        [ 0.2812,  0.3085,  0.2862,  0.2972,  0.3195,  0.4987,  0.4275,  0.5205,\n",
      "          0.7860,  0.4156,  0.5495,  0.5843],\n",
      "        [ 1.3367,  1.3530,  1.3529,  1.3720,  1.4426,  1.4356,  1.3803,  1.3772,\n",
      "          1.1344,  1.0845,  1.1945,  1.3459],\n",
      "        [ 0.8711,  0.8509,  0.8513,  0.8928,  0.8667,  0.7977,  0.6864,  0.6710,\n",
      "          0.6292,  0.8634,  0.6686,  0.5244],\n",
      "        [-0.2742, -0.2888, -0.2686, -0.3415, -0.3874, -0.3391, -0.1748, -0.0425,\n",
      "          0.0788,  0.0561,  0.2843,  0.2132],\n",
      "        [ 1.6250,  1.6186,  1.6383,  1.6136,  1.6331,  1.5996,  1.4815,  1.3798,\n",
      "          1.3452,  1.1962,  1.2538,  1.1815],\n",
      "        [-1.6512, -1.6948, -1.5921, -1.6351, -1.6525, -1.6056, -1.7239, -1.5570,\n",
      "         -1.3953, -1.2891, -1.1470, -1.3545],\n",
      "        [-0.9571, -0.9744, -1.0280, -0.9506, -0.9816, -1.1780, -1.0617, -1.2176,\n",
      "         -1.4036, -1.4769, -1.8154, -1.5643],\n",
      "        [-0.5284, -0.5981, -0.5137, -0.5150, -0.4610, -0.4349, -0.5232, -0.7369,\n",
      "         -0.7043, -1.0540, -0.7361, -0.7087],\n",
      "        [-0.0120,  0.0280, -0.0091, -0.0437, -0.0508, -0.0852,  0.0668,  0.0122,\n",
      "          0.2229,  0.3730,  0.1157,  0.2730],\n",
      "        [-0.6848, -0.6586, -0.7360, -0.6587, -0.6953, -0.6781, -0.7954, -0.9250,\n",
      "         -1.1627, -1.1094, -1.1346, -1.0665],\n",
      "        [ 0.2521,  0.2183,  0.2399,  0.2715,  0.2222,  0.1825,  0.1490,  0.2865,\n",
      "          0.2171,  0.4846,  0.4796,  0.3251],\n",
      "        [ 1.1302,  1.1377,  1.1400,  1.0966,  1.0498,  1.0667,  1.3047,  1.3531,\n",
      "          1.3489,  1.4132,  1.2494,  1.3479]], dtype=torch.float64,\n",
      "       grad_fn=<NativeBatchNormBackward0>)\n",
      "relu shape: torch.Size([13, 12]), val: tensor([[-1.3884e-01, -1.3004e-01, -1.3609e-01, -1.3990e-01, -1.3054e-01,\n",
      "         -1.2600e-01, -1.2171e-01, -1.1214e-01, -1.0966e-01, -9.5699e-02,\n",
      "         -9.6232e-02, -1.1012e-01],\n",
      "        [ 2.8119e-01,  3.0852e-01,  2.8623e-01,  2.9721e-01,  3.1954e-01,\n",
      "          4.9869e-01,  4.2750e-01,  5.2054e-01,  7.8599e-01,  4.1563e-01,\n",
      "          5.4952e-01,  5.8430e-01],\n",
      "        [ 1.3367e+00,  1.3530e+00,  1.3529e+00,  1.3720e+00,  1.4426e+00,\n",
      "          1.4356e+00,  1.3803e+00,  1.3772e+00,  1.1344e+00,  1.0845e+00,\n",
      "          1.1945e+00,  1.3459e+00],\n",
      "        [ 8.7113e-01,  8.5086e-01,  8.5129e-01,  8.9285e-01,  8.6666e-01,\n",
      "          7.9768e-01,  6.8644e-01,  6.7102e-01,  6.2917e-01,  8.6336e-01,\n",
      "          6.6864e-01,  5.2437e-01],\n",
      "        [-2.7418e-02, -2.8880e-02, -2.6858e-02, -3.4153e-02, -3.8736e-02,\n",
      "         -3.3907e-02, -1.7485e-02, -4.2535e-03,  7.8833e-02,  5.6137e-02,\n",
      "          2.8432e-01,  2.1321e-01],\n",
      "        [ 1.6250e+00,  1.6186e+00,  1.6383e+00,  1.6136e+00,  1.6331e+00,\n",
      "          1.5996e+00,  1.4815e+00,  1.3798e+00,  1.3452e+00,  1.1962e+00,\n",
      "          1.2538e+00,  1.1815e+00],\n",
      "        [-1.6512e-01, -1.6948e-01, -1.5921e-01, -1.6351e-01, -1.6525e-01,\n",
      "         -1.6056e-01, -1.7239e-01, -1.5570e-01, -1.3953e-01, -1.2891e-01,\n",
      "         -1.1470e-01, -1.3545e-01],\n",
      "        [-9.5713e-02, -9.7439e-02, -1.0280e-01, -9.5064e-02, -9.8159e-02,\n",
      "         -1.1780e-01, -1.0617e-01, -1.2176e-01, -1.4036e-01, -1.4769e-01,\n",
      "         -1.8154e-01, -1.5643e-01],\n",
      "        [-5.2839e-02, -5.9805e-02, -5.1374e-02, -5.1503e-02, -4.6099e-02,\n",
      "         -4.3490e-02, -5.2318e-02, -7.3692e-02, -7.0429e-02, -1.0540e-01,\n",
      "         -7.3612e-02, -7.0874e-02],\n",
      "        [-1.2035e-03,  2.7996e-02, -9.1480e-04, -4.3722e-03, -5.0834e-03,\n",
      "         -8.5152e-03,  6.6785e-02,  1.2227e-02,  2.2289e-01,  3.7297e-01,\n",
      "          1.1567e-01,  2.7302e-01],\n",
      "        [-6.8482e-02, -6.5855e-02, -7.3604e-02, -6.5867e-02, -6.9527e-02,\n",
      "         -6.7807e-02, -7.9545e-02, -9.2497e-02, -1.1627e-01, -1.1094e-01,\n",
      "         -1.1346e-01, -1.0665e-01],\n",
      "        [ 2.5209e-01,  2.1833e-01,  2.3987e-01,  2.7145e-01,  2.2223e-01,\n",
      "          1.8252e-01,  1.4898e-01,  2.8649e-01,  2.1710e-01,  4.8458e-01,\n",
      "          4.7962e-01,  3.2507e-01],\n",
      "        [ 1.1302e+00,  1.1377e+00,  1.1400e+00,  1.0966e+00,  1.0498e+00,\n",
      "          1.0667e+00,  1.3047e+00,  1.3531e+00,  1.3489e+00,  1.4132e+00,\n",
      "          1.2494e+00,  1.3479e+00]], dtype=torch.float64,\n",
      "       grad_fn=<LeakyReluBackward1>)\n",
      "pool shape: torch.Size([13, 6]), val: tensor([[-0.1344, -0.1380, -0.1283, -0.1169, -0.1027, -0.1032],\n",
      "        [ 0.2949,  0.2917,  0.4091,  0.4740,  0.6008,  0.5669],\n",
      "        [ 1.3449,  1.3625,  1.4391,  1.3788,  1.1094,  1.2702],\n",
      "        [ 0.8610,  0.8721,  0.8322,  0.6787,  0.7463,  0.5965],\n",
      "        [-0.0281, -0.0305, -0.0363, -0.0109,  0.0675,  0.2488],\n",
      "        [ 1.6218,  1.6260,  1.6164,  1.4306,  1.2707,  1.2177],\n",
      "        [-0.1673, -0.1614, -0.1629, -0.1640, -0.1342, -0.1251],\n",
      "        [-0.0966, -0.0989, -0.1080, -0.1140, -0.1440, -0.1690],\n",
      "        [-0.0563, -0.0514, -0.0448, -0.0630, -0.0879, -0.0722],\n",
      "        [ 0.0134, -0.0026, -0.0068,  0.0395,  0.2979,  0.1943],\n",
      "        [-0.0672, -0.0697, -0.0687, -0.0860, -0.1136, -0.1101],\n",
      "        [ 0.2352,  0.2557,  0.2024,  0.2177,  0.3508,  0.4023],\n",
      "        [ 1.1339,  1.1183,  1.0582,  1.3289,  1.3810,  1.2986]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([13, 6]), val: tensor([[ 0.1393,  0.1353,  0.1206,  0.1371,  0.1118,  0.0289],\n",
      "        [ 0.4640,  0.4679,  0.4775,  0.4214,  0.2745,  0.3060],\n",
      "        [-0.4295, -0.4344, -0.3964, -0.3810, -0.3287, -0.2959],\n",
      "        [ 0.1675,  0.1613,  0.2011,  0.2116,  0.1441,  0.1751],\n",
      "        [ 0.6766,  0.6795,  0.7012,  0.6712,  0.6618,  0.6478],\n",
      "        [-0.3741, -0.3714, -0.3630, -0.3406, -0.3984, -0.3040],\n",
      "        [ 0.0280,  0.0351,  0.0232, -0.1053, -0.0595, -0.0790],\n",
      "        [-0.9254, -0.9226, -0.9357, -0.9602, -1.0285, -1.0020],\n",
      "        [ 0.0652,  0.0642,  0.0587, -0.0128, -0.0874, -0.0695],\n",
      "        [-0.5073, -0.5120, -0.5457, -0.5507, -0.5039, -0.5614],\n",
      "        [-0.5794, -0.5810, -0.5772, -0.5160, -0.3752, -0.4432],\n",
      "        [ 0.5110,  0.5141,  0.5295,  0.4646,  0.4026,  0.4449],\n",
      "        [ 0.1501,  0.1450,  0.1546,  0.1833,  0.1171,  0.1662]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([13, 6]), val: tensor([[ 0.1393,  0.1353,  0.1206,  0.1371,  0.1118,  0.0289],\n",
      "        [ 0.4640,  0.4679,  0.4775,  0.4214,  0.2745,  0.3060],\n",
      "        [-0.0430, -0.0434, -0.0396, -0.0381, -0.0329, -0.0296],\n",
      "        [ 0.1675,  0.1613,  0.2011,  0.2116,  0.1441,  0.1751],\n",
      "        [ 0.6766,  0.6795,  0.7012,  0.6712,  0.6618,  0.6478],\n",
      "        [-0.0374, -0.0371, -0.0363, -0.0341, -0.0398, -0.0304],\n",
      "        [ 0.0280,  0.0351,  0.0232, -0.0105, -0.0059, -0.0079],\n",
      "        [-0.0925, -0.0923, -0.0936, -0.0960, -0.1029, -0.1002],\n",
      "        [ 0.0652,  0.0642,  0.0587, -0.0013, -0.0087, -0.0069],\n",
      "        [-0.0507, -0.0512, -0.0546, -0.0551, -0.0504, -0.0561],\n",
      "        [-0.0579, -0.0581, -0.0577, -0.0516, -0.0375, -0.0443],\n",
      "        [ 0.5110,  0.5141,  0.5295,  0.4646,  0.4026,  0.4449],\n",
      "        [ 0.1501,  0.1450,  0.1546,  0.1833,  0.1171,  0.1662]],\n",
      "       dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([13, 6]), val: tensor([[ 0.1393,  0.1353,  0.1206,  0.1371,  0.1118,  0.0289],\n",
      "        [ 0.4640,  0.4679,  0.4775,  0.4214,  0.2745,  0.3060],\n",
      "        [-0.0430, -0.0434, -0.0396, -0.0381, -0.0329, -0.0296],\n",
      "        [ 0.1675,  0.1613,  0.2011,  0.2116,  0.1441,  0.1751],\n",
      "        [ 0.6766,  0.6795,  0.7012,  0.6712,  0.6618,  0.6478],\n",
      "        [-0.0374, -0.0371, -0.0363, -0.0341, -0.0398, -0.0304],\n",
      "        [ 0.0280,  0.0351,  0.0232, -0.0105, -0.0059, -0.0079],\n",
      "        [-0.0925, -0.0923, -0.0936, -0.0960, -0.1029, -0.1002],\n",
      "        [ 0.0652,  0.0642,  0.0587, -0.0013, -0.0087, -0.0069],\n",
      "        [-0.0507, -0.0512, -0.0546, -0.0551, -0.0504, -0.0561],\n",
      "        [-0.0579, -0.0581, -0.0577, -0.0516, -0.0375, -0.0443],\n",
      "        [ 0.5110,  0.5141,  0.5295,  0.4646,  0.4026,  0.4449],\n",
      "        [ 0.1501,  0.1450,  0.1546,  0.1833,  0.1171,  0.1662]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([13, 6]), val: tensor([[ 0.1393,  0.1353,  0.1206,  0.1371,  0.1118,  0.0289],\n",
      "        [ 0.4640,  0.4679,  0.4775,  0.4214,  0.2745,  0.3060],\n",
      "        [-0.0430, -0.0434, -0.0396, -0.0381, -0.0329, -0.0296],\n",
      "        [ 0.1675,  0.1613,  0.2011,  0.2116,  0.1441,  0.1751],\n",
      "        [ 0.6766,  0.6795,  0.7012,  0.6712,  0.6618,  0.6478],\n",
      "        [-0.0374, -0.0371, -0.0363, -0.0341, -0.0398, -0.0304],\n",
      "        [ 0.0280,  0.0351,  0.0232, -0.0105, -0.0059, -0.0079],\n",
      "        [-0.0925, -0.0923, -0.0936, -0.0960, -0.1029, -0.1002],\n",
      "        [ 0.0652,  0.0642,  0.0587, -0.0013, -0.0087, -0.0069],\n",
      "        [-0.0507, -0.0512, -0.0546, -0.0551, -0.0504, -0.0561],\n",
      "        [-0.0579, -0.0581, -0.0577, -0.0516, -0.0375, -0.0443],\n",
      "        [ 0.5110,  0.5141,  0.5295,  0.4646,  0.4026,  0.4449],\n",
      "        [ 0.1501,  0.1450,  0.1546,  0.1833,  0.1171,  0.1662]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([13, 1]), val: tensor([[-0.0486],\n",
      "        [ 0.0402],\n",
      "        [-0.0771],\n",
      "        [-0.0368],\n",
      "        [ 0.0877],\n",
      "        [-0.0770],\n",
      "        [-0.0517],\n",
      "        [-0.0878],\n",
      "        [-0.0361],\n",
      "        [-0.0790],\n",
      "        [-0.0816],\n",
      "        [ 0.0600],\n",
      "        [-0.0484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 58797630462.83333, val: 58797630462.83333\n",
      "conv1 shape: torch.Size([13, 12]), val: tensor([[-221269.5729, -219712.3190, -223953.2792, -230123.7165, -239608.4428,\n",
      "         -249103.8924, -240973.6124, -249625.6332, -197862.2283, -207195.8650,\n",
      "         -199419.4449, -186949.3178],\n",
      "        [  63610.0188,   54896.9485,   55942.0586,   48193.8594,   61443.9843,\n",
      "           54732.6082,   34280.9632,   30233.4931,   41558.0649,   88474.3963,\n",
      "           20176.9142,   52458.8197],\n",
      "        [ 291031.1768,  287957.5188,  287257.4683,  303342.8059,  312344.0582,\n",
      "          301942.7240,  304038.1307,  322066.7414,  292054.9953,  219837.1966,\n",
      "          247260.4803,  212046.8149],\n",
      "        [ 184927.4491,  173794.6926,  176039.7803,  179623.5815,  195998.6329,\n",
      "          194413.9124,  192713.8544,  185326.8011,  159343.2967,  127597.2828,\n",
      "          137215.7353,  145082.4571],\n",
      "        [ -37448.9921,  -37686.2471,  -36406.9352,  -40638.1935,  -35326.6564,\n",
      "          -45022.7650,  -52604.5950,  -35359.6390,   11109.7641,   17318.4786,\n",
      "           79235.3293,   69168.6353],\n",
      "        [ 307616.6780,  302258.1557,  308836.1188,  318171.0292,  313040.8968,\n",
      "          323340.5099,  324029.3133,  338436.4558,  306252.3066,  284756.9884,\n",
      "          249686.6702,  261959.4105],\n",
      "        [-240048.7408, -237117.6121, -244163.8858, -251659.9926, -244971.3575,\n",
      "         -244665.4281, -262547.4502, -297682.4951, -282137.7998, -257857.2459,\n",
      "         -265787.0278, -210790.0002],\n",
      "        [-151824.0214, -153392.9397, -161293.9156, -162884.7977, -161447.0121,\n",
      "         -182994.1009, -196960.9192, -214262.0201, -230859.0832, -190553.1719,\n",
      "         -214496.7717, -253047.3318],\n",
      "        [ -60556.5466,  -56478.8519,  -66827.8096,  -77525.8460,  -80666.7957,\n",
      "          -78251.2876,  -79380.7440, -106625.4289, -156006.7331,  -97002.1079,\n",
      "         -102356.0065,  -83286.8493],\n",
      "        [ -33011.1348,  -29415.5103,  -26816.9596,  -31953.2117,  -31012.6416,\n",
      "          -35724.2841,  -29564.4765,   -3061.5568,  -12563.1752,   14735.0589,\n",
      "           68093.9187,   24780.8167],\n",
      "        [ -98221.2642,  -99700.2316, -104626.1264, -109794.8279, -107810.8711,\n",
      "         -118809.9833, -122654.8418, -147327.7791, -133529.3645, -138480.2867,\n",
      "         -129111.3273, -162057.3507],\n",
      "        [  82215.1159,   83503.8212,   92418.4828,   87898.6943,   96882.4836,\n",
      "           82410.6788,   81175.7921,   86033.5024,   79735.1969,   94598.4756,\n",
      "          133165.6164,  148587.1202],\n",
      "        [ 124462.3862,  134735.9869,  136280.5756,  143546.4412,  134534.8295,\n",
      "          138649.7357,  150373.6055,  171144.1685,  209786.7392,  223746.6348,\n",
      "          241829.4986,  216578.1906]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "bn1 shape: torch.Size([13, 12]), val: tensor([[-1.3860e+00, -1.3898e+00, -1.3794e+00, -1.3600e+00, -1.4151e+00,\n",
      "         -1.4130e+00, -1.3301e+00, -1.2775e+00, -1.0752e+00, -1.2846e+00,\n",
      "         -1.2410e+00, -1.1925e+00],\n",
      "        [ 2.7624e-01,  2.3165e-01,  2.3755e-01,  1.9333e-01,  2.4887e-01,\n",
      "          2.3860e-01,  1.4134e-01,  1.2056e-01,  1.8332e-01,  4.3373e-01,\n",
      "         -1.3867e-03,  2.0022e-01],\n",
      "        [ 1.6032e+00,  1.6078e+00,  1.5739e+00,  1.6174e+00,  1.6356e+00,\n",
      "          1.5824e+00,  1.5834e+00,  1.5785e+00,  1.5001e+00,  1.1972e+00,\n",
      "          1.2804e+00,  1.1286e+00],\n",
      "        [ 9.8413e-01,  9.3369e-01,  9.3137e-01,  9.2687e-01,  9.9257e-01,\n",
      "          9.9790e-01,  9.8827e-01,  8.9536e-01,  8.0246e-01,  6.6111e-01,\n",
      "          6.5927e-01,  7.3904e-01],\n",
      "        [-3.1344e-01, -3.1502e-01, -2.9595e-01, -3.0245e-01, -2.8598e-01,\n",
      "         -3.0367e-01, -3.2312e-01, -2.0712e-01,  2.3268e-02,  2.0191e-02,\n",
      "          3.3198e-01,  2.9742e-01],\n",
      "        [ 1.7000e+00,  1.6922e+00,  1.6985e+00,  1.7001e+00,  1.6395e+00,\n",
      "          1.6987e+00,  1.6902e+00,  1.6602e+00,  1.5747e+00,  1.5745e+00,\n",
      "          1.2941e+00,  1.4189e+00],\n",
      "        [-1.4956e+00, -1.4926e+00, -1.4962e+00, -1.4802e+00, -1.4447e+00,\n",
      "         -1.3889e+00, -1.4454e+00, -1.5176e+00, -1.5182e+00, -1.5791e+00,\n",
      "         -1.6156e+00, -1.3312e+00],\n",
      "        [-9.8082e-01, -9.9821e-01, -1.0174e+00, -9.8474e-01, -9.8306e-01,\n",
      "         -1.0537e+00, -1.0948e+00, -1.1009e+00, -1.2486e+00, -1.1879e+00,\n",
      "         -1.3261e+00, -1.5770e+00],\n",
      "        [-4.4827e-01, -4.2598e-01, -4.7170e-01, -5.0833e-01, -5.3658e-01,\n",
      "         -4.8429e-01, -4.6625e-01, -5.6314e-01, -8.5518e-01, -6.4422e-01,\n",
      "         -6.9306e-01, -5.8945e-01],\n",
      "        [-2.8754e-01, -2.6618e-01, -2.4055e-01, -2.5398e-01, -2.6214e-01,\n",
      "         -2.5312e-01, -1.9995e-01, -4.5767e-02, -1.0117e-01,  5.1769e-03,\n",
      "          2.6909e-01,  3.9208e-02],\n",
      "        [-6.6804e-01, -6.8118e-01, -6.9006e-01, -6.8843e-01, -6.8661e-01,\n",
      "         -7.0477e-01, -6.9758e-01, -7.6647e-01, -7.3703e-01, -8.8528e-01,\n",
      "         -8.4409e-01, -1.0477e+00],\n",
      "        [ 3.8480e-01,  4.0056e-01,  4.4828e-01,  4.1493e-01,  4.4475e-01,\n",
      "          3.8905e-01,  3.9202e-01,  3.9932e-01,  3.8400e-01,  4.6933e-01,\n",
      "          6.3641e-01,  7.5942e-01],\n",
      "        [ 6.3132e-01,  7.0306e-01,  7.0168e-01,  7.2552e-01,  6.5285e-01,\n",
      "          6.9477e-01,  7.6193e-01,  8.2451e-01,  1.0676e+00,  1.2199e+00,\n",
      "          1.2498e+00,  1.1549e+00]], dtype=torch.float64,\n",
      "       grad_fn=<NativeBatchNormBackward0>)\n",
      "relu shape: torch.Size([13, 12]), val: tensor([[-1.3860e-01, -1.3898e-01, -1.3794e-01, -1.3600e-01, -1.4151e-01,\n",
      "         -1.4130e-01, -1.3301e-01, -1.2775e-01, -1.0752e-01, -1.2846e-01,\n",
      "         -1.2410e-01, -1.1925e-01],\n",
      "        [ 2.7624e-01,  2.3165e-01,  2.3755e-01,  1.9333e-01,  2.4887e-01,\n",
      "          2.3860e-01,  1.4134e-01,  1.2056e-01,  1.8332e-01,  4.3373e-01,\n",
      "         -1.3867e-04,  2.0022e-01],\n",
      "        [ 1.6032e+00,  1.6078e+00,  1.5739e+00,  1.6174e+00,  1.6356e+00,\n",
      "          1.5824e+00,  1.5834e+00,  1.5785e+00,  1.5001e+00,  1.1972e+00,\n",
      "          1.2804e+00,  1.1286e+00],\n",
      "        [ 9.8413e-01,  9.3369e-01,  9.3137e-01,  9.2687e-01,  9.9257e-01,\n",
      "          9.9790e-01,  9.8827e-01,  8.9536e-01,  8.0246e-01,  6.6111e-01,\n",
      "          6.5927e-01,  7.3904e-01],\n",
      "        [-3.1344e-02, -3.1502e-02, -2.9595e-02, -3.0245e-02, -2.8598e-02,\n",
      "         -3.0367e-02, -3.2312e-02, -2.0712e-02,  2.3268e-02,  2.0191e-02,\n",
      "          3.3198e-01,  2.9742e-01],\n",
      "        [ 1.7000e+00,  1.6922e+00,  1.6985e+00,  1.7001e+00,  1.6395e+00,\n",
      "          1.6987e+00,  1.6902e+00,  1.6602e+00,  1.5747e+00,  1.5745e+00,\n",
      "          1.2941e+00,  1.4189e+00],\n",
      "        [-1.4956e-01, -1.4926e-01, -1.4962e-01, -1.4802e-01, -1.4447e-01,\n",
      "         -1.3889e-01, -1.4454e-01, -1.5176e-01, -1.5182e-01, -1.5791e-01,\n",
      "         -1.6156e-01, -1.3312e-01],\n",
      "        [-9.8082e-02, -9.9821e-02, -1.0174e-01, -9.8474e-02, -9.8306e-02,\n",
      "         -1.0537e-01, -1.0948e-01, -1.1009e-01, -1.2486e-01, -1.1879e-01,\n",
      "         -1.3261e-01, -1.5770e-01],\n",
      "        [-4.4827e-02, -4.2598e-02, -4.7170e-02, -5.0833e-02, -5.3658e-02,\n",
      "         -4.8429e-02, -4.6625e-02, -5.6314e-02, -8.5518e-02, -6.4422e-02,\n",
      "         -6.9306e-02, -5.8945e-02],\n",
      "        [-2.8754e-02, -2.6618e-02, -2.4055e-02, -2.5398e-02, -2.6214e-02,\n",
      "         -2.5312e-02, -1.9995e-02, -4.5767e-03, -1.0117e-02,  5.1769e-03,\n",
      "          2.6909e-01,  3.9208e-02],\n",
      "        [-6.6804e-02, -6.8118e-02, -6.9006e-02, -6.8843e-02, -6.8661e-02,\n",
      "         -7.0477e-02, -6.9758e-02, -7.6647e-02, -7.3703e-02, -8.8528e-02,\n",
      "         -8.4409e-02, -1.0477e-01],\n",
      "        [ 3.8480e-01,  4.0056e-01,  4.4828e-01,  4.1493e-01,  4.4475e-01,\n",
      "          3.8905e-01,  3.9202e-01,  3.9932e-01,  3.8400e-01,  4.6933e-01,\n",
      "          6.3641e-01,  7.5942e-01],\n",
      "        [ 6.3132e-01,  7.0306e-01,  7.0168e-01,  7.2552e-01,  6.5285e-01,\n",
      "          6.9477e-01,  7.6193e-01,  8.2451e-01,  1.0676e+00,  1.2199e+00,\n",
      "          1.2498e+00,  1.1549e+00]], dtype=torch.float64,\n",
      "       grad_fn=<LeakyReluBackward1>)\n",
      "pool shape: torch.Size([13, 6]), val: tensor([[-0.1388, -0.1370, -0.1414, -0.1304, -0.1180, -0.1217],\n",
      "        [ 0.2539,  0.2154,  0.2437,  0.1310,  0.3085,  0.1000],\n",
      "        [ 1.6055,  1.5956,  1.6090,  1.5809,  1.3486,  1.2045],\n",
      "        [ 0.9589,  0.9291,  0.9952,  0.9418,  0.7318,  0.6992],\n",
      "        [-0.0314, -0.0299, -0.0295, -0.0265,  0.0217,  0.3147],\n",
      "        [ 1.6961,  1.6993,  1.6691,  1.6752,  1.5746,  1.3565],\n",
      "        [-0.1494, -0.1488, -0.1417, -0.1481, -0.1549, -0.1473],\n",
      "        [-0.0990, -0.1001, -0.1018, -0.1098, -0.1218, -0.1452],\n",
      "        [-0.0437, -0.0490, -0.0510, -0.0515, -0.0750, -0.0641],\n",
      "        [-0.0277, -0.0247, -0.0258, -0.0123, -0.0025,  0.1542],\n",
      "        [-0.0675, -0.0689, -0.0696, -0.0732, -0.0811, -0.0946],\n",
      "        [ 0.3927,  0.4316,  0.4169,  0.3957,  0.4267,  0.6979],\n",
      "        [ 0.6672,  0.7136,  0.6738,  0.7932,  1.1438,  1.2024]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([13, 6]), val: tensor([[ 0.0191,  0.0229,  0.0179,  0.0443,  0.1033, -0.0142],\n",
      "        [ 0.4887,  0.4809,  0.4829,  0.4917,  0.4123,  0.3153],\n",
      "        [-0.4558, -0.4747, -0.4625, -0.4902, -0.4442, -0.4381],\n",
      "        [ 0.1528,  0.1477,  0.1356,  0.1473,  0.1461,  0.0567],\n",
      "        [ 0.6919,  0.6795,  0.6950,  0.6676,  0.6519,  0.5658],\n",
      "        [-0.3118, -0.3022, -0.3153, -0.3023, -0.3243, -0.2626],\n",
      "        [ 0.1609,  0.1469,  0.1640,  0.1309,  0.0023,  0.0371],\n",
      "        [-0.9121, -0.9142, -0.9163, -0.9070, -0.9357, -0.9451],\n",
      "        [ 0.1126,  0.1103,  0.1071,  0.1172,  0.0355,  0.0045],\n",
      "        [-0.5804, -0.5787, -0.5786, -0.5559, -0.5400, -0.5047],\n",
      "        [-0.6167, -0.6154, -0.6121, -0.6447, -0.5307, -0.5208],\n",
      "        [ 0.6161,  0.6063,  0.6172,  0.6046,  0.4860,  0.4595],\n",
      "        [ 0.1306,  0.1365,  0.1200,  0.1393,  0.1513,  0.1566]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([13, 6]), val: tensor([[ 0.0191,  0.0229,  0.0179,  0.0443,  0.1033, -0.0014],\n",
      "        [ 0.4887,  0.4809,  0.4829,  0.4917,  0.4123,  0.3153],\n",
      "        [-0.0456, -0.0475, -0.0462, -0.0490, -0.0444, -0.0438],\n",
      "        [ 0.1528,  0.1477,  0.1356,  0.1473,  0.1461,  0.0567],\n",
      "        [ 0.6919,  0.6795,  0.6950,  0.6676,  0.6519,  0.5658],\n",
      "        [-0.0312, -0.0302, -0.0315, -0.0302, -0.0324, -0.0263],\n",
      "        [ 0.1609,  0.1469,  0.1640,  0.1309,  0.0023,  0.0371],\n",
      "        [-0.0912, -0.0914, -0.0916, -0.0907, -0.0936, -0.0945],\n",
      "        [ 0.1126,  0.1103,  0.1071,  0.1172,  0.0355,  0.0045],\n",
      "        [-0.0580, -0.0579, -0.0579, -0.0556, -0.0540, -0.0505],\n",
      "        [-0.0617, -0.0615, -0.0612, -0.0645, -0.0531, -0.0521],\n",
      "        [ 0.6161,  0.6063,  0.6172,  0.6046,  0.4860,  0.4595],\n",
      "        [ 0.1306,  0.1365,  0.1200,  0.1393,  0.1513,  0.1566]],\n",
      "       dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([13, 6]), val: tensor([[ 0.0191,  0.0229,  0.0179,  0.0443,  0.1033, -0.0014],\n",
      "        [ 0.4887,  0.4809,  0.4829,  0.4917,  0.4123,  0.3153],\n",
      "        [-0.0456, -0.0475, -0.0462, -0.0490, -0.0444, -0.0438],\n",
      "        [ 0.1528,  0.1477,  0.1356,  0.1473,  0.1461,  0.0567],\n",
      "        [ 0.6919,  0.6795,  0.6950,  0.6676,  0.6519,  0.5658],\n",
      "        [-0.0312, -0.0302, -0.0315, -0.0302, -0.0324, -0.0263],\n",
      "        [ 0.1609,  0.1469,  0.1640,  0.1309,  0.0023,  0.0371],\n",
      "        [-0.0912, -0.0914, -0.0916, -0.0907, -0.0936, -0.0945],\n",
      "        [ 0.1126,  0.1103,  0.1071,  0.1172,  0.0355,  0.0045],\n",
      "        [-0.0580, -0.0579, -0.0579, -0.0556, -0.0540, -0.0505],\n",
      "        [-0.0617, -0.0615, -0.0612, -0.0645, -0.0531, -0.0521],\n",
      "        [ 0.6161,  0.6063,  0.6172,  0.6046,  0.4860,  0.4595],\n",
      "        [ 0.1306,  0.1365,  0.1200,  0.1393,  0.1513,  0.1566]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([13, 6]), val: tensor([[ 0.0191,  0.0229,  0.0179,  0.0443,  0.1033, -0.0014],\n",
      "        [ 0.4887,  0.4809,  0.4829,  0.4917,  0.4123,  0.3153],\n",
      "        [-0.0456, -0.0475, -0.0462, -0.0490, -0.0444, -0.0438],\n",
      "        [ 0.1528,  0.1477,  0.1356,  0.1473,  0.1461,  0.0567],\n",
      "        [ 0.6919,  0.6795,  0.6950,  0.6676,  0.6519,  0.5658],\n",
      "        [-0.0312, -0.0302, -0.0315, -0.0302, -0.0324, -0.0263],\n",
      "        [ 0.1609,  0.1469,  0.1640,  0.1309,  0.0023,  0.0371],\n",
      "        [-0.0912, -0.0914, -0.0916, -0.0907, -0.0936, -0.0945],\n",
      "        [ 0.1126,  0.1103,  0.1071,  0.1172,  0.0355,  0.0045],\n",
      "        [-0.0580, -0.0579, -0.0579, -0.0556, -0.0540, -0.0505],\n",
      "        [-0.0617, -0.0615, -0.0612, -0.0645, -0.0531, -0.0521],\n",
      "        [ 0.6161,  0.6063,  0.6172,  0.6046,  0.4860,  0.4595],\n",
      "        [ 0.1306,  0.1365,  0.1200,  0.1393,  0.1513,  0.1566]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([13, 1]), val: tensor([[-0.0705],\n",
      "        [ 0.0258],\n",
      "        [-0.0767],\n",
      "        [-0.0424],\n",
      "        [ 0.0867],\n",
      "        [-0.0753],\n",
      "        [-0.0312],\n",
      "        [-0.0883],\n",
      "        [-0.0543],\n",
      "        [-0.0811],\n",
      "        [-0.0794],\n",
      "        [ 0.0591],\n",
      "        [-0.0464]], dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 68978915110.23187, val: 68978915110.23187\n",
      "conv1 shape: torch.Size([13, 12]), val: tensor([[ -93463.5689,  -80399.2129,  -73615.1735,  -89674.9217,  -89036.7465,\n",
      "          -86633.2531, -103066.3339, -107599.2203, -159308.6220, -133507.8286,\n",
      "         -134623.8465,  -98904.9612],\n",
      "        [  95417.4546,  109192.7329,  102284.4244,   93330.2603,   91374.4385,\n",
      "          103441.4462,   80214.2406,   80907.8626,  133501.7395,  176826.6766,\n",
      "          159315.7987,  133079.8792],\n",
      "        [ 261059.9936,  268283.6562,  255800.1130,  260577.3552,  269077.3552,\n",
      "          273374.0124,  273877.3212,  280665.2625,  312201.3458,  263538.0849,\n",
      "          227127.0796,  213042.2075],\n",
      "        [ 163678.0339,  170333.5614,  150019.7413,  159315.5407,  172963.4196,\n",
      "          164471.2078,  179458.7217,  207956.3036,  267088.4880,  187418.3885,\n",
      "          133196.7167,  164423.9100],\n",
      "        [ 135738.2900,  126544.2171,  142264.5748,  132764.9245,  125185.3754,\n",
      "          136292.8848,  107379.2210,  116395.4810,  148063.2410,  160041.8731,\n",
      "          154345.6376,  110316.2916],\n",
      "        [ 154813.2671,  170768.2056,  157538.2290,  168052.3018,  176778.3830,\n",
      "          179007.0159,  190731.4171,  195926.6792,  181843.8122,  161868.4027,\n",
      "          145821.5772,  137360.9597],\n",
      "        [-239710.0999, -257297.9920, -248697.6413, -244570.8255, -259694.2624,\n",
      "         -267895.5824, -245110.9245, -282139.6462, -234517.7322, -219784.3423,\n",
      "         -163702.9794, -146157.4300],\n",
      "        [-212304.0749, -221699.7140, -233868.7676, -211735.5494, -221292.8310,\n",
      "         -234183.0428, -232922.5152, -249347.9219, -315222.0478, -348649.2133,\n",
      "         -267491.6232, -283159.5840],\n",
      "        [  24752.8545,   39876.6106,   37100.7510,   29768.9283,   46510.8014,\n",
      "           48739.9807,   50159.9731,   26365.0743,    9570.4812,  -42893.0243,\n",
      "          -42159.8233,  -72279.8484],\n",
      "        [ 102215.0626,   99452.0329,  109654.3071,   97137.9006,   94821.0170,\n",
      "          100757.4835,  115027.4496,  129376.4884,  147004.2817,  170189.7597,\n",
      "          139646.9057,  158000.8159],\n",
      "        [-109158.0387, -118661.7248, -129279.1852, -127503.5784, -124857.2654,\n",
      "         -131251.4855, -144806.8056, -114087.4552, -116974.6914, -127741.2107,\n",
      "         -140562.5920, -134972.8267],\n",
      "        [ 178099.7244,  162579.0475,  147931.6203,  168875.0893,  167657.4667,\n",
      "          155290.3067,  172977.7095,  178619.2407,  233845.0060,  149006.2962,\n",
      "          165110.5377,  165894.3378],\n",
      "        [ 324950.8377,  326665.5665,  313667.5049,  343452.7052,  334062.2365,\n",
      "          325250.3779,  361054.7152,  344052.3761,  287186.0965,  229644.4472,\n",
      "          295521.7630,  303488.1265]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "bn1 shape: torch.Size([13, 12]), val: tensor([[-0.9119, -0.8176, -0.7647, -0.8676, -0.8500, -0.8182, -0.9033, -0.8977,\n",
      "         -1.1272, -0.9582, -1.0936, -0.8851],\n",
      "        [ 0.2070,  0.2771,  0.2713,  0.1933,  0.1771,  0.2499,  0.1002,  0.0996,\n",
      "          0.3198,  0.6787,  0.6320,  0.4937],\n",
      "        [ 1.1883,  1.1957,  1.1756,  1.1629,  1.1887,  1.2048,  1.1605,  1.1564,\n",
      "          1.2028,  1.1361,  1.0301,  0.9690],\n",
      "        [ 0.6114,  0.6301,  0.5525,  0.5758,  0.6415,  0.5928,  0.6435,  0.7717,\n",
      "          0.9799,  0.7346,  0.4787,  0.6800],\n",
      "        [ 0.4459,  0.3773,  0.5068,  0.4219,  0.3695,  0.4345,  0.2489,  0.2873,\n",
      "          0.3917,  0.5902,  0.6028,  0.3584],\n",
      "        [ 0.5589,  0.6326,  0.5968,  0.6265,  0.6632,  0.6745,  0.7053,  0.7081,\n",
      "          0.5586,  0.5998,  0.5528,  0.5192],\n",
      "        [-1.7782, -1.8391, -1.7959, -1.7655, -1.8215, -1.8369, -1.6810, -1.8211,\n",
      "         -1.4988, -1.4132, -1.2643, -1.1659],\n",
      "        [-1.6158, -1.6335, -1.7086, -1.5752, -1.6029, -1.6474, -1.6143, -1.6476,\n",
      "         -1.8976, -2.0930, -1.8736, -1.9802],\n",
      "        [-0.2116, -0.1231, -0.1126, -0.1752, -0.0783, -0.0575, -0.0644, -0.1890,\n",
      "         -0.2926, -0.4802, -0.5508, -0.7268],\n",
      "        [ 0.2473,  0.2209,  0.3148,  0.2154,  0.1967,  0.2348,  0.2908,  0.3560,\n",
      "          0.3865,  0.6437,  0.5165,  0.6418],\n",
      "        [-1.0048, -1.0386, -1.0926, -1.0869, -1.0539, -1.0690, -1.1319, -0.9320,\n",
      "         -0.9180, -0.9278, -1.1284, -1.0994],\n",
      "        [ 0.6968,  0.5854,  0.5402,  0.6313,  0.6113,  0.5413,  0.6080,  0.6165,\n",
      "          0.8156,  0.5320,  0.6660,  0.6888],\n",
      "        [ 1.5667,  1.5328,  1.5164,  1.6433,  1.5586,  1.4964,  1.6378,  1.4917,\n",
      "          1.0792,  0.9573,  1.4316,  1.5065]], dtype=torch.float64,\n",
      "       grad_fn=<NativeBatchNormBackward0>)\n",
      "relu shape: torch.Size([13, 12]), val: tensor([[-0.0912, -0.0818, -0.0765, -0.0868, -0.0850, -0.0818, -0.0903, -0.0898,\n",
      "         -0.1127, -0.0958, -0.1094, -0.0885],\n",
      "        [ 0.2070,  0.2771,  0.2713,  0.1933,  0.1771,  0.2499,  0.1002,  0.0996,\n",
      "          0.3198,  0.6787,  0.6320,  0.4937],\n",
      "        [ 1.1883,  1.1957,  1.1756,  1.1629,  1.1887,  1.2048,  1.1605,  1.1564,\n",
      "          1.2028,  1.1361,  1.0301,  0.9690],\n",
      "        [ 0.6114,  0.6301,  0.5525,  0.5758,  0.6415,  0.5928,  0.6435,  0.7717,\n",
      "          0.9799,  0.7346,  0.4787,  0.6800],\n",
      "        [ 0.4459,  0.3773,  0.5068,  0.4219,  0.3695,  0.4345,  0.2489,  0.2873,\n",
      "          0.3917,  0.5902,  0.6028,  0.3584],\n",
      "        [ 0.5589,  0.6326,  0.5968,  0.6265,  0.6632,  0.6745,  0.7053,  0.7081,\n",
      "          0.5586,  0.5998,  0.5528,  0.5192],\n",
      "        [-0.1778, -0.1839, -0.1796, -0.1766, -0.1821, -0.1837, -0.1681, -0.1821,\n",
      "         -0.1499, -0.1413, -0.1264, -0.1166],\n",
      "        [-0.1616, -0.1634, -0.1709, -0.1575, -0.1603, -0.1647, -0.1614, -0.1648,\n",
      "         -0.1898, -0.2093, -0.1874, -0.1980],\n",
      "        [-0.0212, -0.0123, -0.0113, -0.0175, -0.0078, -0.0058, -0.0064, -0.0189,\n",
      "         -0.0293, -0.0480, -0.0551, -0.0727],\n",
      "        [ 0.2473,  0.2209,  0.3148,  0.2154,  0.1967,  0.2348,  0.2908,  0.3560,\n",
      "          0.3865,  0.6437,  0.5165,  0.6418],\n",
      "        [-0.1005, -0.1039, -0.1093, -0.1087, -0.1054, -0.1069, -0.1132, -0.0932,\n",
      "         -0.0918, -0.0928, -0.1128, -0.1099],\n",
      "        [ 0.6968,  0.5854,  0.5402,  0.6313,  0.6113,  0.5413,  0.6080,  0.6165,\n",
      "          0.8156,  0.5320,  0.6660,  0.6888],\n",
      "        [ 1.5667,  1.5328,  1.5164,  1.6433,  1.5586,  1.4964,  1.6378,  1.4917,\n",
      "          1.0792,  0.9573,  1.4316,  1.5065]], dtype=torch.float64,\n",
      "       grad_fn=<LeakyReluBackward1>)\n",
      "pool shape: torch.Size([13, 6]), val: tensor([[-0.0865, -0.0816, -0.0834, -0.0901, -0.1043, -0.0989],\n",
      "        [ 0.2421,  0.2323,  0.2135,  0.0999,  0.4992,  0.5629],\n",
      "        [ 1.1920,  1.1692,  1.1967,  1.1584,  1.1694,  0.9995],\n",
      "        [ 0.6208,  0.5642,  0.6172,  0.7076,  0.8572,  0.5793],\n",
      "        [ 0.4116,  0.4644,  0.4020,  0.2681,  0.4910,  0.4806],\n",
      "        [ 0.5958,  0.6116,  0.6689,  0.7067,  0.5792,  0.5360],\n",
      "        [-0.1809, -0.1781, -0.1829, -0.1751, -0.1456, -0.1215],\n",
      "        [-0.1625, -0.1642, -0.1625, -0.1631, -0.1995, -0.1927],\n",
      "        [-0.0167, -0.0144, -0.0068, -0.0127, -0.0386, -0.0639],\n",
      "        [ 0.2341,  0.2651,  0.2157,  0.3234,  0.5151,  0.5792],\n",
      "        [-0.1022, -0.1090, -0.1061, -0.1032, -0.0923, -0.1114],\n",
      "        [ 0.6411,  0.5857,  0.5763,  0.6123,  0.6738,  0.6774],\n",
      "        [ 1.5498,  1.5798,  1.5275,  1.5648,  1.0183,  1.4691]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([13, 6]), val: tensor([[-0.0766, -0.0774, -0.0602, -0.0281, -0.2101, -0.1323],\n",
      "        [ 0.2300,  0.2334,  0.2596,  0.2360,  0.1307,  0.0618],\n",
      "        [-0.3304, -0.3116, -0.3312, -0.4001, -0.2072, -0.2222],\n",
      "        [ 0.0277,  0.0564,  0.0494,  0.0299,  0.0147,  0.0557],\n",
      "        [ 0.4906,  0.4853,  0.4959,  0.4751,  0.5505,  0.5131],\n",
      "        [-0.2139, -0.2050, -0.2170, -0.2479, -0.2737, -0.2701],\n",
      "        [-0.1935, -0.1976, -0.1761, -0.1626,  0.0133, -0.1517],\n",
      "        [-0.9234, -0.9273, -0.9117, -0.9174, -0.9876, -1.0219],\n",
      "        [-0.1887, -0.1775, -0.1625, -0.1628, -0.1779, -0.2507],\n",
      "        [-0.5010, -0.4938, -0.4919, -0.4611, -0.5286, -0.5144],\n",
      "        [-0.4235, -0.4450, -0.4577, -0.4430, -0.3357, -0.2578],\n",
      "        [ 0.3480,  0.3557,  0.3641,  0.3471,  0.4223,  0.3181],\n",
      "        [ 0.1874,  0.2060,  0.1918,  0.1715,  0.1082,  0.1567]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([13, 6]), val: tensor([[-0.0077, -0.0077, -0.0060, -0.0028, -0.0210, -0.0132],\n",
      "        [ 0.2300,  0.2334,  0.2596,  0.2360,  0.1307,  0.0618],\n",
      "        [-0.0330, -0.0312, -0.0331, -0.0400, -0.0207, -0.0222],\n",
      "        [ 0.0277,  0.0564,  0.0494,  0.0299,  0.0147,  0.0557],\n",
      "        [ 0.4906,  0.4853,  0.4959,  0.4751,  0.5505,  0.5131],\n",
      "        [-0.0214, -0.0205, -0.0217, -0.0248, -0.0274, -0.0270],\n",
      "        [-0.0193, -0.0198, -0.0176, -0.0163,  0.0133, -0.0152],\n",
      "        [-0.0923, -0.0927, -0.0912, -0.0917, -0.0988, -0.1022],\n",
      "        [-0.0189, -0.0178, -0.0162, -0.0163, -0.0178, -0.0251],\n",
      "        [-0.0501, -0.0494, -0.0492, -0.0461, -0.0529, -0.0514],\n",
      "        [-0.0424, -0.0445, -0.0458, -0.0443, -0.0336, -0.0258],\n",
      "        [ 0.3480,  0.3557,  0.3641,  0.3471,  0.4223,  0.3181],\n",
      "        [ 0.1874,  0.2060,  0.1918,  0.1715,  0.1082,  0.1567]],\n",
      "       dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([13, 6]), val: tensor([[-0.0077, -0.0077, -0.0060, -0.0028, -0.0210, -0.0132],\n",
      "        [ 0.2300,  0.2334,  0.2596,  0.2360,  0.1307,  0.0618],\n",
      "        [-0.0330, -0.0312, -0.0331, -0.0400, -0.0207, -0.0222],\n",
      "        [ 0.0277,  0.0564,  0.0494,  0.0299,  0.0147,  0.0557],\n",
      "        [ 0.4906,  0.4853,  0.4959,  0.4751,  0.5505,  0.5131],\n",
      "        [-0.0214, -0.0205, -0.0217, -0.0248, -0.0274, -0.0270],\n",
      "        [-0.0193, -0.0198, -0.0176, -0.0163,  0.0133, -0.0152],\n",
      "        [-0.0923, -0.0927, -0.0912, -0.0917, -0.0988, -0.1022],\n",
      "        [-0.0189, -0.0178, -0.0162, -0.0163, -0.0178, -0.0251],\n",
      "        [-0.0501, -0.0494, -0.0492, -0.0461, -0.0529, -0.0514],\n",
      "        [-0.0424, -0.0445, -0.0458, -0.0443, -0.0336, -0.0258],\n",
      "        [ 0.3480,  0.3557,  0.3641,  0.3471,  0.4223,  0.3181],\n",
      "        [ 0.1874,  0.2060,  0.1918,  0.1715,  0.1082,  0.1567]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([13, 6]), val: tensor([[-0.0077, -0.0077, -0.0060, -0.0028, -0.0210, -0.0132],\n",
      "        [ 0.2300,  0.2334,  0.2596,  0.2360,  0.1307,  0.0618],\n",
      "        [-0.0330, -0.0312, -0.0331, -0.0400, -0.0207, -0.0222],\n",
      "        [ 0.0277,  0.0564,  0.0494,  0.0299,  0.0147,  0.0557],\n",
      "        [ 0.4906,  0.4853,  0.4959,  0.4751,  0.5505,  0.5131],\n",
      "        [-0.0214, -0.0205, -0.0217, -0.0248, -0.0274, -0.0270],\n",
      "        [-0.0193, -0.0198, -0.0176, -0.0163,  0.0133, -0.0152],\n",
      "        [-0.0923, -0.0927, -0.0912, -0.0917, -0.0988, -0.1022],\n",
      "        [-0.0189, -0.0178, -0.0162, -0.0163, -0.0178, -0.0251],\n",
      "        [-0.0501, -0.0494, -0.0492, -0.0461, -0.0529, -0.0514],\n",
      "        [-0.0424, -0.0445, -0.0458, -0.0443, -0.0336, -0.0258],\n",
      "        [ 0.3480,  0.3557,  0.3641,  0.3471,  0.4223,  0.3181],\n",
      "        [ 0.1874,  0.2060,  0.1918,  0.1715,  0.1082,  0.1567]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([13, 1]), val: tensor([[-0.0718],\n",
      "        [-0.0179],\n",
      "        [-0.0717],\n",
      "        [-0.0551],\n",
      "        [ 0.0491],\n",
      "        [-0.0720],\n",
      "        [-0.0710],\n",
      "        [-0.0882],\n",
      "        [-0.0722],\n",
      "        [-0.0803],\n",
      "        [-0.0770],\n",
      "        [ 0.0165],\n",
      "        [-0.0253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 72556027085.88089, val: 72556027085.88089\n",
      "conv1 shape: torch.Size([13, 12]), val: tensor([[ -68260.6282,  -51353.0629,  -90679.9669,  -91058.7587,  -66969.7954,\n",
      "          -72103.3120, -103912.8270, -136012.5691, -150567.3162, -116872.2787,\n",
      "         -120402.7783, -139142.2188],\n",
      "        [  81516.9430,   81525.6384,   49403.8185,   63168.4713,   51626.7294,\n",
      "           69021.8912,   86806.1437,   65772.9649,   60651.5155,  131847.5086,\n",
      "          103025.1415,  117496.5642],\n",
      "        [ 195886.5332,  190576.8343,  185482.6528,  184008.3714,  171028.7240,\n",
      "          178421.5857,  183819.9822,  166815.4876,  206642.1061,  190226.6290,\n",
      "          167537.5331,  155839.5991],\n",
      "        [  93636.0388,  118532.3615,  106137.9532,  113138.6872,  107037.5983,\n",
      "          110145.5164,  137133.2977,  158791.3160,  170845.7744,  105183.8040,\n",
      "          138189.7136,  139972.1231],\n",
      "        [ -20408.4366,    6804.9722,   27719.1762,   36482.3315,   24292.3747,\n",
      "           46994.0668,   41909.4661,   42351.6053,   58467.1825,   63219.5933,\n",
      "          101958.5086,   97519.8111],\n",
      "        [ 234027.8488,  236697.7430,  214446.2326,  210048.5580,  212961.2634,\n",
      "          201934.5346,  199501.4234,  195821.3129,  206090.7876,  170257.1235,\n",
      "          166799.0052,  168978.9100],\n",
      "        [-220607.3474, -209385.2169, -194336.7744, -182477.6213, -192514.8039,\n",
      "         -198525.6687, -164336.9445, -140783.0662, -152586.9800, -165981.2868,\n",
      "         -114422.7437, -106705.9104],\n",
      "        [-125954.1222, -149003.6584, -144743.4578, -156753.4618, -158788.5866,\n",
      "         -144745.1592, -136092.5736, -200616.5765, -272059.1477, -237766.7777,\n",
      "         -247190.2997, -245721.4475],\n",
      "        [  79025.3723,   48000.0096,   58568.6427,   65727.3063,   69915.5023,\n",
      "           79063.0374,   51422.7742,   36800.0926,    6990.6006,  -47660.8336,\n",
      "          -47687.3152,  -55566.6607],\n",
      "        [  63794.2906,   29064.4302,   60487.0165,   67515.6498,   65671.9547,\n",
      "           85778.0503,  114730.9377,  111893.1855,  103395.0936,  138098.3804,\n",
      "          133228.6998,  150489.5340],\n",
      "        [-105557.5488, -113373.7432, -108834.0940,  -94481.0000,  -99578.3866,\n",
      "          -83435.8405,  -89189.5145,  -97708.6993,  -98946.5981, -117515.5843,\n",
      "         -121020.7095, -149357.0528],\n",
      "        [  88033.0869,  130768.5507,  138259.4607,  144089.1329,  142579.7089,\n",
      "          150754.7906,  205603.9095,  192997.5763,  191305.8802,  136224.8787,\n",
      "          179746.5995,  195703.3977],\n",
      "        [ 259990.4268,  329007.7243,  298137.1450,  303174.6191,  318513.2297,\n",
      "          327159.4049,  312520.0989,  305476.1859,  276471.1542,  283660.4096,\n",
      "          326531.3309,  312764.8623]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "bn1 shape: torch.Size([13, 12]), val: tensor([[-0.7982, -0.6765, -0.9657, -1.0063, -0.8171, -0.9106, -1.1749, -1.2672,\n",
      "         -1.2078, -1.0261, -1.0871, -1.1671],\n",
      "        [ 0.2792,  0.2119,  0.0229,  0.0864,  0.0137,  0.0792,  0.1548,  0.0787,\n",
      "          0.0856,  0.5906,  0.3279,  0.4215],\n",
      "        [ 1.1019,  0.9410,  0.9832,  0.9426,  0.8501,  0.8465,  0.8311,  0.7527,\n",
      "          0.9796,  0.9700,  0.7364,  0.6588],\n",
      "        [ 0.3664,  0.4593,  0.4233,  0.4405,  0.4019,  0.3676,  0.5056,  0.6992,\n",
      "          0.7604,  0.4172,  0.5506,  0.5606],\n",
      "        [-0.4540, -0.2877, -0.1301, -0.1026, -0.1778, -0.0753, -0.1583, -0.0775,\n",
      "          0.0722,  0.1445,  0.3211,  0.2978],\n",
      "        [ 1.3762,  1.2493,  1.1876,  1.1271,  1.1439,  1.0114,  0.9405,  0.9462,\n",
      "          0.9762,  0.8402,  0.7317,  0.7401],\n",
      "        [-1.8940, -1.7331, -1.6971, -1.6540, -1.6966, -1.7972, -1.5962, -1.2990,\n",
      "         -1.2202, -1.3454, -1.0492, -0.9663],\n",
      "        [-1.2131, -1.3294, -1.3472, -1.4717, -1.4604, -1.4200, -1.3993, -1.6981,\n",
      "         -1.9518, -1.8120, -1.8900, -1.8268],\n",
      "        [ 0.2613, -0.0123,  0.0876,  0.1046,  0.1418,  0.1496, -0.0919, -0.1145,\n",
      "         -0.2430, -0.5763, -0.6266, -0.6498],\n",
      "        [ 0.1517, -0.1389,  0.1011,  0.1172,  0.1121,  0.1967,  0.3494,  0.3864,\n",
      "          0.3474,  0.6312,  0.5191,  0.6257],\n",
      "        [-1.0664, -1.0912, -1.0938, -1.0305, -1.0456, -0.9901, -1.0723, -1.0117,\n",
      "         -0.8917, -1.0303, -1.0910, -1.2303],\n",
      "        [ 0.3261,  0.5411,  0.6500,  0.6598,  0.6508,  0.6524,  0.9830,  0.9273,\n",
      "          0.8857,  0.6190,  0.8137,  0.9056],\n",
      "        [ 1.5630,  1.8665,  1.7782,  1.7869,  1.8833,  1.8897,  1.7284,  1.6776,\n",
      "          1.4072,  1.5773,  1.7433,  1.6301]], dtype=torch.float64,\n",
      "       grad_fn=<NativeBatchNormBackward0>)\n",
      "relu shape: torch.Size([13, 12]), val: tensor([[-7.9815e-02, -6.7652e-02, -9.6565e-02, -1.0063e-01, -8.1715e-02,\n",
      "         -9.1057e-02, -1.1749e-01, -1.2672e-01, -1.2078e-01, -1.0261e-01,\n",
      "         -1.0871e-01, -1.1671e-01],\n",
      "        [ 2.7920e-01,  2.1187e-01,  2.2909e-02,  8.6442e-02,  1.3672e-02,\n",
      "          7.9210e-02,  1.5476e-01,  7.8733e-02,  8.5622e-02,  5.9055e-01,\n",
      "          3.2787e-01,  4.2147e-01],\n",
      "        [ 1.1019e+00,  9.4096e-01,  9.8321e-01,  9.4260e-01,  8.5014e-01,\n",
      "          8.4648e-01,  8.3112e-01,  7.5270e-01,  9.7961e-01,  9.7002e-01,\n",
      "          7.3642e-01,  6.5881e-01],\n",
      "        [ 3.6637e-01,  4.5929e-01,  4.2328e-01,  4.4048e-01,  4.0185e-01,\n",
      "          3.6763e-01,  5.0563e-01,  6.9918e-01,  7.6041e-01,  4.1724e-01,\n",
      "          5.5056e-01,  5.6059e-01],\n",
      "        [-4.5395e-02, -2.8769e-02, -1.3012e-02, -1.0263e-02, -1.7782e-02,\n",
      "         -7.5282e-03, -1.5826e-02, -7.7491e-03,  7.2246e-02,  1.4447e-01,\n",
      "          3.2111e-01,  2.9782e-01],\n",
      "        [ 1.3762e+00,  1.2493e+00,  1.1876e+00,  1.1271e+00,  1.1439e+00,\n",
      "          1.0114e+00,  9.4045e-01,  9.4618e-01,  9.7624e-01,  8.4022e-01,\n",
      "          7.3174e-01,  7.4013e-01],\n",
      "        [-1.8940e-01, -1.7331e-01, -1.6971e-01, -1.6540e-01, -1.6966e-01,\n",
      "         -1.7972e-01, -1.5962e-01, -1.2990e-01, -1.2202e-01, -1.3454e-01,\n",
      "         -1.0492e-01, -9.6630e-02],\n",
      "        [-1.2131e-01, -1.3294e-01, -1.3472e-01, -1.4717e-01, -1.4604e-01,\n",
      "         -1.4200e-01, -1.3993e-01, -1.6981e-01, -1.9518e-01, -1.8120e-01,\n",
      "         -1.8900e-01, -1.8268e-01],\n",
      "        [ 2.6128e-01, -1.2272e-03,  8.7584e-02,  1.0457e-01,  1.4179e-01,\n",
      "          1.4963e-01, -9.1932e-03, -1.1452e-02, -2.4298e-02, -5.7626e-02,\n",
      "         -6.2658e-02, -6.4976e-02],\n",
      "        [ 1.5172e-01, -1.3887e-02,  1.0112e-01,  1.1724e-01,  1.1207e-01,\n",
      "          1.9673e-01,  3.4944e-01,  3.8636e-01,  3.4737e-01,  6.3118e-01,\n",
      "          5.1914e-01,  6.2569e-01],\n",
      "        [-1.0664e-01, -1.0912e-01, -1.0938e-01, -1.0305e-01, -1.0456e-01,\n",
      "         -9.9005e-02, -1.0723e-01, -1.0117e-01, -8.9170e-02, -1.0303e-01,\n",
      "         -1.0910e-01, -1.2303e-01],\n",
      "        [ 3.2607e-01,  5.4110e-01,  6.4996e-01,  6.5977e-01,  6.5084e-01,\n",
      "          6.5244e-01,  9.8300e-01,  9.2734e-01,  8.8570e-01,  6.1901e-01,\n",
      "          8.1374e-01,  9.0555e-01],\n",
      "        [ 1.5630e+00,  1.8665e+00,  1.7782e+00,  1.7869e+00,  1.8833e+00,\n",
      "          1.8897e+00,  1.7284e+00,  1.6776e+00,  1.4072e+00,  1.5773e+00,\n",
      "          1.7433e+00,  1.6301e+00]], dtype=torch.float64,\n",
      "       grad_fn=<LeakyReluBackward1>)\n",
      "pool shape: torch.Size([13, 6]), val: tensor([[-0.0737, -0.0986, -0.0864, -0.1221, -0.1117, -0.1127],\n",
      "        [ 0.2455,  0.0547,  0.0464,  0.1167,  0.3381,  0.3747],\n",
      "        [ 1.0214,  0.9629,  0.8483,  0.7919,  0.9748,  0.6976],\n",
      "        [ 0.4128,  0.4319,  0.3847,  0.6024,  0.5888,  0.5556],\n",
      "        [-0.0371, -0.0116, -0.0127, -0.0118,  0.1084,  0.3095],\n",
      "        [ 1.3128,  1.1573,  1.0776,  0.9433,  0.9082,  0.7359],\n",
      "        [-0.1814, -0.1676, -0.1747, -0.1448, -0.1283, -0.1008],\n",
      "        [-0.1271, -0.1409, -0.1440, -0.1549, -0.1882, -0.1858],\n",
      "        [ 0.1300,  0.0961,  0.1457, -0.0103, -0.0410, -0.0638],\n",
      "        [ 0.0689,  0.1092,  0.1544,  0.3679,  0.4893,  0.5724],\n",
      "        [-0.1079, -0.1062, -0.1018, -0.1042, -0.0961, -0.1161],\n",
      "        [ 0.4336,  0.6549,  0.6516,  0.9552,  0.7524,  0.8596],\n",
      "        [ 1.7147,  1.7825,  1.8865,  1.7030,  1.4923,  1.6867]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([13, 6]), val: tensor([[ 0.1631,  0.1291,  0.1349,  0.0641, -0.0055, -0.0201],\n",
      "        [ 0.3636,  0.2895,  0.2661,  0.0962,  0.1076,  0.0091],\n",
      "        [-0.4486, -0.5156, -0.5045, -0.5278, -0.4026, -0.3547],\n",
      "        [ 0.1586,  0.0734,  0.0680, -0.0543,  0.0537, -0.0201],\n",
      "        [ 0.5023,  0.4356,  0.3974,  0.4161,  0.4817,  0.4564],\n",
      "        [-0.3146, -0.2819, -0.2926, -0.3232, -0.3088, -0.3287],\n",
      "        [-0.1716, -0.1976, -0.2216, -0.1562, -0.1350, -0.1440],\n",
      "        [-0.8441, -0.8174, -0.7912, -0.8905, -0.9558, -0.9774],\n",
      "        [-0.1326, -0.1646, -0.2054, -0.2352, -0.2023, -0.2641],\n",
      "        [-0.4363, -0.4206, -0.3843, -0.4147, -0.4823, -0.4274],\n",
      "        [-0.4572, -0.4155, -0.3900, -0.2276, -0.2679, -0.1852],\n",
      "        [ 0.2591,  0.2181,  0.1606,  0.1722,  0.2706,  0.2049],\n",
      "        [ 0.1964,  0.1847,  0.1879,  0.1075,  0.1359,  0.1134]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([13, 6]), val: tensor([[ 0.1631,  0.1291,  0.1349,  0.0641, -0.0005, -0.0020],\n",
      "        [ 0.3636,  0.2895,  0.2661,  0.0962,  0.1076,  0.0091],\n",
      "        [-0.0449, -0.0516, -0.0504, -0.0528, -0.0403, -0.0355],\n",
      "        [ 0.1586,  0.0734,  0.0680, -0.0054,  0.0537, -0.0020],\n",
      "        [ 0.5023,  0.4356,  0.3974,  0.4161,  0.4817,  0.4564],\n",
      "        [-0.0315, -0.0282, -0.0293, -0.0323, -0.0309, -0.0329],\n",
      "        [-0.0172, -0.0198, -0.0222, -0.0156, -0.0135, -0.0144],\n",
      "        [-0.0844, -0.0817, -0.0791, -0.0890, -0.0956, -0.0977],\n",
      "        [-0.0133, -0.0165, -0.0205, -0.0235, -0.0202, -0.0264],\n",
      "        [-0.0436, -0.0421, -0.0384, -0.0415, -0.0482, -0.0427],\n",
      "        [-0.0457, -0.0416, -0.0390, -0.0228, -0.0268, -0.0185],\n",
      "        [ 0.2591,  0.2181,  0.1606,  0.1722,  0.2706,  0.2049],\n",
      "        [ 0.1964,  0.1847,  0.1879,  0.1075,  0.1359,  0.1134]],\n",
      "       dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([13, 6]), val: tensor([[ 0.1631,  0.1291,  0.1349,  0.0641, -0.0005, -0.0020],\n",
      "        [ 0.3636,  0.2895,  0.2661,  0.0962,  0.1076,  0.0091],\n",
      "        [-0.0449, -0.0516, -0.0504, -0.0528, -0.0403, -0.0355],\n",
      "        [ 0.1586,  0.0734,  0.0680, -0.0054,  0.0537, -0.0020],\n",
      "        [ 0.5023,  0.4356,  0.3974,  0.4161,  0.4817,  0.4564],\n",
      "        [-0.0315, -0.0282, -0.0293, -0.0323, -0.0309, -0.0329],\n",
      "        [-0.0172, -0.0198, -0.0222, -0.0156, -0.0135, -0.0144],\n",
      "        [-0.0844, -0.0817, -0.0791, -0.0890, -0.0956, -0.0977],\n",
      "        [-0.0133, -0.0165, -0.0205, -0.0235, -0.0202, -0.0264],\n",
      "        [-0.0436, -0.0421, -0.0384, -0.0415, -0.0482, -0.0427],\n",
      "        [-0.0457, -0.0416, -0.0390, -0.0228, -0.0268, -0.0185],\n",
      "        [ 0.2591,  0.2181,  0.1606,  0.1722,  0.2706,  0.2049],\n",
      "        [ 0.1964,  0.1847,  0.1879,  0.1075,  0.1359,  0.1134]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([13, 6]), val: tensor([[ 0.1631,  0.1291,  0.1349,  0.0641, -0.0005, -0.0020],\n",
      "        [ 0.3636,  0.2895,  0.2661,  0.0962,  0.1076,  0.0091],\n",
      "        [-0.0449, -0.0516, -0.0504, -0.0528, -0.0403, -0.0355],\n",
      "        [ 0.1586,  0.0734,  0.0680, -0.0054,  0.0537, -0.0020],\n",
      "        [ 0.5023,  0.4356,  0.3974,  0.4161,  0.4817,  0.4564],\n",
      "        [-0.0315, -0.0282, -0.0293, -0.0323, -0.0309, -0.0329],\n",
      "        [-0.0172, -0.0198, -0.0222, -0.0156, -0.0135, -0.0144],\n",
      "        [-0.0844, -0.0817, -0.0791, -0.0890, -0.0956, -0.0977],\n",
      "        [-0.0133, -0.0165, -0.0205, -0.0235, -0.0202, -0.0264],\n",
      "        [-0.0436, -0.0421, -0.0384, -0.0415, -0.0482, -0.0427],\n",
      "        [-0.0457, -0.0416, -0.0390, -0.0228, -0.0268, -0.0185],\n",
      "        [ 0.2591,  0.2181,  0.1606,  0.1722,  0.2706,  0.2049],\n",
      "        [ 0.1964,  0.1847,  0.1879,  0.1075,  0.1359,  0.1134]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([13, 1]), val: tensor([[-0.0170],\n",
      "        [ 0.0560],\n",
      "        [-0.0763],\n",
      "        [-0.0134],\n",
      "        [ 0.0304],\n",
      "        [-0.0737],\n",
      "        [-0.0741],\n",
      "        [-0.0832],\n",
      "        [-0.0705],\n",
      "        [-0.0765],\n",
      "        [-0.0828],\n",
      "        [-0.0181],\n",
      "        [ 0.0007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 62008020315.06762, val: 62008020315.06762\n",
      "conv1 shape: torch.Size([13, 12]), val: tensor([[ -99040.9125, -105654.1185,  -94789.0545, -109275.6149, -102384.0106,\n",
      "         -114191.8014, -112029.7448, -108708.2496,  -90323.2869, -130123.6777,\n",
      "         -142387.5891, -157356.7302],\n",
      "        [ 124769.2074,  136885.4563,  166632.0029,  160510.8084,  169994.1031,\n",
      "          166404.9083,  143368.5791,  101578.0021,  118793.5871,  142591.9586,\n",
      "          129643.8693,  117109.9058],\n",
      "        [ 186290.3947,  199791.2172,  194320.7424,  198061.5542,  195454.4662,\n",
      "          168115.6752,  150340.1712,  181364.6681,  173067.9851,  179888.3527,\n",
      "          156498.7577,  163719.4160],\n",
      "        [  87641.7956,   79874.3373,   87795.7459,   87010.1503,   88616.5894,\n",
      "           89064.6893,   78396.0911,   70653.1253,  115277.7424,  131067.8154,\n",
      "          135267.9841,   90398.3364],\n",
      "        [  83903.3077,   82782.3111,   63080.4994,   69481.7468,   54136.2023,\n",
      "           74622.1186,   83511.7158,  117064.5118,  128693.9997,  182719.0056,\n",
      "          109099.6525,  124259.5218],\n",
      "        [ 168863.5933,  174902.2007,  165729.5734,  170584.5723,  167487.1363,\n",
      "          155750.6859,  168561.2713,  146443.5981,  118708.6042,  143264.1309,\n",
      "          149511.1830,  140600.6829],\n",
      "        [-126397.9571, -120673.0537, -113341.6997, -106725.1074, -105958.0400,\n",
      "          -96376.0240, -112289.1885, -132353.1485, -123955.6931, -122072.3836,\n",
      "         -110697.9919, -152185.4263],\n",
      "        [-181466.3489, -204229.4905, -207524.8049, -239801.4584, -238090.6339,\n",
      "         -232396.3489, -212226.7983, -229818.0728, -257681.8609, -281140.5221,\n",
      "         -312695.3247, -286752.0346],\n",
      "        [-117150.4650, -111090.9146, -126260.4543, -104609.4286, -119451.8088,\n",
      "         -102451.9326,  -93087.1689,  -87232.4816, -104301.1681,  -66988.0029,\n",
      "         -109454.5114, -110608.3289],\n",
      "        [  76602.1592,   72984.5349,   84138.6308,   96315.6125,  103803.3776,\n",
      "          108869.1826,  104586.7745,  131427.8592,  165718.4585,  180730.5801,\n",
      "          176414.9709,  168111.0781],\n",
      "        [-174506.9286, -164171.6810, -159380.7841, -166568.5371, -157735.2487,\n",
      "         -150367.8722, -155976.4716, -165549.5902, -155572.0737, -105901.9829,\n",
      "         -125659.4176, -117559.0304],\n",
      "        [ 137889.1484,  134657.1405,  136288.6175,  135890.8657,  152109.9948,\n",
      "          165814.7266,  160437.5277,  162909.7376,  188198.6520,  248646.9491,\n",
      "          196329.7412,  184307.7620],\n",
      "        [ 337198.8754,  327728.3114,  339818.9068,  346782.5804,  355600.2535,\n",
      "          350344.4612,  367644.9039,  394448.7799,  444699.5471,  372602.2563,\n",
      "          359398.3342,  330633.2573]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "bn1 shape: torch.Size([13, 12]), val: tensor([[-0.8850, -0.9196, -0.8541, -0.9157, -0.8765, -0.9848, -0.9722, -0.9051,\n",
      "         -0.7984, -1.0924, -1.0512, -1.1241],\n",
      "        [ 0.5518,  0.6249,  0.7869,  0.7243,  0.7616,  0.7526,  0.6197,  0.3349,\n",
      "          0.3466,  0.4164,  0.4586,  0.4548],\n",
      "        [ 0.9468,  1.0255,  0.9607,  0.9526,  0.9148,  0.7632,  0.6632,  0.8054,\n",
      "          0.6438,  0.6227,  0.6076,  0.7229],\n",
      "        [ 0.3135,  0.2619,  0.2921,  0.2775,  0.2722,  0.2737,  0.2148,  0.1525,\n",
      "          0.3274,  0.3526,  0.4898,  0.3011],\n",
      "        [ 0.2895,  0.2804,  0.1369,  0.1710,  0.0649,  0.1843,  0.2466,  0.4262,\n",
      "          0.4008,  0.6384,  0.3445,  0.4959],\n",
      "        [ 0.8349,  0.8670,  0.7813,  0.7856,  0.7466,  0.6866,  0.7767,  0.5995,\n",
      "          0.3462,  0.4201,  0.5688,  0.5899],\n",
      "        [-1.0607, -1.0153, -0.9705, -0.9002, -0.8980, -0.8745, -0.9738, -1.0446,\n",
      "         -0.9825, -1.0478, -0.8754, -1.0944],\n",
      "        [-1.4142, -1.5474, -1.5618, -1.7092, -1.6926, -1.7167, -1.5967, -1.6193,\n",
      "         -1.7147, -1.9278, -1.9965, -1.8685],\n",
      "        [-1.0013, -0.9543, -1.0516, -0.8873, -0.9791, -0.9121, -0.8541, -0.7785,\n",
      "         -0.8749, -0.7431, -0.8685, -0.8552],\n",
      "        [ 0.2426,  0.2180,  0.2691,  0.3341,  0.3636,  0.3963,  0.3780,  0.5109,\n",
      "          0.6036,  0.6274,  0.7182,  0.7482],\n",
      "        [-1.3695, -1.2923, -1.2595, -1.2640, -1.2094, -1.2088, -1.2461, -1.2403,\n",
      "         -1.1556, -0.9584, -0.9584, -0.8952],\n",
      "        [ 0.6361,  0.6108,  0.5965,  0.5747,  0.6541,  0.7489,  0.7261,  0.6966,\n",
      "          0.7266,  1.0031,  0.8287,  0.8414],\n",
      "        [ 1.9157,  1.8403,  1.8741,  1.8567,  1.8779,  1.8915,  2.0176,  2.0619,\n",
      "          2.1311,  1.6889,  1.7337,  1.6831]], dtype=torch.float64,\n",
      "       grad_fn=<NativeBatchNormBackward0>)\n",
      "relu shape: torch.Size([13, 12]), val: tensor([[-0.0885, -0.0920, -0.0854, -0.0916, -0.0876, -0.0985, -0.0972, -0.0905,\n",
      "         -0.0798, -0.1092, -0.1051, -0.1124],\n",
      "        [ 0.5518,  0.6249,  0.7869,  0.7243,  0.7616,  0.7526,  0.6197,  0.3349,\n",
      "          0.3466,  0.4164,  0.4586,  0.4548],\n",
      "        [ 0.9468,  1.0255,  0.9607,  0.9526,  0.9148,  0.7632,  0.6632,  0.8054,\n",
      "          0.6438,  0.6227,  0.6076,  0.7229],\n",
      "        [ 0.3135,  0.2619,  0.2921,  0.2775,  0.2722,  0.2737,  0.2148,  0.1525,\n",
      "          0.3274,  0.3526,  0.4898,  0.3011],\n",
      "        [ 0.2895,  0.2804,  0.1369,  0.1710,  0.0649,  0.1843,  0.2466,  0.4262,\n",
      "          0.4008,  0.6384,  0.3445,  0.4959],\n",
      "        [ 0.8349,  0.8670,  0.7813,  0.7856,  0.7466,  0.6866,  0.7767,  0.5995,\n",
      "          0.3462,  0.4201,  0.5688,  0.5899],\n",
      "        [-0.1061, -0.1015, -0.0971, -0.0900, -0.0898, -0.0875, -0.0974, -0.1045,\n",
      "         -0.0983, -0.1048, -0.0875, -0.1094],\n",
      "        [-0.1414, -0.1547, -0.1562, -0.1709, -0.1693, -0.1717, -0.1597, -0.1619,\n",
      "         -0.1715, -0.1928, -0.1996, -0.1868],\n",
      "        [-0.1001, -0.0954, -0.1052, -0.0887, -0.0979, -0.0912, -0.0854, -0.0779,\n",
      "         -0.0875, -0.0743, -0.0868, -0.0855],\n",
      "        [ 0.2426,  0.2180,  0.2691,  0.3341,  0.3636,  0.3963,  0.3780,  0.5109,\n",
      "          0.6036,  0.6274,  0.7182,  0.7482],\n",
      "        [-0.1370, -0.1292, -0.1260, -0.1264, -0.1209, -0.1209, -0.1246, -0.1240,\n",
      "         -0.1156, -0.0958, -0.0958, -0.0895],\n",
      "        [ 0.6361,  0.6108,  0.5965,  0.5747,  0.6541,  0.7489,  0.7261,  0.6966,\n",
      "          0.7266,  1.0031,  0.8287,  0.8414],\n",
      "        [ 1.9157,  1.8403,  1.8741,  1.8567,  1.8779,  1.8915,  2.0176,  2.0619,\n",
      "          2.1311,  1.6889,  1.7337,  1.6831]], dtype=torch.float64,\n",
      "       grad_fn=<LeakyReluBackward1>)\n",
      "pool shape: torch.Size([13, 6]), val: tensor([[-0.0902, -0.0885, -0.0931, -0.0939, -0.0945, -0.1088],\n",
      "        [ 0.5884,  0.7556,  0.7571,  0.4773,  0.3815,  0.4567],\n",
      "        [ 0.9862,  0.9567,  0.8390,  0.7343,  0.6333,  0.6653],\n",
      "        [ 0.2877,  0.2848,  0.2730,  0.1836,  0.3400,  0.3955],\n",
      "        [ 0.2849,  0.1539,  0.1246,  0.3364,  0.5196,  0.4202],\n",
      "        [ 0.8510,  0.7834,  0.7166,  0.6881,  0.3831,  0.5794],\n",
      "        [-0.1038, -0.0935, -0.0886, -0.1009, -0.1015, -0.0985],\n",
      "        [-0.1481, -0.1635, -0.1705, -0.1608, -0.1821, -0.1932],\n",
      "        [-0.0978, -0.0969, -0.0946, -0.0816, -0.0809, -0.0862],\n",
      "        [ 0.2303,  0.3016,  0.3799,  0.4445,  0.6155,  0.7332],\n",
      "        [-0.1331, -0.1262, -0.1209, -0.1243, -0.1057, -0.0927],\n",
      "        [ 0.6234,  0.5856,  0.7015,  0.7113,  0.8649,  0.8350],\n",
      "        [ 1.8780,  1.8654,  1.8847,  2.0398,  1.9100,  1.7084]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([13, 6]), val: tensor([[ 0.0639,  0.0803,  0.0674,  0.0393, -0.0809, -0.0860],\n",
      "        [ 0.1488,  0.1011,  0.0297,  0.0333, -0.0595, -0.0675],\n",
      "        [-0.3314, -0.3073, -0.3200, -0.3350, -0.2943, -0.2905],\n",
      "        [ 0.1097,  0.1289,  0.0908,  0.0858, -0.0133,  0.0307],\n",
      "        [ 0.5393,  0.5462,  0.5098,  0.4394,  0.3871,  0.4184],\n",
      "        [-0.2671, -0.3031, -0.3151, -0.2658, -0.2579, -0.2947],\n",
      "        [-0.3136, -0.3464, -0.3486, -0.3521, -0.2840, -0.2055],\n",
      "        [-0.9960, -0.9984, -0.9892, -0.9705, -0.9611, -0.9956],\n",
      "        [-0.2428, -0.2942, -0.3365, -0.3009, -0.3375, -0.2968],\n",
      "        [-0.5277, -0.5383, -0.5207, -0.4648, -0.4273, -0.4482],\n",
      "        [-0.2648, -0.1781, -0.1014, -0.1919, -0.1559, -0.1437],\n",
      "        [ 0.2551,  0.2134,  0.1578,  0.1611,  0.1548,  0.1879],\n",
      "        [ 0.1995,  0.1793,  0.1607,  0.2144,  0.1801,  0.1660]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([13, 6]), val: tensor([[ 0.0639,  0.0803,  0.0674,  0.0393, -0.0081, -0.0086],\n",
      "        [ 0.1488,  0.1011,  0.0297,  0.0333, -0.0059, -0.0068],\n",
      "        [-0.0331, -0.0307, -0.0320, -0.0335, -0.0294, -0.0291],\n",
      "        [ 0.1097,  0.1289,  0.0908,  0.0858, -0.0013,  0.0307],\n",
      "        [ 0.5393,  0.5462,  0.5098,  0.4394,  0.3871,  0.4184],\n",
      "        [-0.0267, -0.0303, -0.0315, -0.0266, -0.0258, -0.0295],\n",
      "        [-0.0314, -0.0346, -0.0349, -0.0352, -0.0284, -0.0205],\n",
      "        [-0.0996, -0.0998, -0.0989, -0.0971, -0.0961, -0.0996],\n",
      "        [-0.0243, -0.0294, -0.0336, -0.0301, -0.0338, -0.0297],\n",
      "        [-0.0528, -0.0538, -0.0521, -0.0465, -0.0427, -0.0448],\n",
      "        [-0.0265, -0.0178, -0.0101, -0.0192, -0.0156, -0.0144],\n",
      "        [ 0.2551,  0.2134,  0.1578,  0.1611,  0.1548,  0.1879],\n",
      "        [ 0.1995,  0.1793,  0.1607,  0.2144,  0.1801,  0.1660]],\n",
      "       dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([13, 6]), val: tensor([[ 0.0639,  0.0803,  0.0674,  0.0393, -0.0081, -0.0086],\n",
      "        [ 0.1488,  0.1011,  0.0297,  0.0333, -0.0059, -0.0068],\n",
      "        [-0.0331, -0.0307, -0.0320, -0.0335, -0.0294, -0.0291],\n",
      "        [ 0.1097,  0.1289,  0.0908,  0.0858, -0.0013,  0.0307],\n",
      "        [ 0.5393,  0.5462,  0.5098,  0.4394,  0.3871,  0.4184],\n",
      "        [-0.0267, -0.0303, -0.0315, -0.0266, -0.0258, -0.0295],\n",
      "        [-0.0314, -0.0346, -0.0349, -0.0352, -0.0284, -0.0205],\n",
      "        [-0.0996, -0.0998, -0.0989, -0.0971, -0.0961, -0.0996],\n",
      "        [-0.0243, -0.0294, -0.0336, -0.0301, -0.0338, -0.0297],\n",
      "        [-0.0528, -0.0538, -0.0521, -0.0465, -0.0427, -0.0448],\n",
      "        [-0.0265, -0.0178, -0.0101, -0.0192, -0.0156, -0.0144],\n",
      "        [ 0.2551,  0.2134,  0.1578,  0.1611,  0.1548,  0.1879],\n",
      "        [ 0.1995,  0.1793,  0.1607,  0.2144,  0.1801,  0.1660]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([13, 6]), val: tensor([[ 0.0639,  0.0803,  0.0674,  0.0393, -0.0081, -0.0086],\n",
      "        [ 0.1488,  0.1011,  0.0297,  0.0333, -0.0059, -0.0068],\n",
      "        [-0.0331, -0.0307, -0.0320, -0.0335, -0.0294, -0.0291],\n",
      "        [ 0.1097,  0.1289,  0.0908,  0.0858, -0.0013,  0.0307],\n",
      "        [ 0.5393,  0.5462,  0.5098,  0.4394,  0.3871,  0.4184],\n",
      "        [-0.0267, -0.0303, -0.0315, -0.0266, -0.0258, -0.0295],\n",
      "        [-0.0314, -0.0346, -0.0349, -0.0352, -0.0284, -0.0205],\n",
      "        [-0.0996, -0.0998, -0.0989, -0.0971, -0.0961, -0.0996],\n",
      "        [-0.0243, -0.0294, -0.0336, -0.0301, -0.0338, -0.0297],\n",
      "        [-0.0528, -0.0538, -0.0521, -0.0465, -0.0427, -0.0448],\n",
      "        [-0.0265, -0.0178, -0.0101, -0.0192, -0.0156, -0.0144],\n",
      "        [ 0.2551,  0.2134,  0.1578,  0.1611,  0.1548,  0.1879],\n",
      "        [ 0.1995,  0.1793,  0.1607,  0.2144,  0.1801,  0.1660]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([13, 1]), val: tensor([[-0.0491],\n",
      "        [-0.0496],\n",
      "        [-0.0744],\n",
      "        [-0.0509],\n",
      "        [ 0.0639],\n",
      "        [-0.0758],\n",
      "        [-0.0743],\n",
      "        [-0.0900],\n",
      "        [-0.0754],\n",
      "        [-0.0809],\n",
      "        [-0.0694],\n",
      "        [-0.0218],\n",
      "        [-0.0482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 71665258571.17497, val: 71665258571.17497\n",
      "conv1 shape: torch.Size([13, 12]), val: tensor([[ -93949.3182,  -87139.3130, -102763.2496, -102218.2929, -112457.4459,\n",
      "         -107041.2223, -101567.8302,  -72847.7001,  -77618.5906, -111753.8976,\n",
      "         -133842.5661, -124525.3399],\n",
      "        [  50627.9335,   43417.7373,   39628.6506,   41121.7025,   36085.6748,\n",
      "           32169.0129,   39568.2227,   43052.6850,   32591.0423,   92561.5600,\n",
      "          105139.2538,   75915.5312],\n",
      "        [ 284356.7306,  278589.5158,  290547.2759,  297228.5524,  296139.9562,\n",
      "          294342.2020,  299039.3413,  274614.5429,  332074.9909,  292697.3153,\n",
      "          273608.6094,  227915.8467],\n",
      "        [ 183294.2071,  170918.2627,  185734.4620,  188451.6013,  192650.9590,\n",
      "          195407.1930,  191450.1417,  206383.2578,  260655.3815,  240153.6135,\n",
      "          178972.2589,  187625.1152],\n",
      "        [ 118854.0323,  135072.3916,  159330.6915,  153844.2661,  160279.7446,\n",
      "          173225.6770,  167927.2812,  171837.0223,  223230.5304,  259414.6438,\n",
      "          191642.5445,  209526.4074],\n",
      "        [ 179526.7210,  171331.0991,  161681.1777,  167003.3370,  172826.4049,\n",
      "          166885.0270,  164719.9826,  140741.9314,  142130.7455,   82964.3239,\n",
      "          113821.7671,   90128.2189],\n",
      "        [-246429.4823, -271938.9641, -275629.0170, -282575.7404, -285253.5455,\n",
      "         -282544.1484, -279581.7195, -273483.7172, -288369.6614, -236202.9663,\n",
      "         -216593.6913, -186468.6131],\n",
      "        [-205452.9986, -199725.1796, -212735.2291, -220796.9113, -222592.2251,\n",
      "         -238090.3010, -235605.1500, -271959.9056, -323713.5256, -328446.3010,\n",
      "         -290837.9160, -318770.9197],\n",
      "        [  34252.3275,   39790.9144,   25730.8943,    9298.2343,    8683.9035,\n",
      "             438.2803,  -15435.2880,   -6397.0006,  -15018.3646,  -65311.7507,\n",
      "         -114455.8321,  -76954.9918],\n",
      "        [  62075.8244,   66031.6284,   79774.0476,   80948.7629,   91879.8512,\n",
      "           90688.2342,   95292.3230,  119755.1092,  131577.2781,  198008.8277,\n",
      "          154503.7008,  138247.9463],\n",
      "        [-145015.8735, -139777.3367, -154546.9607, -155789.9542, -165757.4530,\n",
      "         -172865.4360, -176627.6633, -182007.0063, -157957.0481, -160098.5952,\n",
      "         -179903.1173, -167814.2232],\n",
      "        [ 155932.3863,  153087.6711,  163794.2068,  153945.5918,  162677.9956,\n",
      "          164296.8917,  160026.6958,  162284.0029,  188246.9944,  177638.6290,\n",
      "          168336.6412,  178203.6716],\n",
      "        [ 287447.9168,  278011.9951,  271759.7303,  276657.8198,  277144.8951,\n",
      "          282390.5857,  279363.2272,  299873.2702,  261242.7012,  202652.1429,\n",
      "          231706.9095,  201309.4563]], dtype=torch.float64,\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "bn1 shape: torch.Size([13, 12]), val: tensor([[-0.8556, -0.8044, -0.8582, -0.8295, -0.8716, -0.8241, -0.7941, -0.6387,\n",
      "         -0.6421, -0.8128, -0.9182, -0.8917],\n",
      "        [-0.0033, -0.0333, -0.0511, -0.0311, -0.0602, -0.0750, -0.0309, -0.0214,\n",
      "         -0.1067,  0.2167,  0.3656,  0.2400],\n",
      "        [ 1.3745,  1.3557,  1.3712,  1.3955,  1.3604,  1.3359,  1.3724,  1.2120,\n",
      "          1.3483,  1.2251,  1.2705,  1.0982],\n",
      "        [ 0.7788,  0.7198,  0.7771,  0.7896,  0.7951,  0.8035,  0.7905,  0.8486,\n",
      "          1.0013,  0.9603,  0.7622,  0.8707],\n",
      "        [ 0.3989,  0.5081,  0.6274,  0.5968,  0.6182,  0.6841,  0.6633,  0.6646,\n",
      "          0.8195,  1.0574,  0.8302,  0.9944],\n",
      "        [ 0.7565,  0.7222,  0.6407,  0.6701,  0.6868,  0.6500,  0.6460,  0.4990,\n",
      "          0.4255,  0.1683,  0.4122,  0.3202],\n",
      "        [-1.7545, -1.8958, -1.8380, -1.8342, -1.8155, -1.7686, -1.7568, -1.7074,\n",
      "         -1.6660, -1.4399, -1.3627, -1.2415],\n",
      "        [-1.5130, -1.4693, -1.4815, -1.4901, -1.4732, -1.5293, -1.5190, -1.6992,\n",
      "         -1.8377, -1.9047, -1.7615, -1.9885],\n",
      "        [-0.0999, -0.0547, -0.1298, -0.2084, -0.2099, -0.2457, -0.3283, -0.2848,\n",
      "         -0.3380, -0.5788, -0.8140, -0.6231],\n",
      "        [ 0.0642,  0.1003,  0.1765,  0.1908,  0.2446,  0.2399,  0.2705,  0.3872,\n",
      "          0.3742,  0.7480,  0.6307,  0.5919],\n",
      "        [-1.1567, -1.1153, -1.1517, -1.1280, -1.1627, -1.1783, -1.2001, -1.2201,\n",
      "         -1.0324, -1.0564, -1.1656, -1.1361],\n",
      "        [ 0.6175,  0.6145,  0.6527,  0.5974,  0.6313,  0.6361,  0.6206,  0.6137,\n",
      "          0.6496,  0.6453,  0.7050,  0.8175],\n",
      "        [ 1.3928,  1.3523,  1.2647,  1.2810,  1.2566,  1.2716,  1.2660,  1.3465,\n",
      "          1.0042,  0.7714,  1.0455,  0.9480]], dtype=torch.float64,\n",
      "       grad_fn=<NativeBatchNormBackward0>)\n",
      "relu shape: torch.Size([13, 12]), val: tensor([[-8.5565e-02, -8.0437e-02, -8.5816e-02, -8.2955e-02, -8.7160e-02,\n",
      "         -8.2411e-02, -7.9413e-02, -6.3870e-02, -6.4208e-02, -8.1281e-02,\n",
      "         -9.1817e-02, -8.9173e-02],\n",
      "        [-3.3363e-04, -3.3275e-03, -5.1072e-03, -3.1082e-03, -6.0170e-03,\n",
      "         -7.4969e-03, -3.0860e-03, -2.1372e-03, -1.0665e-02,  2.1667e-01,\n",
      "          3.6557e-01,  2.3999e-01],\n",
      "        [ 1.3745e+00,  1.3557e+00,  1.3712e+00,  1.3955e+00,  1.3604e+00,\n",
      "          1.3359e+00,  1.3724e+00,  1.2120e+00,  1.3483e+00,  1.2251e+00,\n",
      "          1.2705e+00,  1.0982e+00],\n",
      "        [ 7.7876e-01,  7.1977e-01,  7.7707e-01,  7.8961e-01,  7.9508e-01,\n",
      "          8.0348e-01,  7.9053e-01,  8.4858e-01,  1.0013e+00,  9.6034e-01,\n",
      "          7.6218e-01,  8.7071e-01],\n",
      "        [ 3.9887e-01,  5.0806e-01,  6.2741e-01,  5.9683e-01,  6.1825e-01,\n",
      "          6.8411e-01,  6.6331e-01,  6.6458e-01,  8.1953e-01,  1.0574e+00,\n",
      "          8.3024e-01,  9.9437e-01],\n",
      "        [ 7.5655e-01,  7.2221e-01,  6.4074e-01,  6.7013e-01,  6.8678e-01,\n",
      "          6.4999e-01,  6.4597e-01,  4.9895e-01,  4.2552e-01,  1.6831e-01,\n",
      "          4.1221e-01,  3.2023e-01],\n",
      "        [-1.7545e-01, -1.8958e-01, -1.8380e-01, -1.8342e-01, -1.8155e-01,\n",
      "         -1.7686e-01, -1.7568e-01, -1.7074e-01, -1.6660e-01, -1.4399e-01,\n",
      "         -1.3627e-01, -1.2415e-01],\n",
      "        [-1.5130e-01, -1.4693e-01, -1.4815e-01, -1.4901e-01, -1.4732e-01,\n",
      "         -1.5293e-01, -1.5190e-01, -1.6992e-01, -1.8377e-01, -1.9047e-01,\n",
      "         -1.7615e-01, -1.9885e-01],\n",
      "        [-9.9874e-03, -5.4695e-03, -1.2985e-02, -2.0835e-02, -2.0985e-02,\n",
      "         -2.4572e-02, -3.2832e-02, -2.8476e-02, -3.3795e-02, -5.7880e-02,\n",
      "         -8.1403e-02, -6.2314e-02],\n",
      "        [ 6.4151e-02,  1.0029e-01,  1.7648e-01,  1.9077e-01,  2.4461e-01,\n",
      "          2.3995e-01,  2.7050e-01,  3.8717e-01,  3.7425e-01,  7.4799e-01,\n",
      "          6.3074e-01,  5.9192e-01],\n",
      "        [-1.1567e-01, -1.1153e-01, -1.1517e-01, -1.1280e-01, -1.1627e-01,\n",
      "         -1.1783e-01, -1.2001e-01, -1.2201e-01, -1.0324e-01, -1.0564e-01,\n",
      "         -1.1656e-01, -1.1361e-01],\n",
      "        [ 6.1745e-01,  6.1446e-01,  6.5271e-01,  5.9740e-01,  6.3135e-01,\n",
      "          6.3606e-01,  6.2059e-01,  6.1370e-01,  6.4957e-01,  6.4535e-01,\n",
      "          7.0505e-01,  8.1752e-01],\n",
      "        [ 1.3928e+00,  1.3523e+00,  1.2647e+00,  1.2810e+00,  1.2566e+00,\n",
      "          1.2716e+00,  1.2660e+00,  1.3465e+00,  1.0042e+00,  7.7138e-01,\n",
      "          1.0455e+00,  9.4798e-01]], dtype=torch.float64,\n",
      "       grad_fn=<LeakyReluBackward1>)\n",
      "pool shape: torch.Size([13, 6]), val: tensor([[-0.0830, -0.0844, -0.0848, -0.0716, -0.0727, -0.0905],\n",
      "        [-0.0018, -0.0041, -0.0068, -0.0026,  0.1030,  0.3028],\n",
      "        [ 1.3651,  1.3834,  1.3481,  1.2922,  1.2867,  1.1844],\n",
      "        [ 0.7493,  0.7833,  0.7993,  0.8196,  0.9808,  0.8164],\n",
      "        [ 0.4535,  0.6121,  0.6512,  0.6639,  0.9385,  0.9123],\n",
      "        [ 0.7394,  0.6554,  0.6684,  0.5725,  0.2969,  0.3662],\n",
      "        [-0.1825, -0.1836, -0.1792, -0.1732, -0.1553, -0.1302],\n",
      "        [-0.1491, -0.1486, -0.1501, -0.1609, -0.1871, -0.1875],\n",
      "        [-0.0077, -0.0169, -0.0228, -0.0307, -0.0458, -0.0719],\n",
      "        [ 0.0822,  0.1836,  0.2423,  0.3288,  0.5611,  0.6113],\n",
      "        [-0.1136, -0.1140, -0.1171, -0.1210, -0.1044, -0.1151],\n",
      "        [ 0.6160,  0.6251,  0.6337,  0.6171,  0.6475,  0.7613],\n",
      "        [ 1.3725,  1.2728,  1.2641,  1.3063,  0.8878,  0.9967]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "conv2 shape: torch.Size([13, 6]), val: tensor([[-8.8055e-02, -1.6454e-01, -1.7442e-01, -1.7670e-01, -3.6667e-01,\n",
      "         -3.5072e-01],\n",
      "        [ 3.4579e-01,  3.2373e-01,  3.0854e-01,  2.7502e-01,  1.8323e-01,\n",
      "          1.0292e-01],\n",
      "        [-3.8774e-01, -3.3768e-01, -3.2816e-01, -3.1423e-01, -1.7205e-01,\n",
      "         -1.5972e-01],\n",
      "        [ 1.2848e-02,  1.0065e-02,  7.5419e-03,  7.2377e-04, -3.3495e-02,\n",
      "         -1.7977e-02],\n",
      "        [ 5.0756e-01,  5.0712e-01,  5.0906e-01,  4.9747e-01,  4.9906e-01,\n",
      "          5.0814e-01],\n",
      "        [-1.7549e-01, -1.5415e-01, -1.6168e-01, -1.7293e-01, -1.5981e-01,\n",
      "         -1.7278e-01],\n",
      "        [-9.9439e-02, -5.5137e-02, -3.1184e-02, -4.0135e-02,  9.9401e-02,\n",
      "          5.2757e-02],\n",
      "        [-8.8646e-01, -9.1819e-01, -9.3784e-01, -9.4795e-01, -9.6893e-01,\n",
      "         -1.0196e+00],\n",
      "        [-9.0293e-02, -7.8684e-02, -7.2648e-02, -9.4193e-02, -1.0020e-01,\n",
      "         -1.5077e-01],\n",
      "        [-4.9673e-01, -5.0393e-01, -4.9557e-01, -4.7504e-01, -4.8637e-01,\n",
      "         -5.2031e-01],\n",
      "        [-5.7661e-01, -5.9602e-01, -5.9431e-01, -5.7272e-01, -5.4293e-01,\n",
      "         -4.2458e-01],\n",
      "        [ 4.5445e-01,  4.9778e-01,  5.0431e-01,  4.9070e-01,  5.6619e-01,\n",
      "          5.0349e-01],\n",
      "        [ 1.8523e-01,  1.9544e-01,  1.9167e-01,  1.8258e-01,  1.5223e-01,\n",
      "          1.5514e-01]], dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "relu2 shape: torch.Size([13, 6]), val: tensor([[-0.0088, -0.0165, -0.0174, -0.0177, -0.0367, -0.0351],\n",
      "        [ 0.3458,  0.3237,  0.3085,  0.2750,  0.1832,  0.1029],\n",
      "        [-0.0388, -0.0338, -0.0328, -0.0314, -0.0172, -0.0160],\n",
      "        [ 0.0128,  0.0101,  0.0075,  0.0007, -0.0033, -0.0018],\n",
      "        [ 0.5076,  0.5071,  0.5091,  0.4975,  0.4991,  0.5081],\n",
      "        [-0.0175, -0.0154, -0.0162, -0.0173, -0.0160, -0.0173],\n",
      "        [-0.0099, -0.0055, -0.0031, -0.0040,  0.0994,  0.0528],\n",
      "        [-0.0886, -0.0918, -0.0938, -0.0948, -0.0969, -0.1020],\n",
      "        [-0.0090, -0.0079, -0.0073, -0.0094, -0.0100, -0.0151],\n",
      "        [-0.0497, -0.0504, -0.0496, -0.0475, -0.0486, -0.0520],\n",
      "        [-0.0577, -0.0596, -0.0594, -0.0573, -0.0543, -0.0425],\n",
      "        [ 0.4544,  0.4978,  0.5043,  0.4907,  0.5662,  0.5035],\n",
      "        [ 0.1852,  0.1954,  0.1917,  0.1826,  0.1522,  0.1551]],\n",
      "       dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "pool2 shape: torch.Size([13, 6]), val: tensor([[-0.0088, -0.0165, -0.0174, -0.0177, -0.0367, -0.0351],\n",
      "        [ 0.3458,  0.3237,  0.3085,  0.2750,  0.1832,  0.1029],\n",
      "        [-0.0388, -0.0338, -0.0328, -0.0314, -0.0172, -0.0160],\n",
      "        [ 0.0128,  0.0101,  0.0075,  0.0007, -0.0033, -0.0018],\n",
      "        [ 0.5076,  0.5071,  0.5091,  0.4975,  0.4991,  0.5081],\n",
      "        [-0.0175, -0.0154, -0.0162, -0.0173, -0.0160, -0.0173],\n",
      "        [-0.0099, -0.0055, -0.0031, -0.0040,  0.0994,  0.0528],\n",
      "        [-0.0886, -0.0918, -0.0938, -0.0948, -0.0969, -0.1020],\n",
      "        [-0.0090, -0.0079, -0.0073, -0.0094, -0.0100, -0.0151],\n",
      "        [-0.0497, -0.0504, -0.0496, -0.0475, -0.0486, -0.0520],\n",
      "        [-0.0577, -0.0596, -0.0594, -0.0573, -0.0543, -0.0425],\n",
      "        [ 0.4544,  0.4978,  0.5043,  0.4907,  0.5662,  0.5035],\n",
      "        [ 0.1852,  0.1954,  0.1917,  0.1826,  0.1522,  0.1551]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "flat shape: torch.Size([13, 6]), val: tensor([[-0.0088, -0.0165, -0.0174, -0.0177, -0.0367, -0.0351],\n",
      "        [ 0.3458,  0.3237,  0.3085,  0.2750,  0.1832,  0.1029],\n",
      "        [-0.0388, -0.0338, -0.0328, -0.0314, -0.0172, -0.0160],\n",
      "        [ 0.0128,  0.0101,  0.0075,  0.0007, -0.0033, -0.0018],\n",
      "        [ 0.5076,  0.5071,  0.5091,  0.4975,  0.4991,  0.5081],\n",
      "        [-0.0175, -0.0154, -0.0162, -0.0173, -0.0160, -0.0173],\n",
      "        [-0.0099, -0.0055, -0.0031, -0.0040,  0.0994,  0.0528],\n",
      "        [-0.0886, -0.0918, -0.0938, -0.0948, -0.0969, -0.1020],\n",
      "        [-0.0090, -0.0079, -0.0073, -0.0094, -0.0100, -0.0151],\n",
      "        [-0.0497, -0.0504, -0.0496, -0.0475, -0.0486, -0.0520],\n",
      "        [-0.0577, -0.0596, -0.0594, -0.0573, -0.0543, -0.0425],\n",
      "        [ 0.4544,  0.4978,  0.5043,  0.4907,  0.5662,  0.5035],\n",
      "        [ 0.1852,  0.1954,  0.1917,  0.1826,  0.1522,  0.1551]],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "lin shape: torch.Size([13, 1]), val: tensor([[-0.0719],\n",
      "        [ 0.0056],\n",
      "        [-0.0754],\n",
      "        [-0.0639],\n",
      "        [ 0.0449],\n",
      "        [-0.0714],\n",
      "        [-0.0629],\n",
      "        [-0.0875],\n",
      "        [-0.0694],\n",
      "        [-0.0796],\n",
      "        [-0.0808],\n",
      "        [ 0.0412],\n",
      "        [-0.0274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)\n",
      "loss shape: 69917971494.55667, val: 69917971494.55667\n"
     ]
    }
   ],
   "source": [
    "conv1 = nn.Conv1d(13, 13, kernel_size=2, dilation=1)\n",
    "bn1 = nn.BatchNorm1d(12)\n",
    "pool1 = nn.AdaptiveAvgPool1d(6)\n",
    "relu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "conv2 = nn.Conv1d(13, 13, kernel_size=1, dilation=2)\n",
    "pool2 = nn.MaxPool1d(1)\n",
    "flat = nn.Flatten()\n",
    "lin = nn.Linear(6,1)\n",
    "conv1.double()\n",
    "conv2.double()\n",
    "lin.double()\n",
    "bn1.double()\n",
    "criterion = nn.MSELoss()\n",
    "for i, (forecast, billing) in enumerate(train_loader):\n",
    "#     forecast = forecast.view(-1, 4, 13)\n",
    "#     print(i, forecast, forecast.shape)\n",
    "    x = conv1(forecast)\n",
    "    print(f'conv1 shape: {x.shape}, val: {x}')\n",
    "    x = bn1(x)\n",
    "    print(f'bn1 shape: {x.shape}, val: {x}')\n",
    "    x = relu(x)\n",
    "    print(f'relu shape: {x.shape}, val: {x}')\n",
    "    x = pool1(x)\n",
    "    print(f'pool shape: {x.shape}, val: {x}')\n",
    "    x = conv2(x)\n",
    "    print(f'conv2 shape: {x.shape}, val: {x}')\n",
    "    x = relu(x)\n",
    "    print(f'relu2 shape: {x.shape}, val: {x}')\n",
    "    x = pool2(x)\n",
    "    print(f'pool2 shape: {x.shape}, val: {x}')\n",
    "    x = flat(x)\n",
    "    print(f'flat shape: {x.shape}, val: {x}')\n",
    "    x = lin(x)\n",
    "    print(f'lin shape: {x.shape}, val: {x}')\n",
    "    loss = criterion(x, billing)\n",
    "    print(f'loss shape: {loss}, val: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([[133000.],\n",
      "        [329000.],\n",
      "        [212000.],\n",
      "        [245000.],\n",
      "        [293000.],\n",
      "        [302000.],\n",
      "        [334000.],\n",
      "        [341000.],\n",
      "        [165000.],\n",
      "        [232000.],\n",
      "        [281000.],\n",
      "        [332155.],\n",
      "        [332155.]], dtype=torch.float64) torch.Size([13, 1])\n",
      "1 tensor([[ 93000.],\n",
      "        [323000.],\n",
      "        [269000.],\n",
      "        [282193.],\n",
      "        [283000.],\n",
      "        [343000.],\n",
      "        [230000.],\n",
      "        [266000.],\n",
      "        [330000.],\n",
      "        [222000.],\n",
      "        [338000.],\n",
      "        [301000.],\n",
      "        [156000.]], dtype=torch.float64) torch.Size([13, 1])\n",
      "2 tensor([[ 56000.],\n",
      "        [226000.],\n",
      "        [284000.],\n",
      "        [278000.],\n",
      "        [321000.],\n",
      "        [298000.],\n",
      "        [277000.],\n",
      "        [275000.],\n",
      "        [219000.],\n",
      "        [205000.],\n",
      "        [272000.],\n",
      "        [215000.],\n",
      "        [347000.]], dtype=torch.float64) torch.Size([13, 1])\n",
      "3 tensor([[134000.],\n",
      "        [409000.],\n",
      "        [239000.],\n",
      "        [220000.],\n",
      "        [212000.],\n",
      "        [156000.],\n",
      "        [114000.],\n",
      "        [237000.],\n",
      "        [365000.],\n",
      "        [115000.],\n",
      "        [194000.],\n",
      "        [238000.],\n",
      "        [223000.]], dtype=torch.float64) torch.Size([13, 1])\n",
      "4 tensor([[174000.],\n",
      "        [207000.],\n",
      "        [362000.],\n",
      "        [181000.],\n",
      "        [173000.],\n",
      "        [228000.],\n",
      "        [250000.],\n",
      "        [335000.],\n",
      "        [104000.],\n",
      "        [144000.],\n",
      "        [209000.],\n",
      "        [277000.],\n",
      "        [251000.]], dtype=torch.float64) torch.Size([13, 1])\n",
      "5 tensor([[239000.],\n",
      "        [228000.],\n",
      "        [228000.],\n",
      "        [181000.],\n",
      "        [280000.],\n",
      "        [167000.],\n",
      "        [178000.],\n",
      "        [294000.],\n",
      "        [265000.],\n",
      "        [232000.],\n",
      "        [284000.],\n",
      "        [268000.],\n",
      "        [259000.]], dtype=torch.float64) torch.Size([13, 1])\n",
      "6 tensor([[ 43000.],\n",
      "        [ 43000.],\n",
      "        [262000.],\n",
      "        [304000.],\n",
      "        [278000.],\n",
      "        [181000.],\n",
      "        [250000.],\n",
      "        [249000.],\n",
      "        [290000.],\n",
      "        [250000.],\n",
      "        [225000.],\n",
      "        [297000.],\n",
      "        [142000.]], dtype=torch.float64) torch.Size([13, 1])\n",
      "7 tensor([[289000.],\n",
      "        [152000.],\n",
      "        [296000.],\n",
      "        [224000.],\n",
      "        [240000.],\n",
      "        [182000.],\n",
      "        [281000.],\n",
      "        [114000.],\n",
      "        [282000.],\n",
      "        [153000.],\n",
      "        [252000.],\n",
      "        [321000.],\n",
      "        [204000.]], dtype=torch.float64) torch.Size([13, 1])\n",
      "8 tensor([[274000.],\n",
      "        [154000.],\n",
      "        [207000.],\n",
      "        [193000.],\n",
      "        [211000.],\n",
      "        [158000.],\n",
      "        [270000.],\n",
      "        [280000.],\n",
      "        [196000.],\n",
      "        [156000.],\n",
      "        [195000.],\n",
      "        [324000.],\n",
      "        [316000.]], dtype=torch.float64) torch.Size([13, 1])\n",
      "9 tensor([[262000.],\n",
      "        [164000.],\n",
      "        [200000.],\n",
      "        [206000.],\n",
      "        [213000.],\n",
      "        [167000.],\n",
      "        [368000.],\n",
      "        [237000.],\n",
      "        [320000.],\n",
      "        [129000.],\n",
      "        [234000.],\n",
      "        [297000.],\n",
      "        [244000.]], dtype=torch.float64) torch.Size([13, 1])\n",
      "10 tensor([[ 20000.],\n",
      "        [237000.],\n",
      "        [240000.],\n",
      "        [265000.],\n",
      "        [328000.],\n",
      "        [237000.],\n",
      "        [320758.],\n",
      "        [373000.],\n",
      "        [207000.],\n",
      "        [217000.],\n",
      "        [260000.],\n",
      "        [295000.],\n",
      "        [249000.]], dtype=torch.float64) torch.Size([13, 1])\n",
      "11 tensor([[209000.],\n",
      "        [318000.],\n",
      "        [295000.],\n",
      "        [355000.],\n",
      "        [167000.],\n",
      "        [223000.],\n",
      "        [194000.],\n",
      "        [277000.],\n",
      "        [334000.],\n",
      "        [198000.],\n",
      "        [355000.],\n",
      "        [262000.],\n",
      "        [219000.]], dtype=torch.float64) torch.Size([13, 1])\n",
      "12 tensor([[246000.],\n",
      "        [289000.],\n",
      "        [241000.],\n",
      "        [275000.],\n",
      "        [114000.],\n",
      "        [268000.],\n",
      "        [273000.],\n",
      "        [295000.],\n",
      "        [183000.],\n",
      "        [265000.],\n",
      "        [276000.],\n",
      "        [276000.],\n",
      "        [161000.]], dtype=torch.float64) torch.Size([13, 1])\n",
      "13 tensor([[206000.],\n",
      "        [344000.],\n",
      "        [221000.],\n",
      "        [253000.],\n",
      "        [262000.],\n",
      "        [319000.],\n",
      "        [222000.],\n",
      "        [295000.],\n",
      "        [173000.],\n",
      "        [365000.],\n",
      "        [282000.],\n",
      "        [315000.],\n",
      "        [ 97000.]], dtype=torch.float64) torch.Size([13, 1])\n",
      "14 tensor([[ 79000.],\n",
      "        [198000.],\n",
      "        [170000.],\n",
      "        [296000.],\n",
      "        [227000.],\n",
      "        [282000.],\n",
      "        [330000.],\n",
      "        [243000.],\n",
      "        [269000.],\n",
      "        [207000.],\n",
      "        [459000.],\n",
      "        [279997.],\n",
      "        [209000.]], dtype=torch.float64) torch.Size([13, 1])\n"
     ]
    }
   ],
   "source": [
    "for i, (forecast, billing) in enumerate(train_loader):\n",
    "#     forecast = forecast.view(-1, 4, 13)\n",
    "    print(i, billing, billing.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs [tensor([[168835.9640],\n",
      "        [247802.5983],\n",
      "        [253332.9313],\n",
      "        [257661.0597],\n",
      "        [246965.0832],\n",
      "        [239988.9306],\n",
      "        [265892.8698],\n",
      "        [280358.5748],\n",
      "        [252622.5699],\n",
      "        [212566.0414],\n",
      "        [281775.3707],\n",
      "        [295486.8087],\n",
      "        [230443.2958]], dtype=torch.float64)]\n",
      "diff [tensor([[132164.0360],\n",
      "        [-23802.5983],\n",
      "        [ 26667.0687],\n",
      "        [ 58338.9403],\n",
      "        [-12965.0832],\n",
      "        [ -5988.9306],\n",
      "        [-19892.8698],\n",
      "        [106641.4252],\n",
      "        [-29622.5699],\n",
      "        [ 10433.9586],\n",
      "        [ 59224.6293],\n",
      "        [-51486.8087],\n",
      "        [ 18556.7042]], dtype=torch.float64)]\n"
     ]
    }
   ],
   "source": [
    "zz = []\n",
    "with torch.no_grad():\n",
    "    for forecast, billing in test_loader:\n",
    "        try:\n",
    "            outputs = model(forecast)\n",
    "        \n",
    "            print(f\"outputs [{outputs}]\")\n",
    "            dff = billing - outputs\n",
    "            print(f'diff [{dff}]')\n",
    "            zz.append(outputs)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[168835.9640],\n",
       "         [247802.5983],\n",
       "         [253332.9313],\n",
       "         [257661.0597],\n",
       "         [246965.0832],\n",
       "         [239988.9306],\n",
       "         [265892.8698],\n",
       "         [280358.5748],\n",
       "         [252622.5699],\n",
       "         [212566.0414],\n",
       "         [281775.3707],\n",
       "         [295486.8087],\n",
       "         [230443.2958]], dtype=torch.float64)]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[301000.],\n",
       "        [224000.],\n",
       "        [280000.],\n",
       "        [316000.],\n",
       "        [234000.],\n",
       "        [234000.],\n",
       "        [246000.],\n",
       "        [387000.],\n",
       "        [223000.],\n",
       "        [223000.],\n",
       "        [341000.],\n",
       "        [244000.],\n",
       "        [249000.],\n",
       "        [133000.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[132164.0360],\n",
       "        [-23802.5983],\n",
       "        [ 26667.0687],\n",
       "        [ 58338.9403],\n",
       "        [-12965.0832],\n",
       "        [ -5988.9306],\n",
       "        [-19892.8698],\n",
       "        [106641.4252],\n",
       "        [-29622.5699],\n",
       "        [ 10433.9586],\n",
       "        [ 59224.6293],\n",
       "        [-51486.8087],\n",
       "        [ 18556.7042]], dtype=torch.float64)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.y[:13] - zz[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128704.6723],\n",
       "        [188218.6695],\n",
       "        [195512.2947],\n",
       "        [197807.6950],\n",
       "        [188213.1969],\n",
       "        [185516.2014],\n",
       "        [204133.8966],\n",
       "        [215888.7913],\n",
       "        [194467.2224],\n",
       "        [163568.6816],\n",
       "        [218535.1709],\n",
       "        [224803.7465],\n",
       "        [178517.8794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(predict_set.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model(predict_set.x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
